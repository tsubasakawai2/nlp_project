{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "13b60299-c959-479a-8d50-c3cb97e49d40",
   "metadata": {
    "id": "13b60299-c959-479a-8d50-c3cb97e49d40",
    "outputId": "f0fb5718-2fe6-41e2-db04-6818ea514abe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>学習者のID</th>\n",
       "      <th>学習者の性別</th>\n",
       "      <th>学習環境</th>\n",
       "      <th>作文テーマ</th>\n",
       "      <th>学習者の母語</th>\n",
       "      <th>日本語学習履歴</th>\n",
       "      <th>日本語レベル</th>\n",
       "      <th>テスト成績（文字語彙）</th>\n",
       "      <th>テスト成績（文法）</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CG009</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年未満</td>\n",
       "      <td>初級</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CG011</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年未満</td>\n",
       "      <td>初級</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CG013</td>\n",
       "      <td>男</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年未満</td>\n",
       "      <td>上級</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CG015</td>\n",
       "      <td>男</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年未満</td>\n",
       "      <td>初級</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CG017</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CG018</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CG019</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CG020</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>上級</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CG021</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CG022</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>KG153</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>上級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>KG154</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>2年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>KG155</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>5年以上</td>\n",
       "      <td>上級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>KN001</td>\n",
       "      <td>男</td>\n",
       "      <td>国内</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>上級</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>KN002</td>\n",
       "      <td>男</td>\n",
       "      <td>国内</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>KN303</td>\n",
       "      <td>女</td>\n",
       "      <td>国内</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>2年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>KN307</td>\n",
       "      <td>女</td>\n",
       "      <td>国内</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>上級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>KN312</td>\n",
       "      <td>女</td>\n",
       "      <td>国内</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>上級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>KN313</td>\n",
       "      <td>男</td>\n",
       "      <td>国内</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>上級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>KN316</td>\n",
       "      <td>女</td>\n",
       "      <td>国内</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>5年以上</td>\n",
       "      <td>上級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>304 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    学習者のID 学習者の性別 学習環境                作文テーマ 学習者の母語   日本語学習履歴 日本語レベル  \\\n",
       "0    CG009      女   国外      外国語がうまくなる方法について    中国語      2年未満     初級   \n",
       "1    CG011      女   国外      外国語がうまくなる方法について    中国語      2年未満     初級   \n",
       "2    CG013      男   国外      外国語がうまくなる方法について    中国語      2年未満     上級   \n",
       "3    CG015      男   国外      外国語がうまくなる方法について    中国語      2年未満     初級   \n",
       "4    CG017      女   国外      外国語がうまくなる方法について    中国語      2年未満     中級   \n",
       "5    CG018      女   国外      外国語がうまくなる方法について    中国語  2年以上5年未満     中級   \n",
       "6    CG019      女   国外      外国語がうまくなる方法について    中国語  2年以上5年未満     中級   \n",
       "7    CG020      女   国外      外国語がうまくなる方法について    中国語  2年以上5年未満     上級   \n",
       "8    CG021      女   国外      外国語がうまくなる方法について    中国語  2年以上5年未満     中級   \n",
       "9    CG022      女   国外      外国語がうまくなる方法について    中国語  2年以上5年未満     中級   \n",
       "..     ...    ...  ...                  ...    ...       ...    ...   \n",
       "294  KG153      女   国外  インターネット時代に新聞や雑誌は必要か    韓国語  2年以上5年未満     上級   \n",
       "295  KG154      女   国外  インターネット時代に新聞や雑誌は必要か    韓国語      2年未満     中級   \n",
       "296  KG155      女   国外  インターネット時代に新聞や雑誌は必要か    韓国語      5年以上     上級   \n",
       "297  KN001      男   国内      外国語がうまくなる方法について    韓国語  2年以上5年未満     上級   \n",
       "298  KN002      男   国内      外国語がうまくなる方法について    韓国語  2年以上5年未満     中級   \n",
       "299  KN303      女   国内      外国語がうまくなる方法について    韓国語      2年未満     中級   \n",
       "300  KN307      女   国内      外国語がうまくなる方法について    韓国語  2年以上5年未満     上級   \n",
       "301  KN312      女   国内      外国語がうまくなる方法について    韓国語  2年以上5年未満     上級   \n",
       "302  KN313      男   国内      外国語がうまくなる方法について    韓国語  2年以上5年未満     上級   \n",
       "303  KN316      女   国内      外国語がうまくなる方法について    韓国語      5年以上     上級   \n",
       "\n",
       "    テスト成績（文字語彙） テスト成績（文法）  \n",
       "0             B         C  \n",
       "1             B         B  \n",
       "2             A         A  \n",
       "3             B         B  \n",
       "4             B         A  \n",
       "5             A         A  \n",
       "6             A         A  \n",
       "7             A         A  \n",
       "8             A         B  \n",
       "9             A         B  \n",
       "..          ...       ...  \n",
       "294           X         X  \n",
       "295           X         X  \n",
       "296           X         X  \n",
       "297           A         A  \n",
       "298           A         A  \n",
       "299           X         X  \n",
       "300           X         X  \n",
       "301           X         X  \n",
       "302           X         X  \n",
       "303           X         X  \n",
       "\n",
       "[304 rows x 9 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "\n",
    "path = \"data/register.xls\"\n",
    "labels = pd.read_excel(path)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e5a45fc-5165-4705-a4ab-b9cd5bbd5da3",
   "metadata": {
    "id": "9e5a45fc-5165-4705-a4ab-b9cd5bbd5da3",
    "outputId": "2f2f1efb-5051-4732-bfc5-bb78eebc8868"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'初級': 31, '上級': 124, '中級': 149})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "a = Counter(labels.日本語レベル)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98583a0b-403d-4212-8f28-f019e4a13f37",
   "metadata": {
    "id": "98583a0b-403d-4212-8f28-f019e4a13f37",
    "outputId": "ec537b9d-a0f1-4ed2-d868-832a2f3c34d7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>学習者のID</th>\n",
       "      <th>学習者の性別</th>\n",
       "      <th>学習環境</th>\n",
       "      <th>作文テーマ</th>\n",
       "      <th>学習者の母語</th>\n",
       "      <th>日本語学習履歴</th>\n",
       "      <th>日本語レベル</th>\n",
       "      <th>テスト成績（文字語彙）</th>\n",
       "      <th>テスト成績（文法）</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CG009</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年未満</td>\n",
       "      <td>初級</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CG011</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年未満</td>\n",
       "      <td>初級</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CG013</td>\n",
       "      <td>男</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年未満</td>\n",
       "      <td>上級</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CG015</td>\n",
       "      <td>男</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年未満</td>\n",
       "      <td>初級</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CG017</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CG018</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CG019</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CG020</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>上級</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CG021</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CG022</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>KG093</td>\n",
       "      <td>男</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>KG094</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>KG095</td>\n",
       "      <td>男</td>\n",
       "      <td>国外</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>5年以上</td>\n",
       "      <td>中級</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>KN001</td>\n",
       "      <td>男</td>\n",
       "      <td>国内</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>上級</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>KN002</td>\n",
       "      <td>男</td>\n",
       "      <td>国内</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>KN303</td>\n",
       "      <td>女</td>\n",
       "      <td>国内</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>2年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>KN307</td>\n",
       "      <td>女</td>\n",
       "      <td>国内</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>上級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>KN312</td>\n",
       "      <td>女</td>\n",
       "      <td>国内</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>上級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>KN313</td>\n",
       "      <td>男</td>\n",
       "      <td>国内</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>上級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>KN316</td>\n",
       "      <td>女</td>\n",
       "      <td>国内</td>\n",
       "      <td>外国語がうまくなる方法について</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>5年以上</td>\n",
       "      <td>上級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    学習者のID 学習者の性別 学習環境            作文テーマ 学習者の母語   日本語学習履歴 日本語レベル テスト成績（文字語彙）  \\\n",
       "0    CG009      女   国外  外国語がうまくなる方法について    中国語      2年未満     初級           B   \n",
       "1    CG011      女   国外  外国語がうまくなる方法について    中国語      2年未満     初級           B   \n",
       "2    CG013      男   国外  外国語がうまくなる方法について    中国語      2年未満     上級           A   \n",
       "3    CG015      男   国外  外国語がうまくなる方法について    中国語      2年未満     初級           B   \n",
       "4    CG017      女   国外  外国語がうまくなる方法について    中国語      2年未満     中級           B   \n",
       "5    CG018      女   国外  外国語がうまくなる方法について    中国語  2年以上5年未満     中級           A   \n",
       "6    CG019      女   国外  外国語がうまくなる方法について    中国語  2年以上5年未満     中級           A   \n",
       "7    CG020      女   国外  外国語がうまくなる方法について    中国語  2年以上5年未満     上級           A   \n",
       "8    CG021      女   国外  外国語がうまくなる方法について    中国語  2年以上5年未満     中級           A   \n",
       "9    CG022      女   国外  外国語がうまくなる方法について    中国語  2年以上5年未満     中級           A   \n",
       "..     ...    ...  ...              ...    ...       ...    ...         ...   \n",
       "239  KG093      男   国外  外国語がうまくなる方法について    韓国語  2年以上5年未満     中級           B   \n",
       "240  KG094      女   国外  外国語がうまくなる方法について    韓国語  2年以上5年未満     中級           A   \n",
       "241  KG095      男   国外  外国語がうまくなる方法について    韓国語      5年以上     中級           A   \n",
       "297  KN001      男   国内  外国語がうまくなる方法について    韓国語  2年以上5年未満     上級           A   \n",
       "298  KN002      男   国内  外国語がうまくなる方法について    韓国語  2年以上5年未満     中級           A   \n",
       "299  KN303      女   国内  外国語がうまくなる方法について    韓国語      2年未満     中級           X   \n",
       "300  KN307      女   国内  外国語がうまくなる方法について    韓国語  2年以上5年未満     上級           X   \n",
       "301  KN312      女   国内  外国語がうまくなる方法について    韓国語  2年以上5年未満     上級           X   \n",
       "302  KN313      男   国内  外国語がうまくなる方法について    韓国語  2年以上5年未満     上級           X   \n",
       "303  KN316      女   国内  外国語がうまくなる方法について    韓国語      5年以上     上級           X   \n",
       "\n",
       "    テスト成績（文法）  \n",
       "0           C  \n",
       "1           B  \n",
       "2           A  \n",
       "3           B  \n",
       "4           A  \n",
       "5           A  \n",
       "6           A  \n",
       "7           A  \n",
       "8           B  \n",
       "9           B  \n",
       "..        ...  \n",
       "239         A  \n",
       "240         B  \n",
       "241         B  \n",
       "297         A  \n",
       "298         A  \n",
       "299         X  \n",
       "300         X  \n",
       "301         X  \n",
       "302         X  \n",
       "303         X  \n",
       "\n",
       "[192 rows x 9 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "label1 = labels.loc[labels.作文テーマ == \"外国語がうまくなる方法について\",:]\n",
    "label1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2ae02239-03db-4330-b29d-d68fc8164afd",
   "metadata": {
    "id": "2ae02239-03db-4330-b29d-d68fc8164afd",
    "outputId": "f22d7756-73b8-440e-e4c8-29f42411c6b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'初級': 31, '上級': 61, '中級': 100})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = Counter(label1.日本語レベル)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "834e01d4-9aac-47c2-97bb-142ca7a152b9",
   "metadata": {
    "id": "834e01d4-9aac-47c2-97bb-142ca7a152b9",
    "outputId": "23c1cfc9-7a11-4765-df73-95583e4530b4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>学習者のID</th>\n",
       "      <th>学習者の性別</th>\n",
       "      <th>学習環境</th>\n",
       "      <th>作文テーマ</th>\n",
       "      <th>学習者の母語</th>\n",
       "      <th>日本語学習履歴</th>\n",
       "      <th>日本語レベル</th>\n",
       "      <th>テスト成績（文字語彙）</th>\n",
       "      <th>テスト成績（文法）</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>CG101</td>\n",
       "      <td>男</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>CG102</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>CG103</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>CG104</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>CG105</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>上級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>CG106</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>CG107</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>CG108</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>CG109</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>上級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>CG110</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>中国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>KG146</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>5年以上</td>\n",
       "      <td>上級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>KG147</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>5年以上</td>\n",
       "      <td>上級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>KG148</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>5年以上</td>\n",
       "      <td>上級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>KG149</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>上級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>KG150</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>上級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>KG151</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>5年以上</td>\n",
       "      <td>上級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>KG152</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>上級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>KG153</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>2年以上5年未満</td>\n",
       "      <td>上級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>KG154</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>2年未満</td>\n",
       "      <td>中級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>KG155</td>\n",
       "      <td>女</td>\n",
       "      <td>国外</td>\n",
       "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
       "      <td>韓国語</td>\n",
       "      <td>5年以上</td>\n",
       "      <td>上級</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    学習者のID 学習者の性別 学習環境                作文テーマ 学習者の母語   日本語学習履歴 日本語レベル  \\\n",
       "76   CG101      男   国外  インターネット時代に新聞や雑誌は必要か    中国語  2年以上5年未満     中級   \n",
       "77   CG102      女   国外  インターネット時代に新聞や雑誌は必要か    中国語  2年以上5年未満     中級   \n",
       "78   CG103      女   国外  インターネット時代に新聞や雑誌は必要か    中国語  2年以上5年未満     中級   \n",
       "79   CG104      女   国外  インターネット時代に新聞や雑誌は必要か    中国語  2年以上5年未満     中級   \n",
       "80   CG105      女   国外  インターネット時代に新聞や雑誌は必要か    中国語  2年以上5年未満     上級   \n",
       "81   CG106      女   国外  インターネット時代に新聞や雑誌は必要か    中国語  2年以上5年未満     中級   \n",
       "82   CG107      女   国外  インターネット時代に新聞や雑誌は必要か    中国語  2年以上5年未満     中級   \n",
       "83   CG108      女   国外  インターネット時代に新聞や雑誌は必要か    中国語  2年以上5年未満     中級   \n",
       "84   CG109      女   国外  インターネット時代に新聞や雑誌は必要か    中国語  2年以上5年未満     上級   \n",
       "85   CG110      女   国外  インターネット時代に新聞や雑誌は必要か    中国語  2年以上5年未満     中級   \n",
       "..     ...    ...  ...                  ...    ...       ...    ...   \n",
       "287  KG146      女   国外  インターネット時代に新聞や雑誌は必要か    韓国語      5年以上     上級   \n",
       "288  KG147      女   国外  インターネット時代に新聞や雑誌は必要か    韓国語      5年以上     上級   \n",
       "289  KG148      女   国外  インターネット時代に新聞や雑誌は必要か    韓国語      5年以上     上級   \n",
       "290  KG149      女   国外  インターネット時代に新聞や雑誌は必要か    韓国語  2年以上5年未満     上級   \n",
       "291  KG150      女   国外  インターネット時代に新聞や雑誌は必要か    韓国語  2年以上5年未満     上級   \n",
       "292  KG151      女   国外  インターネット時代に新聞や雑誌は必要か    韓国語      5年以上     上級   \n",
       "293  KG152      女   国外  インターネット時代に新聞や雑誌は必要か    韓国語  2年以上5年未満     上級   \n",
       "294  KG153      女   国外  インターネット時代に新聞や雑誌は必要か    韓国語  2年以上5年未満     上級   \n",
       "295  KG154      女   国外  インターネット時代に新聞や雑誌は必要か    韓国語      2年未満     中級   \n",
       "296  KG155      女   国外  インターネット時代に新聞や雑誌は必要か    韓国語      5年以上     上級   \n",
       "\n",
       "    テスト成績（文字語彙） テスト成績（文法）  \n",
       "76            X         X  \n",
       "77            X         X  \n",
       "78            X         X  \n",
       "79            X         X  \n",
       "80            X         X  \n",
       "81            X         X  \n",
       "82            X         X  \n",
       "83            X         X  \n",
       "84            X         X  \n",
       "85            X         X  \n",
       "..          ...       ...  \n",
       "287           X         X  \n",
       "288           X         X  \n",
       "289           X         X  \n",
       "290           X         X  \n",
       "291           X         X  \n",
       "292           X         X  \n",
       "293           X         X  \n",
       "294           X         X  \n",
       "295           X         X  \n",
       "296           X         X  \n",
       "\n",
       "[112 rows x 9 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2 = labels.loc[labels.作文テーマ == \"インターネット時代に新聞や雑誌は必要か\",:]\n",
    "label2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d2be1bc8-8312-4010-b717-64244a03c21b",
   "metadata": {
    "id": "d2be1bc8-8312-4010-b717-64244a03c21b",
    "outputId": "3331d0b3-6928-48de-bbb9-e15434734d7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'中級': 49, '上級': 63})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter(label2.日本語レベル)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c1ec06dd-dcd9-4ff8-be26-c2eb5e3feef9",
   "metadata": {
    "id": "c1ec06dd-dcd9-4ff8-be26-c2eb5e3feef9",
    "outputId": "6f4b5e7b-4319-4bc8-9c06-80dd41a3f749"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['初級', '初級', '上級', '初級', '中級', '中級', '中級', '上級', '中級', '中級', '中級',\n",
       "        '上級', '中級', '中級', '中級', '初級', '中級', '上級', '初級', '初級', '中級', '中級',\n",
       "        '初級', '中級', '中級', '上級', '中級', '中級', '初級', '上級', '上級', '初級', '中級',\n",
       "        '中級', '上級', '中級', '中級', '初級', '初級', '中級', '上級', '中級', '上級', '中級',\n",
       "        '上級', '中級', '初級', '中級', '中級', '初級', '上級', '初級', '中級', '中級', '上級',\n",
       "        '初級', '初級', '上級', '上級', '中級', '中級', '中級', '上級', '中級', '中級', '上級',\n",
       "        '中級', '中級', '中級', '初級', '中級', '初級', '中級', '中級', '上級', '上級', '初級',\n",
       "        '上級', '中級', '中級', '上級', '中級', '中級', '中級', '中級', '中級', '中級', '中級',\n",
       "        '中級', '中級', '上級', '中級', '中級', '初級', '上級', '上級', '上級', '上級', '上級',\n",
       "        '中級', '中級', '中級', '中級', '初級', '中級', '中級', '初級', '中級', '中級', '中級',\n",
       "        '上級', '中級', '上級', '上級', '上級', '上級', '上級', '上級', '上級', '中級', '中級',\n",
       "        '上級', '中級', '上級', '上級', '中級', '上級', '上級', '中級', '中級', '初級', '中級',\n",
       "        '中級', '上級', '中級', '上級', '上級', '上級', '中級', '上級', '中級', '上級', '上級',\n",
       "        '上級', '中級', '初級', '中級', '中級', '上級', '上級', '中級', '中級', '上級', '上級',\n",
       "        '上級', '中級', '初級', '中級', '中級', '中級', '上級', '中級', '初級', '中級', '中級',\n",
       "        '中級', '上級', '初級', '上級', '中級', '中級', '中級', '初級', '中級', '中級', '中級',\n",
       "        '中級', '初級', '中級', '初級', '初級', '中級', '中級', '中級', '中級', '上級', '中級',\n",
       "        '中級', '上級', '上級', '上級', '上級'], dtype=object),\n",
       " 192)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_gaigo = np.array(label1.日本語レベル)\n",
    "level_gaigo, len(level_gaigo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c847ac2a-1c01-4872-b656-12ebc25ad33c",
   "metadata": {
    "id": "c847ac2a-1c01-4872-b656-12ebc25ad33c",
    "outputId": "19ad7cfe-19ac-4ced-c51d-f757ac485475"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['中級', '中級', '中級', '中級', '上級', '中級', '中級', '中級', '上級', '中級', '中級',\n",
       "        '中級', '中級', '上級', '上級', '中級', '中級', '中級', '上級', '中級', '中級', '中級',\n",
       "        '中級', '上級', '中級', '中級', '中級', '中級', '中級', '上級', '中級', '中級', '中級',\n",
       "        '上級', '中級', '中級', '中級', '中級', '中級', '中級', '中級', '上級', '中級', '中級',\n",
       "        '中級', '中級', '上級', '中級', '中級', '上級', '中級', '中級', '中級', '上級', '中級',\n",
       "        '上級', '上級', '上級', '上級', '中級', '上級', '上級', '上級', '中級', '上級', '上級',\n",
       "        '中級', '上級', '上級', '上級', '中級', '上級', '上級', '上級', '上級', '上級', '上級',\n",
       "        '上級', '上級', '上級', '上級', '上級', '上級', '中級', '上級', '上級', '上級', '上級',\n",
       "        '上級', '上級', '上級', '上級', '上級', '上級', '上級', '上級', '上級', '上級', '上級',\n",
       "        '上級', '上級', '上級', '上級', '上級', '上級', '上級', '上級', '上級', '上級', '上級',\n",
       "        '中級', '上級'], dtype=object),\n",
       " 112)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_internet = np.array(label2.日本語レベル)\n",
    "level_internet, len(level_internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c8c86efb-bf58-4252-be6f-ca08ba4da7a8",
   "metadata": {
    "id": "c8c86efb-bf58-4252-be6f-ca08ba4da7a8"
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "txt_path = \"data/txt/\"\n",
    "txt_topics = listdir(txt_path)\n",
    "\n",
    "txt_gaigo_path = txt_path + txt_topics[0]\n",
    "txt_internet_path = txt_path + txt_topics[1]\n",
    "\n",
    "txt_gaigo_files = listdir(txt_gaigo_path)\n",
    "txt_internet_files = listdir(txt_internet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5889ef08-8537-4546-a485-ddfdd2aa65b1",
   "metadata": {
    "id": "5889ef08-8537-4546-a485-ddfdd2aa65b1",
    "outputId": "cd5c8377-b367-41b5-ce16-e3365ac939f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os.path as osp\n",
    "# import re\n",
    "# gaigo_files = [f for f in listdir(osp.join(txt_path, txt_topics[0])) if f.endswith('.txt')]\n",
    "# gaigo = []\n",
    "# for gaigo_file in gaigo_files:\n",
    "#     # print(gaigo_file)\n",
    "#     with open(osp.join(txt_path, txt_topics[0], gaigo_file), \"r\", encoding=\"utf-8\") as f:\n",
    "#         lines = f.read()\n",
    "#         # type(lines)\n",
    "#         lines = re.sub('\\n', '', lines)\n",
    "#         lines = re.sub('\\u3000', '', lines)\n",
    "#     gaigo.append(lines)\n",
    "# # type(gaigo[0])\n",
    "# # gaigo[0][0]\n",
    "\n",
    "import os.path as osp\n",
    "import re\n",
    "gaigo_files = [f for f in listdir(osp.join(txt_path, txt_topics[0])) if f.endswith('.txt')]\n",
    "gaigo = []\n",
    "for gaigo_file in gaigo_files:\n",
    "    # print(gaigo_file)\n",
    "    with open(osp.join(txt_path, txt_topics[0], gaigo_file), \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.read()\n",
    "        # type(lines)\n",
    "        lines = re.sub(r'[0-9０-９a-zA-Zａ-ｚＡ-Ｚ]+', \" \", lines)\n",
    "        lines = re.sub(r'[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒—●★☆〇◎◆▼◇△□(：〜～＋=)／*&^%$#@!~`){}・［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕〈〉？、。・,\\./『』【】「」→←○《》≪≫\\n\\u3000]+', \"\", lines)\n",
    "        lines = lines.replace(\" \", \"\")\n",
    "\n",
    "\n",
    "    gaigo.append(lines)\n",
    "# type(gaigo[0])\n",
    "len(gaigo)\n",
    "\n",
    "# 注意。注釈は削除していない。記号のみ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ec8cc9c2-1357-4ff9-90ce-b5585ec755b5",
   "metadata": {
    "id": "ec8cc9c2-1357-4ff9-90ce-b5585ec755b5",
    "outputId": "7c42c261-a99a-4a4d-bce3-8af6bc4a7ab9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import re\n",
    "# internet_files = [f for f in listdir(osp.join(txt_path, txt_topics[1])) if f.endswith('.txt')]\n",
    "# internet = []\n",
    "# for internet_file in internet_files:\n",
    "#     with open(osp.join(txt_path, txt_topics[1], internet_file), \"r\", encoding=\"utf-8\") as f:\n",
    "#         lines = f.read()\n",
    "#         # print(lines)\n",
    "#         lines = lines.strip('\\ufeff□')\n",
    "#         lines = re.sub('■', '', lines)\n",
    "#         lines = re.sub('\\n', '', lines)\n",
    "#         lines = re.sub('□', '', lines)\n",
    "#         lines = re.sub('\\u3000', '', lines)\n",
    "        \n",
    "#     internet.append(lines)\n",
    "\n",
    "# internet[0]\n",
    "\n",
    "import re\n",
    "internet_files = [f for f in listdir(osp.join(txt_path, txt_topics[1])) if f.endswith('.txt')]\n",
    "internet = []\n",
    "for internet_file in internet_files:\n",
    "    with open(osp.join(txt_path, txt_topics[1], internet_file), \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.read()\n",
    "        # print(lines)\n",
    "        lines = lines.strip('\\ufeff□')\n",
    "        lines = re.sub(r'[0-9０-９a-zA-Zａ-ｚＡ-Ｚ]+', \" \", lines)\n",
    "        lines = re.sub(r'[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒—●★☆〇◎◆▼◇△□(：〜～＋=)／*&^%$#@!~`){}・［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕〈〉？、。・,\\./『』【】「」→←○《》≪≫\\n\\u3000]+', \"\", lines)\n",
    "        lines = lines.replace(\" \", \"\")\n",
    "        \n",
    "    internet.append(lines)\n",
    "\n",
    "len(internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6080253b-8225-4834-b212-f6da98da0835",
   "metadata": {
    "id": "6080253b-8225-4834-b212-f6da98da0835",
    "outputId": "8410c43a-6e38-4cdb-ed1c-33148c572912"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "124\n",
      "149\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict as dd\n",
    "text_tagged = dd(list)\n",
    "for level, txt in zip(level_gaigo, gaigo):\n",
    "    text_tagged[level].append(txt)\n",
    "\n",
    "for level, txt in zip(level_internet, internet):\n",
    "    text_tagged[level].append(txt)\n",
    "\n",
    "text_tagged_dict = dict(text_tagged)\n",
    "# text_tagged_dict\n",
    "for level in text_tagged_dict.keys():\n",
    "    print(len(text_tagged_dict[level]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "efc35f04-43f2-49e7-929c-e29f49948bd4",
   "metadata": {
    "id": "efc35f04-43f2-49e7-929c-e29f49948bd4"
   },
   "outputs": [],
   "source": [
    "for level in text_tagged_dict.keys():\n",
    "    text_tagged_dict[level] = \"\".join(text_tagged_dict[level])\n",
    "        \n",
    "# text_tagged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ec0de867-5c50-4c98-b8d6-43dd0b155c73",
   "metadata": {
    "id": "ec0de867-5c50-4c98-b8d6-43dd0b155c73"
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "# with open('text_tagged_dict2_joined.pickle', 'wb') as f:\n",
    "#     pickle.dump(text_tagged_dict, f)\n",
    "    \n",
    "text_tagged_dict2_joined = pickle.load(open('text_tagged_dict2_joined.pickle', 'rb'))\n",
    "# text_tagged_dict2_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9cd19413-0a78-491a-a043-5725826708be",
   "metadata": {
    "id": "9cd19413-0a78-491a-a043-5725826708be"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# text_length_dist = [len(text) for level in text_tagged_dict2.keys() for text in text_tagged_dict2[level]]\n",
    "# # [len(text) for text in text_tagged_dict2[\"初級\"]]\n",
    "# # text_length_dist\n",
    "\n",
    "# # np.max(text_length_dist) = 914\n",
    "# # to determine how many paddings we should add \n",
    "# plt.hist(text_length_dist, bins = np.arange(0, 1000, 10))\n",
    "# plt.show()\n",
    "# # we can determine the max length of texts as 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7df0bec8-4824-42bf-bc32-0adffa428ae8",
   "metadata": {
    "id": "7df0bec8-4824-42bf-bc32-0adffa428ae8",
    "outputId": "436e34e9-78f1-43df-8526-e27d1f627d8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'私は大学日本語科の一'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tagged_dict2_joined[\"初級\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1354c976-0341-4a4d-96d5-57a866b755ff",
   "metadata": {
    "id": "1354c976-0341-4a4d-96d5-57a866b755ff",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_tagged_joined2 = {\"初級\": list(), \"上級\": list(), \"中級\": list()}\n",
    "text_tagged_joined2[\"初級\"].append(text_tagged_dict2_joined[\"初級\"])\n",
    "text_len_advanced = len(text_tagged_dict2_joined[\"上級\"])\n",
    "text_len_intermediate = len(text_tagged_dict2_joined[\"中級\"])\n",
    "\n",
    "for i in range(5):\n",
    "    text_tagged_joined2[\"上級\"].append(text_tagged_dict2_joined[\"上級\"][int(i * text_len_advanced / 5) : int((i + 1) * text_len_advanced / 5)])\n",
    "\n",
    "for i in range(6):\n",
    "    text_tagged_joined2[\"中級\"].append(text_tagged_dict2_joined[\"中級\"][int(i * text_len_intermediate / 6) : int((i + 1) * text_len_intermediate / 6)])\n",
    "\n",
    "# text_tagged_joined2[\"中級\"][5]\n",
    "# text_tagged_dict2_joined[\"中級\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e6bd5880-8a56-437d-9d98-1b45a7244c85",
   "metadata": {
    "id": "e6bd5880-8a56-437d-9d98-1b45a7244c85",
    "outputId": "84930939-1e3c-4194-edff-fd0faed69fcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72690"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_len_advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5650580b-c46f-4f42-956f-103d996a6ec6",
   "metadata": {
    "id": "5650580b-c46f-4f42-956f-103d996a6ec6",
    "outputId": "db46d106-150f-47bc-fa97-46f9a8763527"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84307"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_len_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0ff682c6-ce89-44ad-a2e1-38eb842e4883",
   "metadata": {
    "id": "0ff682c6-ce89-44ad-a2e1-38eb842e4883",
    "outputId": "8aa7c5ea-48c4-48a4-d3d8-decb17379039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13407\n",
      "14538\n",
      "14051\n"
     ]
    }
   ],
   "source": [
    "for text in text_tagged_joined2.values():\n",
    "    print(len(text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1c9a7-b298-42ed-9bde-70d575e3d680",
   "metadata": {
    "id": "68d1c9a7-b298-42ed-9bde-70d575e3d680",
    "outputId": "c300876b-4fba-4b8f-dce3-b0ae00707834"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('ja_ginza')\n",
    "\n",
    "text_vectorized = {}\n",
    "for level in tqdm(text_tagged_joined2.keys()):\n",
    "    text_vectorized[level] = []\n",
    "    for text in text_tagged_joined2[level]:\n",
    "        tokenized = nlp(text)\n",
    "        for token in tokenized:\n",
    "            text_vectorized[level].append(token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc63193-6fab-4181-92dd-3d0452c35c2c",
   "metadata": {
    "id": "cfc63193-6fab-4181-92dd-3d0452c35c2c",
    "outputId": "a669c4b7-a458-479d-eb24-856477e598b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7655\n",
      "41449\n",
      "48524\n"
     ]
    }
   ],
   "source": [
    "# for i in text_vectorized.values():\n",
    "#     print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a2682a-7ded-4e4e-840e-64046523a06a",
   "metadata": {
    "id": "a1a2682a-7ded-4e4e-840e-64046523a06a"
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "# with open('text_vectorized_joined.pickle', 'wb') as f:\n",
    "#     pickle.dump(text_vectorized, f)\n",
    "    \n",
    "text_vectorized2 = pickle.load(open('text_vectorized_joined.pickle', 'rb'))\n",
    "# text_vectorized2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd96490-8457-4531-83a1-fbfa68c1e4dc",
   "metadata": {
    "id": "1dd96490-8457-4531-83a1-fbfa68c1e4dc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "window_size = 15\n",
    "n_sample = 1000 # less than or equal to (len(text_vectorized2[\"初級\"]) - window_size)\n",
    "\n",
    "\n",
    "text_transformed = {}\n",
    "for level, vector in text_vectorized2.items():\n",
    "    text_transformed[level] = []\n",
    "    start_index_list = np.random.permutation(list(range(len(text_vectorized2[level]) - window_size)))\n",
    "    for i in range(n_sample):\n",
    "        start_index = start_index_list[i]\n",
    "        # print(start_index)\n",
    "        chunks = []\n",
    "        for index in range(start_index, start_index + window_size):\n",
    "            chunks.append(text_vectorized2[level][index])\n",
    "        text_transformed[level].append(chunks)\n",
    "    text_transformed[level] = np.array(text_transformed[level])\n",
    "    \n",
    "# print(text_transformed)\n",
    "# text_transformed[\"初級\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93216d7f-f969-4d44-ab58-64c423366e29",
   "metadata": {
    "id": "93216d7f-f969-4d44-ab58-64c423366e29",
    "outputId": "b2e65f72-ac3e-4b49-d3a9-75bfdf504901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初級 (1000, 15, 300)\n",
      "上級 (1000, 15, 300)\n",
      "中級 (1000, 15, 300)\n"
     ]
    }
   ],
   "source": [
    "for level, value in text_transformed.items():\n",
    "    print(level, value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dgIN64tnlFd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0dgIN64tnlFd",
    "outputId": "fc8bd03a-2695-4443-eff5-ee3204943d7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e5468d86-f1de-477c-be4c-758fa8981c2b",
   "metadata": {
    "id": "e5468d86-f1de-477c-be4c-758fa8981c2b"
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "# with open('text_transformed_window_size15_sample_size1000.pickle', 'wb') as f:\n",
    "#     pickle.dump(text_transformed, f)\n",
    "    \n",
    "# text_transformed2 = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/nlp_project/text_transformed_window_size15_sample_size1000.pickle', 'rb'))\n",
    "text_transformed2 = pickle.load(open('text_transformed_window_size15_sample_size1000.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "751d8c39-6f9d-4150-8ac9-2c152a81d4c7",
   "metadata": {
    "id": "751d8c39-6f9d-4150-8ac9-2c152a81d4c7"
   },
   "outputs": [],
   "source": [
    "category_vectors = []\n",
    "for level, vectors in text_transformed2.items():\n",
    "    for vector in vectors:\n",
    "        category_vectors.append((level, vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "24dc7fa5-a9b2-4559-9d34-bf2279a54528",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "24dc7fa5-a9b2-4559-9d34-bf2279a54528",
    "outputId": "c377b8fd-1852-475d-fc9b-018e685f7471"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>初級</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>初級</td>\n",
       "      <td>[[0.47779918, -0.290014, 0.35474735, -0.09467977, 0.06885703, 0.3252033, 0.40672505, 0.14516696, -0.31611025, 0.10430088, 0.30429342, 0.04396815, 0.03765120...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>初級</td>\n",
       "      <td>[[-0.11235832, -0.04813227, -0.2808526, 0.009237373, -0.10141474, 0.2152539, -0.04496483, -0.24367486, -0.2034827, -0.09202653, 0.13513672, -0.1294869, 0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>初級</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>初級</td>\n",
       "      <td>[[-0.084042855, -0.03873583, 0.013529049, -0.19598125, -0.24243039, -0.071817756, 0.08944982, 0.015445012, -0.13281377, 0.015941007, -0.24492954, 0.00823402...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cat  \\\n",
       "0  初級   \n",
       "1  初級   \n",
       "2  初級   \n",
       "3  初級   \n",
       "4  初級   \n",
       "\n",
       "                                                                                                                                                             value  \n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1  [[0.47779918, -0.290014, 0.35474735, -0.09467977, 0.06885703, 0.3252033, 0.40672505, 0.14516696, -0.31611025, 0.10430088, 0.30429342, 0.04396815, 0.03765120...  \n",
       "2  [[-0.11235832, -0.04813227, -0.2808526, 0.009237373, -0.10141474, 0.2152539, -0.04496483, -0.24367486, -0.2034827, -0.09202653, 0.13513672, -0.1294869, 0.12...  \n",
       "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4  [[-0.084042855, -0.03873583, 0.013529049, -0.19598125, -0.24243039, -0.071817756, 0.08944982, 0.015445012, -0.13281377, 0.015941007, -0.24492954, 0.00823402...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "key, value = zip(*category_vectors)\n",
    "data2 = pd.DataFrame({'cat': key, 'value': value})\n",
    "\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e631c80f-2b73-4a17-a3a1-94d696166343",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e631c80f-2b73-4a17-a3a1-94d696166343",
    "outputId": "c003d985-e232-427b-c69c-129e602b2071"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((600, 2), (2400, 2))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = data2.sample(frac=0.2, random_state=200)\n",
    "train = data2.drop(test.index)\n",
    "\n",
    "test.shape, train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5c8a5af5-616a-4893-831b-aaeb36cb07ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c8a5af5-616a-4893-831b-aaeb36cb07ed",
    "outputId": "7d3ac23a-6aec-4e8e-99d2-43b4e8d00cb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2400, 15, 300), (2400, 3))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "ohe = preprocessing.OneHotEncoder()\n",
    "\n",
    "le.fit(data2.cat)\n",
    "y_train = le.transform(train.cat).reshape(-1, 1)\n",
    "ohe.fit(y_train)\n",
    "y_train = ohe.transform(y_train).todense()\n",
    "\n",
    "X_train = np.array([x for x in train.value])\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "55ef04c3-43f6-4792-9ae1-ebe94318b5e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55ef04c3-43f6-4792-9ae1-ebe94318b5e7",
    "outputId": "563cc824-f169-423b-96cc-beb6d71c938c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 256)               570368    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               77100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 903       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 648,371\n",
      "Trainable params: 648,371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "n_rnn = window_size = 15 # 時系列の数\n",
    "batch_size = 128\n",
    "epochs = 20  # epochsは、多いほど、精密に学習するが、重くなるため今回は小さくしている\n",
    "n_mid = 256  # 中間層のニューロン数\n",
    "data_dim = 300\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(n_mid, input_shape=(n_rnn, data_dim)))\n",
    "model_lstm.add(Dense(data_dim, activation=\"relu\"))\n",
    "model_lstm.add(Dense(3, activation=\"softmax\"))\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n",
    "print(model_lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13267e31-ab0e-4cac-83fd-49d99eccb2e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13267e31-ab0e-4cac-83fd-49d99eccb2e2",
    "outputId": "719f160b-2310-4416-ebce-977c6f3a6f48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "75/75 [==============================] - 7s 5ms/step - loss: 1.0234 - accuracy: 0.4479\n",
      "Epoch 2/20\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.9656 - accuracy: 0.4996\n",
      "Epoch 3/20\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.9293 - accuracy: 0.5229\n",
      "Epoch 4/20\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.9019 - accuracy: 0.5479\n",
      "Epoch 5/20\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.8629 - accuracy: 0.5817\n",
      "Epoch 6/20\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.8210 - accuracy: 0.5958\n",
      "Epoch 7/20\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.7933 - accuracy: 0.6108\n",
      "Epoch 8/20\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.7318 - accuracy: 0.6513\n",
      "Epoch 9/20\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.6725 - accuracy: 0.6858\n",
      "Epoch 10/20\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.6052 - accuracy: 0.7283\n",
      "Epoch 11/20\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.5547 - accuracy: 0.7508\n",
      "Epoch 12/20\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.5005 - accuracy: 0.7837\n",
      "Epoch 13/20\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.4047 - accuracy: 0.8342\n",
      "Epoch 14/20\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.3160 - accuracy: 0.8729\n",
      "Epoch 15/20\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.2981 - accuracy: 0.8804\n",
      "Epoch 16/20\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2298 - accuracy: 0.9125\n",
      "Epoch 17/20\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.1837 - accuracy: 0.9262\n",
      "Epoch 18/20\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.1223 - accuracy: 0.9588\n",
      "Epoch 19/20\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.1274 - accuracy: 0.9529\n",
      "Epoch 20/20\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.0881 - accuracy: 0.9704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f07203c6b20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "model_lstm.fit(X_train, y_train, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a8060b53-2c09-4e0d-a201-b22ffff3ca61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8060b53-2c09-4e0d-a201-b22ffff3ca61",
    "outputId": "415427ac-28b1-4cfe-fe26-ae1121ea6e68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((600, 15, 300), (600, 3))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "ohe = preprocessing.OneHotEncoder()\n",
    "\n",
    "le.fit(data2.cat)\n",
    "y_test = le.transform(test.cat).reshape(-1, 1)\n",
    "ohe.fit(y_test)\n",
    "y_test = ohe.transform(y_test).todense()\n",
    "\n",
    "X_test = np.array([x for x in test.value])\n",
    "\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f229a42-0179-479b-afd1-d0f07718eb15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f229a42-0179-479b-afd1-d0f07718eb15",
    "outputId": "2ff32e56-d5d5-4893-8abf-a98397c156f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5266666666666666"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(np.argmax(model_lstm.predict(X_test), axis=1), np.argmax(y_test, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0684f0ce-434c-42bf-b956-95e385e56f95",
   "metadata": {
    "id": "0684f0ce-434c-42bf-b956-95e385e56f95"
   },
   "source": [
    "The accuracy was 50.7%. This needs fine-tuning. Possible suggestions for fine-tuning are grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daec05e5-f8d7-4983-b186-ba4f1b6dff06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "daec05e5-f8d7-4983-b186-ba4f1b6dff06",
    "outputId": "508b166c-fd0f-4f61-c363-0b92a43fb475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 2, 1, 0, 0, 2, 2, 2, 2, 0, 2, 0,\n",
       "       0, 1, 0, 1, 0, 2, 1, 0, 0, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2, 2, 1, 2,\n",
       "       0, 2, 2, 1, 0, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 0, 0, 1, 0, 2, 0,\n",
       "       2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 0, 1, 1, 0, 0, 1, 0, 0, 2, 2, 2,\n",
       "       0, 2, 2, 0, 1, 2, 0, 0, 1, 2, 2, 1, 2, 2, 0, 1, 2, 1, 0, 2, 1, 2,\n",
       "       1, 1, 1, 0, 0, 0, 2, 2, 1, 2, 0, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       0, 2, 2, 0, 0, 1, 1, 2, 2, 1, 0, 2, 0, 2, 2, 2, 1, 0, 2, 0, 1, 0,\n",
       "       1, 2, 0, 1, 0, 0, 0, 0, 1, 2, 0, 1, 2, 1, 2, 1, 1, 1, 1, 0, 1, 2,\n",
       "       0, 1, 2, 0, 2, 1, 0, 2, 2, 2, 0, 0, 0, 2, 2, 0, 2, 2, 1, 2, 0, 0,\n",
       "       0, 2, 0, 1, 2, 0, 1, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 2, 1, 2, 0, 1,\n",
       "       0, 0, 2, 1, 0, 0, 1, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 0, 1, 1, 2, 0,\n",
       "       0, 2, 1, 2, 0, 0, 1, 2, 2, 2, 2, 0, 1, 1, 2, 2, 2, 1, 0, 2, 2, 2,\n",
       "       0, 2, 1, 2, 2, 2, 2, 1, 0, 0, 0, 0, 2, 2, 2, 0, 1, 1, 2, 1, 0, 0,\n",
       "       0, 0, 2, 1, 0, 0, 1, 2, 1, 2, 2, 1, 2, 2, 0, 0, 0, 1, 2, 2, 1, 2,\n",
       "       2, 1, 2, 2, 0, 0, 2, 0, 0, 0, 0, 2, 2, 2, 1, 2, 2, 0, 2, 2, 2, 1,\n",
       "       2, 1, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 1, 0, 1, 1, 2,\n",
       "       0, 0, 2, 0, 2, 1, 1, 1, 2, 1, 0, 0, 0, 1, 0, 2, 1, 1, 1, 0, 1, 0,\n",
       "       2, 2, 1, 2, 2, 1, 0, 1, 0, 1, 0, 0, 2, 2, 2, 0, 1, 0, 0, 2, 2, 1,\n",
       "       2, 2, 1, 2, 2, 0, 0, 2, 2, 1, 2, 1, 0, 2, 0, 2, 1, 2, 0, 1, 2, 1,\n",
       "       2, 0, 1, 2, 0, 2, 2, 2, 0, 0, 0, 1, 0, 2, 1, 0, 2, 2, 2, 1, 1, 2,\n",
       "       2, 2, 1, 2, 2, 0, 0, 1, 2, 2, 0, 2, 0, 2, 0, 0, 1, 2, 2, 1, 1, 1,\n",
       "       2, 0, 0, 1, 0, 2, 0, 2, 0, 2, 2, 2, 2, 0, 2, 1, 2, 0, 2, 2, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 1, 1, 0, 2, 2, 2, 0, 1, 2, 2, 0, 1, 1, 2, 2, 0,\n",
       "       0, 1, 0, 2, 1, 0, 1, 2, 0, 0, 0, 2, 1, 2, 1, 1, 2, 0, 2, 0, 1, 2,\n",
       "       2, 0, 1, 0, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 0, 1, 0, 2, 2, 2,\n",
       "       2, 2, 0, 0, 2, 2, 1, 2, 0, 0, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 2, 2,\n",
       "       1, 0, 1, 1, 2, 1, 1, 1, 2, 0, 1, 2, 2, 1, 2, 2, 2, 0, 0, 2, 2, 0,\n",
       "       1, 2, 0, 2, 0, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model_lstm.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eec2547b-32ce-4d9a-871d-80c38b53aa8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eec2547b-32ce-4d9a-871d-80c38b53aa8f",
    "outputId": "6da83794-69ad-435b-c198-240df0686e94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 2, 1, 2, 2, 2, 0, 1, 0, 1, 2, 0, 0, 1, 2, 1, 0, 1, 2, 2,\n",
       "         0, 1, 1, 0, 1, 1, 2, 0, 1, 0, 2, 1, 1, 2, 0, 1, 0, 2, 2, 0, 2,\n",
       "         0, 1, 0, 1, 2, 2, 1, 2, 2, 0, 0, 2, 2, 0, 2, 0, 1, 2, 2, 0, 2,\n",
       "         0, 0, 0, 2, 0, 0, 2, 2, 0, 2, 2, 1, 2, 0, 2, 2, 1, 0, 0, 2, 1,\n",
       "         0, 2, 2, 2, 0, 2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 0, 2, 1, 1, 1, 1,\n",
       "         1, 1, 2, 0, 2, 0, 1, 0, 0, 1, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "         2, 2, 0, 2, 0, 0, 0, 2, 1, 0, 2, 0, 0, 2, 1, 1, 1, 0, 2, 2, 2,\n",
       "         1, 1, 1, 2, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 2, 0,\n",
       "         2, 1, 1, 0, 1, 1, 0, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 2, 0, 1, 0,\n",
       "         2, 0, 0, 2, 2, 1, 0, 0, 1, 0, 2, 1, 0, 2, 1, 1, 0, 1, 0, 2, 2,\n",
       "         0, 2, 0, 1, 0, 2, 1, 1, 1, 0, 1, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2,\n",
       "         1, 0, 2, 1, 2, 2, 2, 0, 1, 2, 0, 0, 1, 1, 2, 1, 0, 0, 2, 2, 2,\n",
       "         2, 0, 1, 0, 2, 2, 1, 1, 0, 2, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2,\n",
       "         1, 0, 1, 2, 2, 1, 0, 2, 1, 1, 2, 2, 1, 0, 0, 2, 1, 0, 0, 0, 1,\n",
       "         0, 0, 2, 0, 0, 2, 1, 0, 0, 2, 2, 2, 0, 1, 1, 0, 2, 2, 0, 1, 1,\n",
       "         0, 1, 0, 1, 0, 0, 2, 1, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 1, 1,\n",
       "         1, 0, 0, 2, 1, 2, 0, 2, 2, 2, 0, 1, 2, 1, 1, 2, 0, 0, 1, 2, 1,\n",
       "         0, 0, 1, 0, 0, 2, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 0, 1, 2,\n",
       "         2, 0, 0, 2, 2, 1, 0, 1, 2, 2, 2, 1, 0, 0, 0, 2, 1, 1, 1, 0, 1,\n",
       "         2, 1, 1, 2, 2, 2, 2, 1, 1, 0, 2, 2, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 2, 0, 2, 0, 0, 2, 2,\n",
       "         0, 2, 2, 0, 0, 1, 0, 2, 1, 0, 0, 1, 2, 2, 0, 2, 2, 2, 2, 1, 1,\n",
       "         2, 0, 0, 1, 1, 1, 1, 2, 1, 1, 2, 2, 2, 0, 1, 2, 0, 0, 1, 2, 2,\n",
       "         2, 0, 1, 2, 2, 0, 1, 2, 2, 0, 2, 2, 1, 0, 1, 0, 2, 0, 1, 2, 0,\n",
       "         0, 1, 0, 1, 1, 0, 1, 0, 0, 2, 1, 1, 0, 2, 1, 2, 0, 2, 2, 0, 2,\n",
       "         1, 0, 2, 2, 0, 1, 0, 2, 2, 1, 0, 1, 0, 2, 0, 0, 2, 2, 0, 0, 1,\n",
       "         2, 2, 2, 2, 2, 0, 0, 2, 1, 2, 1, 2, 1, 0, 0, 1, 2, 0, 0, 0, 0,\n",
       "         1, 2, 1, 1, 2, 1, 2, 0, 1, 2, 0, 0, 2, 1, 0, 1, 2, 0, 2, 2, 2,\n",
       "         1, 0, 0, 2, 2, 0, 1, 1, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_test, axis=1).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "188b9a25-9234-4569-a440-03b0b59439b8",
   "metadata": {
    "id": "188b9a25-9234-4569-a440-03b0b59439b8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "# from keras.layers.core import Dense, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "# from keras.optimizers import Adam, Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "09d1519d-ab7c-48f1-b0d1-9b58d6628412",
   "metadata": {
    "id": "09d1519d-ab7c-48f1-b0d1-9b58d6628412"
   },
   "outputs": [],
   "source": [
    "window_size = 15\n",
    "n_sample = 1000 # less than or equal to (len(text_vectorized2[\"初級\"]) - window_size)\n",
    "\n",
    "n_rnn = window_size  # 時系列の数\n",
    "batch_size = 128\n",
    "epochs = 20  #epochsは、多いほど、精密に学習するが、重くなるため今回は小さくしている\n",
    "n_mid = 256  # 中間層のニューロン数\n",
    "data_dim = 300\n",
    "\n",
    "input_dim = (n_rnn, data_dim)\n",
    "output_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eb800270-72b4-45d2-9528-96c6bf6c8d4a",
   "metadata": {
    "id": "eb800270-72b4-45d2-9528-96c6bf6c8d4a"
   },
   "outputs": [],
   "source": [
    "def language_model(activation=\"relu\", optimizer=\"adam\", hidden_layer_sizes=(100, 100)):\n",
    "    model = Sequential()\n",
    "    firstflag = True\n",
    "    for dim in hidden_layer_sizes:\n",
    "        if firstflag:\n",
    "            model.add(LSTM(dim, input_shape=(n_rnn, data_dim), return_sequences=True, activation=activation))\n",
    "            model.add(Dropout(0.2))\n",
    "            firstflag = False\n",
    "        else:\n",
    "            model.add(LSTM(dim, return_sequences=True, activation=activation))\n",
    "            model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(n_mid, activation=activation))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(output_dim, activation=\"softmax\"))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d75a5336-7e0b-4aac-8835-c2fb553779d5",
   "metadata": {
    "id": "d75a5336-7e0b-4aac-8835-c2fb553779d5"
   },
   "outputs": [],
   "source": [
    "# activation = [\"relu\", \"tanh\"]\n",
    "optimizer = [\"adam\", \"adagrad\", \"sgd\", \"RMSprop\", \"Adamax\"]\n",
    "hidden_layer_sizes = [(50, 50, 50, 50), (50, 50, 50), (50, 50), (50, ) ]\n",
    "nb_epoch = [20, 25]\n",
    "batch_size = [128, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9751e1-4c58-4acd-9068-f7ca0ab799a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b9751e1-4c58-4acd-9068-f7ca0ab799a6",
    "outputId": "597b80c6-ef70-451a-c7e6-8672d5e7b024"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-c64ef7c3159e>:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model2 = KerasClassifier(build_fn=language_model, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "model2 = KerasClassifier(build_fn=language_model, verbose=0)\n",
    "param_grid = dict(activation=activation, \n",
    "                  optimizer=optimizer, \n",
    "                  hidden_layer_sizes=hidden_layer_sizes, \n",
    "                  nb_epoch=nb_epoch, \n",
    "                  batch_size=batch_size,)\n",
    "grid = GridSearchCV(estimator=model2, param_grid=param_grid, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7763d3e-26f0-4cf9-8298-bf378664feb3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7763d3e-26f0-4cf9-8298-bf378664feb3",
    "outputId": "77e7cd85-d5d9-4007-f9fe-77a492b6d3de",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_161 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_162 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_163 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_164 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_165 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_166 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_167 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_168 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_169 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_170 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_171 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_172 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_173 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_174 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_175 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_176 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_177 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_178 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_179 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_180 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_181 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_182 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_183 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   5.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_184 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_185 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_186 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_187 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_188 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   5.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_189 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_190 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_191 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_192 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_193 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   5.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_194 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_195 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_196 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_197 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_198 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_199 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_200 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_201 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_202 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_203 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   5.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_204 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_205 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_206 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_207 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_208 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_209 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_210 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_211 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_212 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_213 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_214 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_215 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_216 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_217 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_218 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_219 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_220 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_221 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_222 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_223 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_224 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_225 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_226 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_227 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_228 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_229 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_230 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_231 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_232 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_233 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_234 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_235 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_236 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_237 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_238 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_239 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_240 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_241 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_242 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_243 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_244 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_245 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_246 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_247 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_248 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_249 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_250 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_251 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_252 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_253 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_254 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_255 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_256 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_257 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   4.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_258 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_259 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_260 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_261 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_262 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_263 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_264 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_265 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   4.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_266 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_267 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_268 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_269 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_270 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_271 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_272 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_273 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_274 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_275 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_276 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_277 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_278 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_279 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_280 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_281 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_282 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_283 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_284 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_285 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_286 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_287 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_288 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_289 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_290 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_291 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_292 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_293 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   4.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_294 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_295 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_296 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_297 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_298 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_299 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_300 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_301 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_302 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_303 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_304 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_305 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_306 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_307 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_308 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_309 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_310 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_311 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_312 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_313 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_314 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_315 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_316 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_317 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_318 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_319 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_320 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_321 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_322 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_323 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_324 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_325 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_326 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_327 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_328 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_329 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_330 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_331 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_332 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_333 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_334 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_335 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_336 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_337 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_338 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_339 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_340 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_341 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_342 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_343 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_344 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_345 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_346 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_347 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_348 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_349 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   4.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_350 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_351 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_352 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_353 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_354 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_355 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_356 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_357 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   4.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_358 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_359 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_360 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_361 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_362 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_363 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_364 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_365 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_366 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_367 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_368 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_369 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   4.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_370 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_371 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_372 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_373 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_374 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_375 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_376 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_377 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_378 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_379 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_380 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_381 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_382 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_383 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_384 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_385 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_386 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_387 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_388 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_389 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_390 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_391 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_392 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_393 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_394 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_395 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_396 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_397 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_398 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_399 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_400 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_401 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_402 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_403 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_404 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_405 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_406 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_407 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_408 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_409 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_410 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_411 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_412 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_413 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_414 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_415 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_416 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_417 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_418 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_419 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_420 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_421 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_422 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_423 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_424 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_425 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_426 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_427 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_428 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_429 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_430 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_431 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_432 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_433 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_434 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_435 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_436 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_437 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_438 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_439 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_440 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_441 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   4.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_442 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_443 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_444 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_445 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_446 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_447 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_448 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_449 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   4.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_450 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_451 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_452 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_453 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_454 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_455 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_456 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_457 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_458 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_459 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_460 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_461 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_462 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_463 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_464 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_465 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_466 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_467 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_468 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_469 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_470 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_471 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_472 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_473 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_474 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   3.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_475 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_476 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_477 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_478 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_479 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_480 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_481 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_482 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_483 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   3.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_484 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_485 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_486 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_487 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_488 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_489 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_490 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_491 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_492 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   3.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_493 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_494 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_495 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_496 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_497 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_498 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_499 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_500 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_501 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   4.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_502 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_503 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_504 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   3.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_505 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_506 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_507 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   4.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_508 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_509 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_510 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   3.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_511 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_512 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_513 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   3.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_514 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_515 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_516 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_517 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_518 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_519 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_520 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_521 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_522 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_523 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_524 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_525 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   3.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_526 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_527 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_528 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_529 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_530 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_531 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_532 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_533 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_534 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_535 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_536 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_537 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_538 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_539 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_540 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_541 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_542 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_543 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   3.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_544 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_545 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_546 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_547 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_548 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_549 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_550 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_551 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_552 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_553 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_554 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_555 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_556 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_557 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_558 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_559 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_560 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_561 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_562 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_563 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_564 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   3.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_565 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_566 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_567 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_568 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_569 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_570 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_571 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_572 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_573 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   3.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_574 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_575 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_576 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   3.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_577 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_578 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_579 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   3.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_580 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_581 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_582 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   3.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_583 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_584 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_585 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   3.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_586 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_587 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   3.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_588 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_589 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_590 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_591 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_592 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_593 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_594 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   3.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_595 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_596 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_597 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   3.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_598 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_599 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_600 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_601 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_602 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_603 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_604 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_605 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_606 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_607 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_608 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_609 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_610 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_611 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_612 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_613 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_614 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_615 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_616 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_617 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_618 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_619 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_620 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_621 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_622 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_623 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_624 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_625 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_626 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_627 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_628 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_629 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_630 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_631 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_632 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_633 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_634 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_635 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_636 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_637 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_638 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_639 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_640 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_641 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_642 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_643 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_644 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_645 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_646 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_647 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_648 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_649 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_650 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_651 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_652 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_653 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_654 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_655 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_656 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_657 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_658 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_659 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_660 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_661 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_662 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_663 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_664 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_665 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_666 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_667 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_668 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_669 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_670 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_671 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_672 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_673 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_674 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_675 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_676 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_677 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_678 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_679 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_680 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_681 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_682 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_683 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_684 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_685 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_686 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   2.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_687 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_688 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_689 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_690 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_691 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_692 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   2.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_693 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_694 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_695 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_696 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_697 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_698 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   2.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_699 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_700 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_701 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_702 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_703 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_704 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_705 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_706 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_707 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_708 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_709 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_710 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_711 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_712 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_713 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_714 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_715 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f36ab695820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Layer lstm_716 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_717 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_718 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_719 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_720 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f36abcde040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Layer lstm_721 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_722 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_723 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_724 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_725 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_726 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_727 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_728 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_729 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_730 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_731 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_732 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_733 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   6.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_734 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_735 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_736 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_737 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_738 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_739 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_740 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_741 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_742 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_743 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_744 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_745 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_746 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_747 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_748 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_749 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_750 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_751 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_752 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_753 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_754 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_755 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_756 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_757 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_758 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_759 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_760 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_761 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_762 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_763 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_764 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_765 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_766 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_767 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_768 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   6.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_769 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_770 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_771 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_772 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_773 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_774 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_775 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_776 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_777 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_778 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   6.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_779 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_780 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_781 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_782 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_783 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_784 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_785 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_786 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_787 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_788 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_789 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_790 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_791 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_792 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_793 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_794 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_795 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_796 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_797 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_798 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_799 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_800 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_801 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_802 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_803 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_804 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_805 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_806 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_807 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_808 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_809 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_810 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_811 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_812 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_813 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_814 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_815 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_816 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_817 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_818 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_819 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_820 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_821 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_822 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_823 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_824 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_825 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_826 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_827 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_828 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_829 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_830 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_831 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_832 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   6.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_833 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_834 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_835 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_836 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_837 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_838 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_839 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_840 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_841 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_842 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_843 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_844 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_845 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_846 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_847 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_848 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_849 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_850 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_851 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_852 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_853 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_854 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_855 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_856 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_857 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_858 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_859 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_860 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_861 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_862 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_863 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_864 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_865 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_866 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_867 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_868 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_869 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_870 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_871 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_872 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_873 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_874 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_875 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_876 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_877 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_878 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_879 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_880 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_881 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_882 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_883 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_884 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_885 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_886 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_887 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_888 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_889 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_890 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_891 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_892 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_893 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_894 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_895 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_896 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_897 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_898 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_899 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_900 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_901 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_902 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_903 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   6.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_904 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_905 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_906 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_907 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_908 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_909 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_910 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_911 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_912 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_913 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_914 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_915 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_916 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_917 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_918 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_919 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_920 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_921 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_922 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_923 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_924 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_925 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_926 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_927 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_928 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_929 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_930 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_931 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_932 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_933 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_934 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_935 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_936 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_937 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_938 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_939 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_940 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_941 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_942 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_943 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_944 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_945 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_946 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_947 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_948 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_949 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_950 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_951 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_952 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_953 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_954 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_955 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_956 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_957 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_958 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_959 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_960 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_961 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_962 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_963 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_964 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_965 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   5.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_966 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_967 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_968 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_969 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_970 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_971 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_972 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_973 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   5.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_974 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_975 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_976 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_977 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_978 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_979 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_980 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_981 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_982 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_983 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_984 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_985 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_986 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_987 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_988 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_989 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_990 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_991 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_992 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_993 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_994 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_995 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_996 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_997 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_998 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_999 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1000 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   5.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1001 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1002 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1003 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1004 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1005 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   6.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1006 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1007 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1008 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1009 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1010 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1011 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1012 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1013 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1014 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1015 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1016 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1017 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1018 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1019 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1020 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1021 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1022 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1023 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1024 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1025 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1026 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1027 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1028 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1029 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1030 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1031 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1032 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1033 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1034 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1035 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1036 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1037 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1038 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1039 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1040 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1041 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1042 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1043 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1044 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1045 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1046 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1047 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1048 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1049 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1050 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1051 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1052 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1053 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1054 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1055 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1056 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1057 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1058 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1059 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1060 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1061 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1062 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1063 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1064 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1065 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1066 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1067 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1068 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1069 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   5.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1070 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1071 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1072 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1073 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1074 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1075 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1076 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1077 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1078 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1079 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1080 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1081 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1082 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1083 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1084 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1085 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1086 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1087 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1088 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1089 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1090 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1091 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1092 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1093 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1094 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1095 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   5.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1096 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1097 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1098 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1099 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1100 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1101 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   6.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1102 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1103 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1104 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1105 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1106 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1107 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1108 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1109 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1110 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1111 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1112 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1113 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1114 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1115 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1116 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1117 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1118 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1119 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1120 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1121 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1122 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1123 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1124 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1125 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1126 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1127 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1128 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1129 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1130 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1131 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1132 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1133 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1134 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1135 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1136 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1137 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1138 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1139 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1140 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1141 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1142 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1143 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1144 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1145 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1146 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1147 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1148 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1149 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1150 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1151 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1152 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1153 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1154 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1155 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1156 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1157 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1158 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1159 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1160 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1161 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1162 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1163 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1164 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1165 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1166 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1167 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1168 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   5.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1169 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1170 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1171 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   4.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1172 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1173 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1174 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1175 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1176 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1177 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1178 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1179 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1180 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1181 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1182 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1183 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   4.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1184 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1185 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1186 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   4.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1187 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1188 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1189 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1190 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1191 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1192 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1193 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1194 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1195 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   4.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1196 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1197 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1198 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   5.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1199 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1200 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1201 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1202 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1203 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1204 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1205 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1206 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1207 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   4.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1208 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1209 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1210 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1211 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1212 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1213 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   4.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1214 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1215 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1216 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   4.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1217 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1218 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1219 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1220 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1221 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1222 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   4.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1223 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1224 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1225 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1226 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1227 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1228 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1229 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1230 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1231 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1232 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1233 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1234 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1235 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1236 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1237 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1238 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1239 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1240 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1241 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1242 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1243 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1244 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1245 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1246 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   4.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1247 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1248 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1249 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1250 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1251 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1252 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1253 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1254 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1255 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1256 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1257 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1258 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1259 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1260 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1261 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   4.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1262 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1263 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1264 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   4.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1265 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1266 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1267 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1268 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1269 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1270 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1271 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1272 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1273 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1274 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1275 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1276 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1277 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1278 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1279 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1280 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1281 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1282 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   4.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1283 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1284 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1285 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1286 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1287 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1288 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1289 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1290 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1291 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1292 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1293 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1294 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1295 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1296 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1297 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1298 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1299 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1300 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1301 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1302 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1303 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1304 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1305 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1306 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1307 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1308 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   3.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1309 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1310 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1311 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1312 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1313 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1314 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1315 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1316 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1317 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1318 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1319 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1320 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1321 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1322 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1323 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1324 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1325 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1326 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1327 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1328 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1329 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1330 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1331 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1332 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1333 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1334 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1335 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1336 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1337 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1338 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1339 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1340 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1341 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1342 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1343 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1344 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1345 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1346 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1347 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1348 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1349 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1350 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1351 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1352 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1353 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1354 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1355 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1356 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1357 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1358 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1359 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1360 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1361 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1362 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1363 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1364 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1365 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1366 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1367 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1368 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1369 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1370 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1371 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1372 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1373 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1374 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1375 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1376 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1377 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1378 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1379 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1380 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1381 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1382 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1383 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1384 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1385 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1386 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1387 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1388 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1389 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1390 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1391 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1392 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1393 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1394 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1395 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1396 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1397 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1398 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1399 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1400 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.1s\n",
      "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   9.2s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=  10.1s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.6s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   9.0s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   9.0s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.9s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.2s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.9s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.1s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.0s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.8s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.1s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.1s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.1s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.1s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.0s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.9s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.9s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.9s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.9s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.2s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.9s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   9.1s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   9.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   9.5s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   9.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   9.0s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   9.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   9.0s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.0s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.8s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.5s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   6.9s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.5s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.1s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   6.9s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.1s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.9s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.6s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.5s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   6.9s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.5s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   6.9s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.2s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   6.9s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.2s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   6.8s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.8s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.6s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.6s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.0s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.6s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   5.5s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   5.8s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   5.8s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   5.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.0s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.6s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   6.0s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.0s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.1s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   5.9s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   5.5s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.0s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.0s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.2s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.9s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.0s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.6s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.0s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   5.6s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   5.5s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.0s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   5.5s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.6s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.5s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.5s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.8s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.2s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.2s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.1s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.5s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.9s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.0s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.8s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.2s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.1s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.6s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.2s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   3.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.1s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   3.8s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.2s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   3.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.1s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   3.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.0s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   3.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.1s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.6s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.0s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.2s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.9s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.8s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.5s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.8s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.1s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   3.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.1s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   3.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   3.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.8s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.1s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.1s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   3.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.1s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   3.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   3.8s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.2s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   3.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.0s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   3.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.0s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   3.7s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.9s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.9s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.9s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.2s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.8s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.4s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.3s\n",
      "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.8s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   9.9s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   9.7s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   9.8s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   9.8s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   9.5s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=  10.3s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   9.6s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   9.4s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=  10.2s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   9.8s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.6s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.7s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.8s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.7s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.8s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=  10.2s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=  10.3s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=  10.2s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=  10.2s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=  11.6s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=  10.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.9s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.9s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.9s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=  10.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.9s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.9s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.8s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=  10.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=  10.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   9.5s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=  10.2s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   9.7s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   9.8s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   9.8s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   9.7s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   9.8s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   9.3s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=  10.2s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   9.7s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=  11.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=  10.2s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=  10.3s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   9.7s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=  10.6s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=  10.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=  10.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   9.9s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=  10.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=  10.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.7s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.9s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.9s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.9s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.8s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.6s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.9s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.4s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.2s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.4s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.4s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.3s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.5s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.8s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.9s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.6s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.9s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.8s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.9s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.5s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.9s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.4s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.2s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.3s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.4s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.4s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.3s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.8s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.5s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.3s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.5s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.4s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.6s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   6.3s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   6.5s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.6s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   6.2s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.3s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   5.7s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.3s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.2s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.6s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.7s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.2s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.5s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.8s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.2s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.8s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.2s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.3s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.4s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.7s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.5s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.3s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.7s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.2s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   6.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.8s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   6.5s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   6.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.7s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.4s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.7s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.3s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.8s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.8s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.3s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.6s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.4s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   5.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.5s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.4s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.5s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.4s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.4s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.9s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.8s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.2s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.6s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.4s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.3s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.6s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.4s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.5s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.4s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.5s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.5s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.3s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.4s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.4s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.4s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.2s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.6s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.1s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.6s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.3s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.4s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.5s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.0s\n",
      "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.7s\n"
     ]
    }
   ],
   "source": [
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46d01e3-41a5-4369-84e2-6dc3f5b1976e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d46d01e3-41a5-4369-84e2-6dc3f5b1976e",
    "outputId": "5fba39c9-1745-4a22-db57-c72fc8f7ae5e",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([4.94819379, 4.92124991, 4.98324423, 5.22386875, 4.77248521,\n",
       "        4.75072613, 4.67256632, 4.61011233, 5.24086461, 5.00736651,\n",
       "        3.94882131, 3.89164729, 3.84395447, 4.23850641, 3.9753469 ,\n",
       "        3.9534544 , 3.8687252 , 3.82825971, 4.40876946, 3.92900877,\n",
       "        3.07817745, 2.99073162, 2.89260201, 3.29045901, 2.98132415,\n",
       "        2.92830539, 2.96136379, 2.99288259, 3.13214078, 3.16562767,\n",
       "        2.18534522, 2.10531325, 2.11448932, 2.29335666, 2.02930427,\n",
       "        2.20381064, 2.12618113, 2.05535188, 2.27125916, 2.11817064,\n",
       "        6.9550415 , 6.73892193, 6.47470922, 7.32712417, 7.14411292,\n",
       "        7.08559074, 6.32410965, 6.71722054, 6.97709999, 7.10189133,\n",
       "        5.12124715, 5.1014461 , 5.15415907, 5.84869766, 5.11562009,\n",
       "        5.02660708, 5.0588623 , 5.14128861, 5.63733301, 5.30480008,\n",
       "        4.36234236, 4.33965964, 4.05626006, 4.34725108, 3.93282399,\n",
       "        4.23629298, 4.13673615, 4.04082627, 4.35143161, 4.23468914,\n",
       "        2.75466423, 2.89158816, 2.6426569 , 2.93391261, 2.7086132 ,\n",
       "        2.82291341, 2.66874838, 2.67332182, 2.89869084, 2.85379529,\n",
       "        7.22828813, 7.09227085, 7.05980096, 7.49937401, 7.09053464,\n",
       "        7.37044687, 7.11842585, 7.05161858, 7.50609341, 7.23551011,\n",
       "        6.07059922, 5.86767306, 5.84408975, 6.19585681, 5.97620101,\n",
       "        5.90424638, 6.14238043, 5.82799978, 6.36126518, 5.91726222,\n",
       "        4.5785181 , 4.58062534, 4.74396853, 4.8968564 , 4.63207192,\n",
       "        4.63105221, 4.5046474 , 4.43844409, 4.78367009, 4.85311046,\n",
       "        3.22178092, 3.01541905, 3.08952818, 3.23945956, 3.05375648,\n",
       "        3.24802771, 3.07297626, 3.01897244, 3.31590576, 3.19481025,\n",
       "        7.82100534, 8.11679621, 7.65705099, 8.50513535, 7.94210243,\n",
       "        7.81555719, 7.86963878, 7.79764967, 8.50899982, 8.35786729,\n",
       "        6.64758039, 6.66963177, 6.8316515 , 6.96389461, 6.60628405,\n",
       "        6.6362977 , 6.59319844, 6.6221149 , 7.02753863, 6.58788295,\n",
       "        5.22958837, 5.02030611, 5.01176763, 5.57041473, 4.94591007,\n",
       "        5.02972898, 5.00276999, 4.83394642, 5.56133094, 5.02116661,\n",
       "        3.74298606, 3.46606617, 3.34673467, 3.66197171, 3.48966603,\n",
       "        3.56590772, 3.35862861, 3.52146378, 3.64991345, 3.54357648]),\n",
       " 'std_fit_time': array([0.17228004, 0.40925488, 0.403983  , 0.19810373, 0.19524084,\n",
       "        0.18971121, 0.19497464, 0.21624066, 0.23382504, 0.28574477,\n",
       "        0.20348934, 0.16847113, 0.1912235 , 0.22030041, 0.18085687,\n",
       "        0.21113148, 0.21087336, 0.16826628, 0.53898239, 0.1981874 ,\n",
       "        0.20387628, 0.19323189, 0.16919152, 0.17655491, 0.18929969,\n",
       "        0.02270238, 0.20369636, 0.19788467, 0.0400491 , 0.42060355,\n",
       "        0.24156604, 0.14741788, 0.18834888, 0.16276355, 0.03039905,\n",
       "        0.17583578, 0.1713409 , 0.15138565, 0.15348717, 0.14925017,\n",
       "        0.49336773, 0.71196999, 0.49426476, 0.62239554, 0.66988387,\n",
       "        0.71179007, 0.1978513 , 0.60687352, 0.37253817, 0.45215352,\n",
       "        0.16153674, 0.16252047, 0.26327026, 0.82493761, 0.2494992 ,\n",
       "        0.1594498 , 0.29361948, 0.29078787, 0.25326642, 0.2133087 ,\n",
       "        0.5805235 , 0.45771727, 0.38466279, 0.38511895, 0.22618052,\n",
       "        0.04175178, 0.39265073, 0.16234739, 0.17144703, 0.06534441,\n",
       "        0.21997481, 0.4345463 , 0.17523482, 0.20413321, 0.12065273,\n",
       "        0.20896741, 0.1445389 , 0.19119516, 0.20907444, 0.1503363 ,\n",
       "        0.65696697, 0.22284846, 0.22821847, 0.21419341, 0.25034965,\n",
       "        0.18374872, 0.19890089, 0.21942948, 0.2533987 , 0.43888863,\n",
       "        0.27144221, 0.27939967, 0.18911229, 0.28623955, 0.16522271,\n",
       "        0.24732734, 0.50382947, 0.21595482, 0.03770367, 0.22101654,\n",
       "        0.25234517, 0.20224394, 0.28147497, 0.24813567, 0.24370907,\n",
       "        0.28896348, 0.1735082 , 0.2128184 , 0.23095953, 0.265424  ,\n",
       "        0.19588815, 0.14583962, 0.19417157, 0.2408365 , 0.16412144,\n",
       "        0.21147286, 0.17626156, 0.14074509, 0.22492262, 0.23069631,\n",
       "        0.17235981, 0.39436309, 0.0663165 , 0.72224454, 0.20184894,\n",
       "        0.08516845, 0.28443236, 0.27877225, 0.4134897 , 0.03191641,\n",
       "        0.16123705, 0.08486345, 0.46415802, 0.1899065 , 0.20905014,\n",
       "        0.15851668, 0.16953313, 0.18310157, 0.07841463, 0.37419635,\n",
       "        0.1915791 , 0.33541106, 0.20783193, 0.34592037, 0.21211502,\n",
       "        0.15986904, 0.31652171, 0.32809035, 0.18485024, 0.20031093,\n",
       "        0.35012173, 0.20512334, 0.16357563, 0.23944995, 0.22795382,\n",
       "        0.22841594, 0.17653614, 0.19319034, 0.19765582, 0.22596752]),\n",
       " 'mean_score_time': array([0.73408332, 0.76565909, 0.78421106, 0.76572223, 0.74590616,\n",
       "        0.82091031, 0.75102806, 0.81743226, 0.69612079, 0.75153837,\n",
       "        0.58958368, 0.58286309, 0.58403425, 0.58764114, 0.58856483,\n",
       "        0.58476872, 0.59264054, 0.5908299 , 0.58694463, 0.59241867,\n",
       "        0.48062   , 0.48390017, 0.47416887, 0.47602906, 0.55172219,\n",
       "        0.61415377, 0.48829021, 0.47880449, 0.53162446, 0.57056623,\n",
       "        0.37863374, 0.37143226, 0.37479067, 0.38549137, 0.43829079,\n",
       "        0.37331271, 0.36968474, 0.37333941, 0.37718749, 0.38267226,\n",
       "        0.69584551, 0.73263712, 0.75382724, 0.70732012, 0.67910123,\n",
       "        0.67483411, 0.69526529, 0.71201892, 0.75563183, 0.69052749,\n",
       "        0.6503068 , 0.70780058, 0.62835541, 0.73606591, 0.64997358,\n",
       "        0.74080205, 0.63649974, 0.63558865, 0.57935901, 0.56924996,\n",
       "        0.48154764, 0.48130593, 0.4738749 , 0.47941332, 0.54929528,\n",
       "        0.47576327, 0.47345028, 0.48444514, 0.48066802, 0.58611546,\n",
       "        0.36839843, 0.37156458, 0.36682601, 0.38413043, 0.3597939 ,\n",
       "        0.3774684 , 0.37409978, 0.38007121, 0.37051806, 0.36691523,\n",
       "        1.69861245, 1.63571944, 1.62565222, 1.68324294, 1.63418546,\n",
       "        1.64298625, 1.62982283, 1.63876796, 1.68431039, 1.64171662,\n",
       "        1.50962439, 1.39947987, 1.39463334, 1.31602249, 1.41220779,\n",
       "        1.31436334, 1.38400097, 1.30763569, 1.32189956, 1.40801821,\n",
       "        1.08347521, 1.04421883, 1.16972942, 1.10296526, 1.05618858,\n",
       "        1.0910316 , 1.08490505, 1.10583768, 1.0974297 , 1.08639059,\n",
       "        0.76207423, 0.80634985, 0.7351337 , 0.81132503, 0.81375251,\n",
       "        0.74018393, 0.81230092, 0.80977616, 0.75348387, 0.8994503 ,\n",
       "        1.92543716, 1.73819089, 2.07112212, 2.00150299, 1.99404931,\n",
       "        2.10923476, 1.92557034, 1.92524176, 1.88025355, 1.6314404 ,\n",
       "        1.30056291, 1.30942268, 1.29236312, 1.3836081 , 1.30141664,\n",
       "        1.38933234, 1.29646692, 1.47267103, 1.30556207, 1.38591728,\n",
       "        1.01063375, 1.00812712, 1.07423697, 1.08263245, 1.07848368,\n",
       "        1.14815168, 1.0810914 , 1.14299798, 1.02259851, 1.06849928,\n",
       "        0.79483867, 0.7364603 , 0.80230951, 0.77763314, 0.72741966,\n",
       "        0.72836714, 0.79679723, 0.73535662, 0.7215344 , 0.77612038]),\n",
       " 'std_score_time': array([0.03184307, 0.13268543, 0.14078678, 0.11243947, 0.10618252,\n",
       "        0.15266714, 0.09457137, 0.1594785 , 0.01851949, 0.12583813,\n",
       "        0.01900107, 0.00926509, 0.0132836 , 0.01415452, 0.009145  ,\n",
       "        0.01158458, 0.01350013, 0.00406997, 0.01030474, 0.01630067,\n",
       "        0.01150435, 0.00837561, 0.01363368, 0.00748613, 0.13217721,\n",
       "        0.17965931, 0.02779825, 0.00848747, 0.11871665, 0.12052568,\n",
       "        0.0133562 , 0.01036272, 0.01537662, 0.01174798, 0.1284906 ,\n",
       "        0.01340016, 0.0110881 , 0.01810278, 0.00969789, 0.00624144,\n",
       "        0.01806148, 0.12278135, 0.1181565 , 0.02630314, 0.01644281,\n",
       "        0.01974054, 0.01661166, 0.04227115, 0.13430084, 0.00556019,\n",
       "        0.16486307, 0.16417643, 0.10353238, 0.19062049, 0.17269664,\n",
       "        0.19187772, 0.10339639, 0.09899747, 0.00587817, 0.00685294,\n",
       "        0.00649587, 0.01118985, 0.01461136, 0.01040242, 0.13857356,\n",
       "        0.01365891, 0.01617411, 0.0195512 , 0.00780688, 0.12493422,\n",
       "        0.0186272 , 0.01743393, 0.02156901, 0.02398893, 0.00805724,\n",
       "        0.01489395, 0.0171031 , 0.01626191, 0.01538508, 0.02323387,\n",
       "        0.12708794, 0.11400674, 0.0908741 , 0.09431268, 0.08291795,\n",
       "        0.13588848, 0.09741517, 0.08714449, 0.09747496, 0.0876956 ,\n",
       "        0.2457015 , 0.1282802 , 0.14644587, 0.01561997, 0.15126066,\n",
       "        0.01853153, 0.13638414, 0.02120462, 0.02491419, 0.14906655,\n",
       "        0.12137789, 0.04253828, 0.22913399, 0.1094201 , 0.02492666,\n",
       "        0.12377036, 0.12503841, 0.11237566, 0.12508459, 0.0318948 ,\n",
       "        0.04038044, 0.13863604, 0.01864786, 0.14465761, 0.14711367,\n",
       "        0.00928167, 0.15258327, 0.14351637, 0.00974728, 0.29592753,\n",
       "        0.18814015, 0.1994006 , 0.03222193, 0.18176962, 0.1846199 ,\n",
       "        0.04238176, 0.20246431, 0.19437254, 0.2425087 , 0.02136771,\n",
       "        0.0234706 , 0.01290291, 0.01617148, 0.14040523, 0.01601339,\n",
       "        0.14793959, 0.01417998, 0.18909975, 0.01530564, 0.15607979,\n",
       "        0.00967208, 0.02069508, 0.13832348, 0.14401866, 0.1323269 ,\n",
       "        0.15540068, 0.12990499, 0.15333725, 0.00925871, 0.13405139,\n",
       "        0.11413403, 0.00634077, 0.15273198, 0.12142294, 0.02111266,\n",
       "        0.01416916, 0.12138321, 0.01076499, 0.00762891, 0.08241422]),\n",
       " 'param_activation': masked_array(data=['relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
       "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
       "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
       "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
       "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
       "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
       "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
       "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
       "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
       "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
       "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
       "                    'relu', 'relu', 'relu', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_batch_size': masked_array(data=[128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
       "                    128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
       "                    128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
       "                    128, 128, 128, 128, 128, 128, 128, 256, 256, 256, 256,\n",
       "                    256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256,\n",
       "                    256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256,\n",
       "                    256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256,\n",
       "                    256, 256, 256, 128, 128, 128, 128, 128, 128, 128, 128,\n",
       "                    128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
       "                    128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
       "                    128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 256,\n",
       "                    256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256,\n",
       "                    256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256,\n",
       "                    256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256,\n",
       "                    256, 256, 256, 256, 256, 256],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_hidden_layer_sizes': masked_array(data=[(50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
       "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
       "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
       "                    (50, 50, 50, 50), (50, 50, 50), (50, 50, 50),\n",
       "                    (50, 50, 50), (50, 50, 50), (50, 50, 50), (50, 50, 50),\n",
       "                    (50, 50, 50), (50, 50, 50), (50, 50, 50), (50, 50, 50),\n",
       "                    (50, 50), (50, 50), (50, 50), (50, 50), (50, 50),\n",
       "                    (50, 50), (50, 50), (50, 50), (50, 50), (50, 50),\n",
       "                    (50,), (50,), (50,), (50,), (50,), (50,), (50,), (50,),\n",
       "                    (50,), (50,), (50, 50, 50, 50), (50, 50, 50, 50),\n",
       "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
       "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
       "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50),\n",
       "                    (50, 50, 50), (50, 50, 50), (50, 50, 50), (50, 50, 50),\n",
       "                    (50, 50, 50), (50, 50, 50), (50, 50, 50), (50, 50, 50),\n",
       "                    (50, 50, 50), (50, 50), (50, 50), (50, 50), (50, 50),\n",
       "                    (50, 50), (50, 50), (50, 50), (50, 50), (50, 50),\n",
       "                    (50, 50), (50,), (50,), (50,), (50,), (50,), (50,),\n",
       "                    (50,), (50,), (50,), (50,), (50, 50, 50, 50),\n",
       "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
       "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
       "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
       "                    (50, 50, 50), (50, 50, 50), (50, 50, 50), (50, 50, 50),\n",
       "                    (50, 50, 50), (50, 50, 50), (50, 50, 50), (50, 50, 50),\n",
       "                    (50, 50, 50), (50, 50, 50), (50, 50), (50, 50),\n",
       "                    (50, 50), (50, 50), (50, 50), (50, 50), (50, 50),\n",
       "                    (50, 50), (50, 50), (50, 50), (50,), (50,), (50,),\n",
       "                    (50,), (50,), (50,), (50,), (50,), (50,), (50,),\n",
       "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
       "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
       "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
       "                    (50, 50, 50, 50), (50, 50, 50), (50, 50, 50),\n",
       "                    (50, 50, 50), (50, 50, 50), (50, 50, 50), (50, 50, 50),\n",
       "                    (50, 50, 50), (50, 50, 50), (50, 50, 50), (50, 50, 50),\n",
       "                    (50, 50), (50, 50), (50, 50), (50, 50), (50, 50),\n",
       "                    (50, 50), (50, 50), (50, 50), (50, 50), (50, 50),\n",
       "                    (50,), (50,), (50,), (50,), (50,), (50,), (50,), (50,),\n",
       "                    (50,), (50,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_nb_epoch': masked_array(data=[20, 20, 20, 20, 20, 25, 25, 25, 25, 25, 20, 20, 20, 20,\n",
       "                    20, 25, 25, 25, 25, 25, 20, 20, 20, 20, 20, 25, 25, 25,\n",
       "                    25, 25, 20, 20, 20, 20, 20, 25, 25, 25, 25, 25, 20, 20,\n",
       "                    20, 20, 20, 25, 25, 25, 25, 25, 20, 20, 20, 20, 20, 25,\n",
       "                    25, 25, 25, 25, 20, 20, 20, 20, 20, 25, 25, 25, 25, 25,\n",
       "                    20, 20, 20, 20, 20, 25, 25, 25, 25, 25, 20, 20, 20, 20,\n",
       "                    20, 25, 25, 25, 25, 25, 20, 20, 20, 20, 20, 25, 25, 25,\n",
       "                    25, 25, 20, 20, 20, 20, 20, 25, 25, 25, 25, 25, 20, 20,\n",
       "                    20, 20, 20, 25, 25, 25, 25, 25, 20, 20, 20, 20, 20, 25,\n",
       "                    25, 25, 25, 25, 20, 20, 20, 20, 20, 25, 25, 25, 25, 25,\n",
       "                    20, 20, 20, 20, 20, 25, 25, 25, 25, 25, 20, 20, 20, 20,\n",
       "                    20, 25, 25, 25, 25, 25],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_optimizer': masked_array(data=['adam', 'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
       "                    'adagrad', 'sgd', 'RMSprop', 'Adamax'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 128,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50, 50),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 20,\n",
       "   'optimizer': 'Adamax'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'adagrad'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'sgd'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'RMSprop'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 256,\n",
       "   'hidden_layer_sizes': (50,),\n",
       "   'nb_epoch': 25,\n",
       "   'optimizer': 'Adamax'}],\n",
       " 'split0_test_score': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.09375   , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.23125   , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.01041667, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0125    , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.20416667, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.08125   , 0.00625   , 0.        , 0.00416667, 0.        ,\n",
       "        0.20625   , 0.01875   , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00416667,\n",
       "        0.        , 0.01666667, 0.        , 0.02291667, 0.01666667,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.02291667, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00416667, 0.01041667, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.98958331, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00416667, 0.        , 0.        , 0.        ]),\n",
       " 'split1_test_score': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.32291666, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.33125001, 0.        , 0.03333334, 0.        ,\n",
       "        0.        , 0.20416667, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.03333334, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.20208333, 0.        , 0.14375   , 0.        ,\n",
       "        0.00625   , 0.52916664, 0.00416667, 0.33541667, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.28958333, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.32499999, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.21458334, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.22083333, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.24583334, 0.28541666, 0.        , 0.        ,\n",
       "        0.20416667, 0.00208333, 0.        , 0.33750001, 0.10625   ,\n",
       "        0.01458333, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.38958332, 0.00208333, 0.        , 0.00208333, 0.33750001,\n",
       "        0.16875   , 0.00208333, 0.        , 0.01666667, 0.23125   ,\n",
       "        0.36458334, 0.0125    , 0.        , 0.24375001, 0.04375   ,\n",
       "        0.18958333, 0.33541667, 0.02083333, 0.08333334, 0.34375   ,\n",
       "        0.40208334, 0.23958333, 0.        , 0.02708333, 0.19583334,\n",
       "        0.63125002, 0.03333334, 0.        , 0.01041667, 0.00416667,\n",
       "        0.33333334, 0.075     , 0.        , 0.        , 0.25208333,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.30000001,\n",
       "        0.38333333, 0.30208334, 0.        , 0.        , 0.00625   ,\n",
       "        0.00625   , 0.0875    , 0.0125    , 0.        , 0.        ,\n",
       "        0.0125    , 0.05      , 0.10208333, 0.        , 0.30833334,\n",
       "        0.20625   , 0.23541667, 0.        , 0.33333334, 0.18541667,\n",
       "        0.0125    , 0.        , 0.00208333, 0.02916667, 0.17916666,\n",
       "        0.03333334, 0.33333334, 0.08541667, 0.07083333, 0.03958333]),\n",
       " 'split2_test_score': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00208333, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00208333, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.23958333, 0.00416667, 0.        , 0.        ,\n",
       "        0.        , 0.70833331, 0.06875   , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00208333, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.19791667, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.0125    , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.01041667, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00416667, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.30000001, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.175     , 0.        , 0.        , 0.        ]),\n",
       " 'split3_test_score': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.3125    , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02708333, 0.19583334, 0.00208333, 0.25208333, 0.        ,\n",
       "        0.11666667, 0.07291666, 0.        , 0.26458332, 0.        ,\n",
       "        0.00833333, 0.29583332, 0.30208334, 0.07916667, 0.12708333,\n",
       "        0.2375    , 0.31458333, 0.36250001, 0.08333334, 0.00416667,\n",
       "        0.        , 0.025     , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00208333, 0.        , 0.20208333, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.31666666, 0.00625   , 0.30625001, 0.        ,\n",
       "        0.        , 0.        , 0.01875   , 0.27500001, 0.        ,\n",
       "        0.        , 0.00416667, 0.00208333, 0.08958333, 0.        ,\n",
       "        0.19166666, 0.27291667, 0.06458333, 0.06666667, 0.        ,\n",
       "        0.1       , 0.28333333, 0.        , 0.38333333, 0.13333334,\n",
       "        0.0625    , 0.        , 0.        , 0.2375    , 0.25833333,\n",
       "        0.00208333, 0.08125   , 0.        , 0.15625   , 0.33125001,\n",
       "        0.09375   , 0.06458333, 0.        , 0.00416667, 0.25416666,\n",
       "        0.1875    , 0.01666667, 0.        , 0.02083333, 0.09791667,\n",
       "        0.21041666, 0.15000001, 0.        , 0.27916667, 0.22291666,\n",
       "        0.21875   , 0.00625   , 0.00416667, 0.22916667, 0.175     ,\n",
       "        0.10208333, 0.00625   , 0.0125    , 0.2       , 0.13333334,\n",
       "        0.03541667, 0.        , 0.        , 0.        , 0.00416667,\n",
       "        0.15416667, 0.11458334, 0.        , 0.        , 0.12291667,\n",
       "        0.27083334, 0.54583335, 0.        , 0.05833333, 0.09166667,\n",
       "        0.18333334, 0.37916666, 0.        , 0.1       , 0.00208333,\n",
       "        0.28541666, 0.31458333, 0.00416667, 0.06666667, 0.01041667,\n",
       "        0.21250001, 0.25416666, 0.02916667, 0.        , 0.01458333,\n",
       "        0.12708333, 0.31458333, 0.35624999, 0.0375    , 0.17291667,\n",
       "        0.18333334, 0.34166667, 0.17083333, 0.02916667, 0.06875   ]),\n",
       " 'split4_test_score': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.75625002, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.0125    , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.15833333, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.03541667, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00416667, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.24791667, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.02708333, 0.00208333, 0.        , 0.        ]),\n",
       " 'mean_test_score': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.06458333, 0.        , 0.0625    , 0.        ,\n",
       "        0.        , 0.06625   , 0.        , 0.00666667, 0.        ,\n",
       "        0.00541667, 0.08      , 0.00041667, 0.05041667, 0.        ,\n",
       "        0.02333333, 0.02125   , 0.        , 0.05291666, 0.        ,\n",
       "        0.00166667, 0.25125   , 0.06041667, 0.04458333, 0.02541667,\n",
       "        0.04875   , 0.19041666, 0.07333334, 0.08375   , 0.00083333,\n",
       "        0.        , 0.005     , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.05833333, 0.        , 0.04041667, 0.        ,\n",
       "        0.        , 0.065     , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.06333333, 0.00125   , 0.06125   , 0.        ,\n",
       "        0.        , 0.13708333, 0.00458333, 0.055     , 0.        ,\n",
       "        0.        , 0.21833333, 0.01416667, 0.01791667, 0.        ,\n",
       "        0.03833333, 0.11291667, 0.07      , 0.01333333, 0.        ,\n",
       "        0.06083333, 0.05708333, 0.        , 0.14416667, 0.04791667,\n",
       "        0.01833333, 0.        , 0.        , 0.0475    , 0.05166667,\n",
       "        0.11916666, 0.01666667, 0.        , 0.03166667, 0.13375   ,\n",
       "        0.0525    , 0.01333333, 0.        , 0.00416667, 0.09708333,\n",
       "        0.12666667, 0.00708333, 0.        , 0.05375   , 0.02833333,\n",
       "        0.12125   , 0.10083334, 0.00416667, 0.0725    , 0.11333333,\n",
       "        0.16375   , 0.04916667, 0.00083333, 0.05125   , 0.075     ,\n",
       "        0.14666667, 0.01375   , 0.0025    , 0.04666667, 0.03083333,\n",
       "        0.07375   , 0.015     , 0.        , 0.        , 0.05125   ,\n",
       "        0.03291667, 0.02291667, 0.        , 0.00083333, 0.08458334,\n",
       "        0.13083333, 0.16958334, 0.        , 0.01166667, 0.01958333,\n",
       "        0.03791667, 0.09875   , 0.0025    , 0.02      , 0.00041667,\n",
       "        0.05958333, 0.07291667, 0.02125   , 0.01333333, 0.06375   ,\n",
       "        0.08458333, 0.14958333, 0.00583333, 0.06666667, 0.04      ,\n",
       "        0.02791667, 0.32083333, 0.07166666, 0.01333333, 0.07041667,\n",
       "        0.04333333, 0.17625   , 0.05166667, 0.02      , 0.02166667]),\n",
       " 'std_test_score': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.12916666, 0.        , 0.125     , 0.        ,\n",
       "        0.        , 0.1325    , 0.        , 0.01333333, 0.        ,\n",
       "        0.01083333, 0.09801502, 0.00083333, 0.10083333, 0.        ,\n",
       "        0.04666667, 0.02887954, 0.        , 0.10583333, 0.        ,\n",
       "        0.00333333, 0.27735921, 0.12083334, 0.05829761, 0.05083333,\n",
       "        0.09440604, 0.20329148, 0.14459234, 0.12990649, 0.00166667,\n",
       "        0.        , 0.01      , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.11562781, 0.        , 0.08083333, 0.        ,\n",
       "        0.        , 0.13      , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.12666667, 0.0025    , 0.1225    , 0.        ,\n",
       "        0.        , 0.11221724, 0.00726483, 0.11      , 0.        ,\n",
       "        0.        , 0.2597087 , 0.02730359, 0.03583333, 0.        ,\n",
       "        0.07666667, 0.12044046, 0.11057457, 0.02666667, 0.        ,\n",
       "        0.08146233, 0.11312788, 0.        , 0.17716126, 0.05930735,\n",
       "        0.02279894, 0.        , 0.        , 0.095     , 0.10333333,\n",
       "        0.15649924, 0.03230174, 0.        , 0.06229689, 0.16382155,\n",
       "        0.06853375, 0.0256377 , 0.        , 0.00645497, 0.11912295,\n",
       "        0.1374457 , 0.00666667, 0.        , 0.09531017, 0.03869844,\n",
       "        0.09924541, 0.13001469, 0.00833333, 0.10825638, 0.14396783,\n",
       "        0.15140889, 0.0952391 , 0.00166667, 0.08957461, 0.09040803,\n",
       "        0.24549625, 0.01130388, 0.005     , 0.07712949, 0.05161288,\n",
       "        0.13051448, 0.03      , 0.        , 0.        , 0.10042963,\n",
       "        0.06075909, 0.04583333, 0.        , 0.00166667, 0.11775976,\n",
       "        0.16413917, 0.22153819, 0.        , 0.02333333, 0.03612286,\n",
       "        0.07274862, 0.14367208, 0.005     , 0.04      , 0.00083333,\n",
       "        0.1130204 , 0.12237522, 0.04044887, 0.02666667, 0.1223582 ,\n",
       "        0.10192249, 0.11808219, 0.01166667, 0.13333334, 0.07292738,\n",
       "        0.04981912, 0.36154327, 0.14229395, 0.01654119, 0.08626509,\n",
       "        0.07118052, 0.14415463, 0.06802522, 0.02781387, 0.02809335]),\n",
       " 'rank_test_score': array([109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109,  37, 109,\n",
       "         40, 109, 109,  35, 109,  93, 109,  95,  25, 107,  55, 109,  73,\n",
       "         77, 109,  49, 109, 102,   2,  43,  61,  72,  57,   4,  28,  24,\n",
       "        104, 109,  96, 109, 109, 109, 109, 109, 109, 109, 109, 109,  45,\n",
       "        109,  63, 109, 109,  36, 109, 109, 109, 109,  39, 103,  41, 109,\n",
       "        109,  11,  97,  47, 109, 109,   3,  85,  82, 109,  65,  18,  33,\n",
       "         87, 109,  42,  46, 109,  10,  58,  81, 109, 109,  59,  52,  16,\n",
       "         83, 109,  68,  12,  50,  90, 109,  98,  21,  14,  92, 109,  48,\n",
       "         70,  15,  19,  99,  30,  17,   7,  56, 104,  53,  26,   9,  86,\n",
       "        100,  60,  69,  27,  84, 109, 109,  54,  67,  74, 109, 104,  22,\n",
       "         13,   6, 109,  91,  80,  66,  20, 100,  78, 107,  44,  29,  76,\n",
       "         87,  38,  23,   8,  94,  34,  64,  71,   1,  31,  89,  32,  62,\n",
       "          5,  51,  79,  75], dtype=int32)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zZiUqBJ2CP8g",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zZiUqBJ2CP8g",
    "outputId": "ecf01cff-eb2d-4926-dd09-855e8af623de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = grid_result.cv_results_['std_test_score'] < 0.1\n",
    "np.argmax(grid_result.cv_results_['mean_test_score'][mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e1f12-84e4-46fa-a483-7b7052858bd4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "218e1f12-84e4-46fa-a483-7b7052858bd4",
    "outputId": "ec0e0ea6-54ee-46fb-9683-472b36837cc2",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.00041666668839752675,\n",
       " 0.00041666668839752675,\n",
       " 0.0008333333767950535,\n",
       " 0.0008333333767950535,\n",
       " 0.0008333333767950535,\n",
       " 0.0012500000186264515,\n",
       " 0.001666666753590107,\n",
       " 0.002500000037252903,\n",
       " 0.002500000037252903,\n",
       " 0.00416666679084301,\n",
       " 0.004166666883975267,\n",
       " 0.004583333525806666,\n",
       " 0.005000000074505806,\n",
       " 0.005416666716337204,\n",
       " 0.005833333358168602,\n",
       " 0.006666667014360428,\n",
       " 0.007083333563059568,\n",
       " 0.011666666716337204,\n",
       " 0.013333332957699895,\n",
       " 0.013333333656191826,\n",
       " 0.013333334028720856,\n",
       " 0.013333334028720856,\n",
       " 0.013750000577419996,\n",
       " 0.01416666698642075,\n",
       " 0.015000000596046448,\n",
       " 0.01666666609235108,\n",
       " 0.017916665971279146,\n",
       " 0.018333333404734732,\n",
       " 0.019583333749324083,\n",
       " 0.019999999925494193,\n",
       " 0.020000000298023225,\n",
       " 0.02124999985098839,\n",
       " 0.021249999944120645,\n",
       " 0.021666666865348815,\n",
       " 0.02291666716337204,\n",
       " 0.023333333432674408,\n",
       " 0.02541666626930237,\n",
       " 0.02791666630655527,\n",
       " 0.028333333879709245,\n",
       " 0.03083333494141698,\n",
       " 0.03166666668839753,\n",
       " 0.03291666712611914,\n",
       " 0.03791666748002172,\n",
       " 0.038333332538604735,\n",
       " 0.04000000040978193,\n",
       " 0.04041666686534882,\n",
       " 0.04333333447575569,\n",
       " 0.044583332538604734,\n",
       " 0.04666666742414236,\n",
       " 0.04749999940395355,\n",
       " 0.04791666865348816,\n",
       " 0.04874999942258,\n",
       " 0.04916666569188237,\n",
       " 0.05041666626930237,\n",
       " 0.05124999964609742,\n",
       " 0.051250001043081285,\n",
       " 0.051666665077209475,\n",
       " 0.05166666698642075,\n",
       " 0.052500000596046446,\n",
       " 0.05291666388511658,\n",
       " 0.05375000135973096,\n",
       " 0.0550000011920929,\n",
       " 0.05708333295769989,\n",
       " 0.058333331765607,\n",
       " 0.05958333257585764,\n",
       " 0.06041666865348816,\n",
       " 0.06083333343267441,\n",
       " 0.061250001192092896,\n",
       " 0.0625,\n",
       " 0.06333333253860474,\n",
       " 0.06375000085681677,\n",
       " 0.06458333134651184,\n",
       " 0.06499999761581421,\n",
       " 0.0662500023841858,\n",
       " 0.06666666865348816,\n",
       " 0.06999999880790711,\n",
       " 0.07041666507720948,\n",
       " 0.07166666430421173,\n",
       " 0.07250000089406967,\n",
       " 0.07291666641831399,\n",
       " 0.07333333576098085,\n",
       " 0.07375000193715095,\n",
       " 0.07500000083819032,\n",
       " 0.0800000011920929,\n",
       " 0.08375000208616257,\n",
       " 0.0845833339728415,\n",
       " 0.08458333611488342,\n",
       " 0.09708333313465119,\n",
       " 0.09874999905005097,\n",
       " 0.10083333626389504,\n",
       " 0.1129166690632701,\n",
       " 0.11333333253860474,\n",
       " 0.11916666370816528,\n",
       " 0.1212499976158142,\n",
       " 0.1266666680574417,\n",
       " 0.13083333373069764,\n",
       " 0.1337500035762787,\n",
       " 0.13708333373069764,\n",
       " 0.14416666626930236,\n",
       " 0.14666667133569716,\n",
       " 0.14958333279937505,\n",
       " 0.16375000178813934,\n",
       " 0.1695833384990692,\n",
       " 0.1762500018812716,\n",
       " 0.19041666076518596,\n",
       " 0.21833332860842347,\n",
       " 0.2512500022072345,\n",
       " 0.3208333313465118]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(grid_result.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kfr2aPpdAlQp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kfr2aPpdAlQp",
    "outputId": "43899fa3-7f7c-4e8c-ab2f-043c7993ad68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'tanh',\n",
       " 'batch_size': 128,\n",
       " 'hidden_layer_sizes': (50, 50),\n",
       " 'nb_epoch': 20,\n",
       " 'optimizer': 'adam'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f0a4b8-17e1-4569-bb6e-f3578e83f3f9",
   "metadata": {
    "id": "03f0a4b8-17e1-4569-bb6e-f3578e83f3f9"
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('/content/drive/MyDrive/Colab Notebooks/nlp_project/grid_result_best_params1.pickle', 'wb') as f:\n",
    "    pickle.dump(grid_result.best_params_, f)\n",
    "    \n",
    "# text_transformed2 = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/nlp_project/text_transformed_window_size15_sample_size1000.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EllPxuj68BQu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EllPxuj68BQu",
    "outputId": "e202cb57-ed17-432d-ad6a-366baf97bb42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10/10 [==============================] - 3s 19ms/step - loss: 1.1005 - accuracy: 0.3179\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 1.0997 - accuracy: 0.3196\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 1.1007 - accuracy: 0.3142\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 1.0998 - accuracy: 0.3271\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 1.0988 - accuracy: 0.3425\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 1.0994 - accuracy: 0.3212\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 1.0993 - accuracy: 0.3317\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 1.0989 - accuracy: 0.3283\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 1.0986 - accuracy: 0.3363\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 1.0991 - accuracy: 0.3329\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 1.0994 - accuracy: 0.3279\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 1.0984 - accuracy: 0.3363\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 1.0981 - accuracy: 0.3429\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 1.0982 - accuracy: 0.3421\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 1.0981 - accuracy: 0.3542\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 1.0987 - accuracy: 0.3379\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 1.0980 - accuracy: 0.3408\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 1.0978 - accuracy: 0.3433\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 1.0980 - accuracy: 0.3479\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 1.0984 - accuracy: 0.3338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f36ad5b6460>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = language_model(activation=grid_result.best_params_['activation'], \n",
    "                   optimizer=grid_result.best_params_['optimizer'], \n",
    "                   hidden_layer_sizes=grid_result.best_params_['hidden_layer_sizes'])\n",
    "\n",
    "model2.fit(X_train, y_train, epochs=grid_result.best_params_['nb_epoch'], batch_size=grid_result.best_params_['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hq2yb0Fz8bKG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hq2yb0Fz8bKG",
    "outputId": "db4e570c-dbbb-4819-b2e5-2ed2e86c08d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 1s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3466666666666667"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(np.argmax(model2.predict(X_test), axis=1), np.argmax(y_test, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K5P1UFOt8QMN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5P1UFOt8QMN",
    "outputId": "1cce32f0-c0f3-4786-9027-bf770437ab58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.32899114, 0.34335256, 0.32765627],\n",
       "       [0.32988042, 0.3293508 , 0.34076878],\n",
       "       [0.3271588 , 0.34085882, 0.33198237],\n",
       "       ...,\n",
       "       [0.32931495, 0.34006554, 0.3306195 ],\n",
       "       [0.3329972 , 0.3345674 , 0.3324354 ],\n",
       "       [0.32666722, 0.33503953, 0.33829325]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc5c1a5-8eb0-4bf4-befa-9c1d383cc028",
   "metadata": {
    "id": "5dc5c1a5-8eb0-4bf4-befa-9c1d383cc028"
   },
   "outputs": [],
   "source": [
    "# for early stopping\n",
    "# https://qiita.com/sasayabaku/items/b7872a3b8acc7d6261bf\n",
    "\n",
    "# stacked lstm model example\n",
    "# https://machinelearningknowledge.ai/keras-lstm-layer-explained-for-beginners-with-example/\n",
    "\n",
    "# lstm layer documentation\n",
    "# https://keras.io/api/layers/recurrent_layers/lstm/\n",
    "\n",
    "# Should you use relu for activation in lstm\n",
    "# https://stats.stackexchange.com/questions/444923/activation-function-between-lstm-layers\n",
    "\n",
    "# lstm explained\n",
    "# https://qiita.com/t_Signull/items/21b82be280b46f467d1b\n",
    "\n",
    "# stacked lstm explained\n",
    "# https://machinelearningmastery.com/stacked-long-short-term-memory-networks/\n",
    "\n",
    "# error solved\n",
    "# https://stackoverflow.com/questions/58119320/valueerror-input-0-of-layer-lstm-is-incompatible-with-the-layer-expected-ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21014f0-96c6-4124-898f-228bc07ef250",
   "metadata": {
    "id": "f21014f0-96c6-4124-898f-228bc07ef250"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24467e8a-3f01-459a-a198-4b6deaa3f78c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24467e8a-3f01-459a-a198-4b6deaa3f78c",
    "outputId": "f3442960-e825-4825-cadf-b398f7ae773f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0836329days (2.00719h)\n"
     ]
    }
   ],
   "source": [
    "!cat /proc/uptime | awk '{print $1 /60 /60 /24 \"days (\" $1 / 60 / 60 \"h)\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hc_3KoTo-CdX",
   "metadata": {
    "id": "Hc_3KoTo-CdX"
   },
   "outputs": [],
   "source": [
    "params = {'activation': 'tanh',\n",
    " 'batch_size': 128,\n",
    " 'hidden_layer_sizes': (50, 50),\n",
    " 'nb_epoch': 20,\n",
    " 'optimizer': 'adam'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n3ACfYqKAGO4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3ACfYqKAGO4",
    "outputId": "ff33ace0-3aaf-4b29-c681-a5757d3feff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "19/19 [==============================] - 5s 18ms/step - loss: 1.0728 - accuracy: 0.4187\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 0s 12ms/step - loss: 1.0115 - accuracy: 0.4529\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 0.9750 - accuracy: 0.5071\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 0.9525 - accuracy: 0.5121\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 0.9291 - accuracy: 0.5221\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 0s 12ms/step - loss: 0.9039 - accuracy: 0.5412\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 0.9035 - accuracy: 0.5442\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 0.8892 - accuracy: 0.5592\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 0.9078 - accuracy: 0.5521\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 0.8461 - accuracy: 0.5800\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 0.8117 - accuracy: 0.6075\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 0.7826 - accuracy: 0.6271\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 0.7967 - accuracy: 0.6079\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 0.7431 - accuracy: 0.6558\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 0.7291 - accuracy: 0.6371\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 0.6837 - accuracy: 0.6833\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 0.6631 - accuracy: 0.6992\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 0.6256 - accuracy: 0.7146\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 0.5990 - accuracy: 0.7271\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 0.5800 - accuracy: 0.7221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f36ad377d90>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = language_model(activation=params['activation'], \n",
    "                   optimizer=params['optimizer'], \n",
    "                   hidden_layer_sizes=params['hidden_layer_sizes'])\n",
    "\n",
    "model3.fit(X_train, y_train, epochs=params['nb_epoch'], batch_size=params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gYC4V_2dARYo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gYC4V_2dARYo",
    "outputId": "cf30cd79-84fc-4672-aed1-a16875863eae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 1s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5466666666666666"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(model3.predict(X_test), axis=1), np.argmax(y_test, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ej798su8HONS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ej798su8HONS",
    "outputId": "f3a37ed9-6190-46a0-e5b4-d8dedf46ddf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.3287083e-01, 4.6697378e-01, 1.5539600e-04],\n",
       "       [3.9313397e-01, 4.1999364e-01, 1.8687238e-01],\n",
       "       [4.2869858e-02, 3.6385888e-01, 5.9327132e-01],\n",
       "       ...,\n",
       "       [4.5412302e-04, 2.6838525e-04, 9.9927753e-01],\n",
       "       [7.0622128e-01, 2.9362595e-01, 1.5275500e-04],\n",
       "       [6.4372247e-01, 3.5624692e-01, 3.0594765e-05]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iMCrIHknJh21",
   "metadata": {
    "id": "iMCrIHknJh21"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2Lhli50MJh5A",
   "metadata": {
    "id": "2Lhli50MJh5A"
   },
   "outputs": [],
   "source": [
    "def language_model2(optimizer=\"adam\", hidden_layer_sizes=(100, 100)):\n",
    "    model = Sequential()\n",
    "    firstflag = True\n",
    "    for dim in hidden_layer_sizes:\n",
    "        if firstflag:\n",
    "            model.add(LSTM(dim, input_shape=(n_rnn, data_dim), return_sequences=True))\n",
    "            firstflag = False\n",
    "        else:\n",
    "            model.add(LSTM(dim, return_sequences=True))\n",
    "            # model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(n_mid))\n",
    "    model.add(Dense(output_dim, activation=\"softmax\"))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "oBBeHPAaJjLD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBBeHPAaJjLD",
    "outputId": "237ba05e-bd8e-416f-9e5f-5bd0dabaaa82"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-c76952355f90>:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model5 = KerasClassifier(build_fn=language_model2, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "model5 = KerasClassifier(build_fn=language_model2, verbose=0)\n",
    "param_grid2 = dict(optimizer=optimizer, \n",
    "                  hidden_layer_sizes=hidden_layer_sizes, \n",
    "                  nb_epoch=nb_epoch, \n",
    "                  batch_size=batch_size,)\n",
    "grid3 = GridSearchCV(estimator=model5, param_grid=param_grid2, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "MYoElLCDJwNr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYoElLCDJwNr",
    "outputId": "638589fe-6ab4-493f-8a14-01edfdfa0131",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.7s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.8s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   9.2s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.9s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.5s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.7s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.8s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.0s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   9.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.8s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.4s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.9s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.2s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.0s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.5s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.1s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.0s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.4s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.2s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.7s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.9s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   9.9s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   9.2s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   9.1s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.5s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.7s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   9.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.8s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   9.4s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   9.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.8s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   9.0s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.4s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   9.0s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.2s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.4s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.1s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.2s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.2s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.2s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.0s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.2s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   6.8s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.5s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.2s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.1s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.7s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.7s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.9s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.7s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   6.7s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.4s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.2s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.4s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.4s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.9s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   6.8s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.0s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   6.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.7s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.9s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.4s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.9s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.4s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.9s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   5.5s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   5.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   5.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   5.1s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   5.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.5s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.2s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.7s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.5s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.0s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   5.9s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   5.1s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   5.5s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   5.5s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   5.0s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.8s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.7s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.4s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.8s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.7s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.2s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.8s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.2s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.7s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   5.4s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   5.2s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.0s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   5.2s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   5.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.5s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.1s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.7s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.4s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.2s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.7s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.4s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.1s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.1s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.8s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.8s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.5s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.1s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.7s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.1s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.1s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   3.8s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.1s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.1s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   3.9s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   3.7s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   3.7s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.1s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   3.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.9s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.5s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.0s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.0s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.9s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.1s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.8s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.8s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   3.7s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   3.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.1s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   3.8s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.1s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.0s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.0s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   3.7s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.0s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   3.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.0s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   3.7s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   3.8s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.0s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   3.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   3.9s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.8s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.1s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.9s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.8s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.3s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.6s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.1s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.8s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.9s\n",
      "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   9.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=  10.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f06c66c78b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   9.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f06c76ddd30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   9.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   9.2s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   9.7s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   9.5s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   9.4s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   9.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   9.8s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.5s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.3s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.9s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.4s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.9s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.7s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.8s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=  10.2s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.5s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.2s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=  10.4s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.8s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.5s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=  10.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.7s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.4s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   9.8s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   9.7s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   9.7s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   9.7s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   9.4s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   9.5s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   9.5s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   9.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   9.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   9.5s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=  10.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=  10.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=  10.9s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=  10.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=  10.2s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   9.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   9.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   9.2s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=  10.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   9.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.2s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.3s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.2s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.9s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.7s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.3s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.7s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.3s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.5s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.5s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.4s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.3s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.8s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.2s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.8s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.7s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.9s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.7s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.5s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.5s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.8s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.8s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   9.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.4s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.5s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.3s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   5.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.4s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.8s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   6.2s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.9s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.8s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.7s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   5.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.2s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.9s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.2s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.2s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.2s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.8s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.7s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.8s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   5.9s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   5.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   5.5s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.9s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.2s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.5s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   6.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.9s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.9s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.4s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.2s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.8s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.4s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.3s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.5s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.5s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.2s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.5s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.3s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.3s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.4s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.4s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.2s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.9s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.8s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.4s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.5s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.1s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.4s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.3s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.4s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.3s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.9s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.5s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.9s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.4s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.3s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.5s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   3.8s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.5s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.3s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   3.9s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   3.9s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.4s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   3.9s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.4s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.4s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.3s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.0s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.6s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.4s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.2s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.9s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.8s\n",
      "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.1s\n"
     ]
    }
   ],
   "source": [
    "grid3_result = grid3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "TEPoG5ZVJ5ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TEPoG5ZVJ5ca",
    "outputId": "43bb07c2-58d5-4301-8eec-44f472a8a4f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 256,\n",
       " 'hidden_layer_sizes': (50, 50),\n",
       " 'nb_epoch': 20,\n",
       " 'optimizer': 'adagrad'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid3_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "J-GhuIe5UrSp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J-GhuIe5UrSp",
    "outputId": "1bbcf59e-72c4-4e1a-ec1d-8894e2536057"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4762499988079071"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid3_result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "hMJgXb0wE6QY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hMJgXb0wE6QY",
    "outputId": "0455547a-9d1f-400f-a0d0-0069f553da65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.91666657e-03, 8.33333377e-04, 0.00000000e+00, 1.25000000e-02,\n",
       "       0.00000000e+00, 3.04166679e-02, 2.91666668e-03, 0.00000000e+00,\n",
       "       4.83333319e-02, 6.25000000e-03, 5.83333351e-02, 5.12499996e-02,\n",
       "       0.00000000e+00, 6.62500024e-02, 6.00000028e-02, 4.33333345e-02,\n",
       "       4.16666688e-04, 0.00000000e+00, 5.66666675e-02, 0.00000000e+00,\n",
       "       4.33333337e-02, 1.20833334e-02, 8.33333377e-04, 1.37500000e-01,\n",
       "       4.62500006e-02, 5.87499991e-02, 9.20833319e-02, 8.33333377e-04,\n",
       "       1.58333331e-02, 3.66666675e-02, 3.24999992e-02, 1.53750001e-01,\n",
       "       4.58333353e-03, 4.54166675e-02, 5.00000000e-02, 8.62500012e-02,\n",
       "       2.09166671e-01, 0.00000000e+00, 7.62499988e-02, 5.75000003e-02,\n",
       "       2.08333340e-03, 3.16666679e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 8.33333328e-02, 0.00000000e+00,\n",
       "       5.87499976e-02, 0.00000000e+00, 0.00000000e+00, 1.22916671e-01,\n",
       "       8.33333377e-04, 7.08333328e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       8.33333377e-04, 4.16666688e-04, 5.74999988e-02, 1.95833340e-02,\n",
       "       4.41666678e-02, 4.76249999e-01, 3.79166670e-02, 2.37499997e-02,\n",
       "       6.66666701e-03, 4.95833337e-02, 2.66250005e-01, 1.58333331e-02,\n",
       "       8.20833325e-02, 0.00000000e+00, 5.54166643e-02, 2.29166659e-01,\n",
       "       8.74999976e-02, 2.24999994e-02, 2.91666668e-03, 3.08333345e-02,\n",
       "       2.58749992e-01, 5.83333345e-03, 6.87500015e-02, 2.75000006e-02])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid3_result.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e08faf-9e69-4f17-aee7-9fe85a21b80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cYQ-tPUiB0o3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cYQ-tPUiB0o3",
    "outputId": "3a47455a-33ba-4d9f-829e-eb581d5a0565"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4762499988079071"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "grid3_result.cv_results_['mean_test_score'][np.argmin(grid3_result.cv_results_['rank_test_score'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "JmWx3Bl0Uv-B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JmWx3Bl0Uv-B",
    "outputId": "095e9212-c43c-4e41-c806-149824b8b8ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10/10 [==============================] - 4s 15ms/step - loss: 1.0984 - accuracy: 0.3400\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 1.0984 - accuracy: 0.3408\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 1.0983 - accuracy: 0.3433\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.0983 - accuracy: 0.3421\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 1.0983 - accuracy: 0.3433\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.0983 - accuracy: 0.3433\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 1.0983 - accuracy: 0.3433\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 1.0983 - accuracy: 0.3438\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 1.0983 - accuracy: 0.3454\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.0983 - accuracy: 0.3425\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 1.0983 - accuracy: 0.3450\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 1.0983 - accuracy: 0.3429\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.0982 - accuracy: 0.3442\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 1.0982 - accuracy: 0.3438\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 1.0982 - accuracy: 0.3429\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.0982 - accuracy: 0.3421\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.0982 - accuracy: 0.3433\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 1.0982 - accuracy: 0.3458\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1.0982 - accuracy: 0.3433\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 1.0982 - accuracy: 0.3446\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f06c6cc0790>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6 = language_model2(optimizer=grid3_result.best_params_['optimizer'], \n",
    "                   hidden_layer_sizes=grid3_result.best_params_['hidden_layer_sizes'])\n",
    "\n",
    "model6.fit(X_train, y_train, epochs=grid3_result.best_params_['nb_epoch'], batch_size=grid3_result.best_params_['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "_Rz3OEy0VCH-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Rz3OEy0VCH-",
    "outputId": "fe2e2e18-8959-4162-87cd-bbb06f307605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 1s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33166666666666667"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(model6.predict(X_test), axis=1), np.argmax(y_test, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thiJN4CoVRpA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "thiJN4CoVRpA",
    "outputId": "4baf59d0-9ace-48ee-effa-a108425e89bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[8.02448750e-01, 1.97545916e-01, 5.38991071e-06],\n",
       "       [4.49187368e-01, 4.51720476e-01, 9.90922004e-02],\n",
       "       [1.68219313e-01, 2.58350998e-01, 5.73429704e-01],\n",
       "       ...,\n",
       "       [8.77957284e-01, 1.21951364e-01, 9.13981567e-05],\n",
       "       [8.25690866e-01, 1.74283981e-01, 2.52116788e-05],\n",
       "       [7.93215096e-01, 2.06522450e-01, 2.62525835e-04]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb287a4-c194-4395-8b94-43225a419ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2765f94-1a3c-4ed2-8bd7-83da77502f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "grid_cv_result = pickle.load(open('gridcv_result.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00270919-538d-4b87-872a-346c1f0767ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21206349, 0.21666667, 0.08333333, 0.35555555, 0.33428572,\n",
       "       0.16111111, 0.18333333, 0.11666666, 0.56111111, 0.33666667,\n",
       "       0.33888889, 0.2115873 , 0.08333333, 0.23333333, 0.34444444,\n",
       "       0.30777778, 0.28507937, 0.08333333, 0.11666666, 0.41666666,\n",
       "       0.26666667, 0.14460317, 0.28333333, 0.30555556, 0.32222222,\n",
       "       0.27777779, 0.18333333, 0.08333333, 0.23333334, 0.15000001,\n",
       "       0.13333334, 0.22777778, 0.08888889, 0.10555556, 0.32222223,\n",
       "       0.22777778, 0.11126985, 0.08333333, 0.23333333, 0.20015873,\n",
       "       0.26714285, 0.10158731, 0.11666666, 0.43714286, 0.27222221,\n",
       "       0.47555556, 0.38936508, 0.08333333, 0.19444444, 0.26666667,\n",
       "       0.38349206, 0.40761905, 0.11666666, 0.11111111, 0.31666666,\n",
       "       0.07777778, 0.41507937, 0.08333333, 0.11666666, 0.46666666,\n",
       "       0.40809523, 0.23349206, 0.38333333, 0.15555555, 0.32253969,\n",
       "       0.35571429, 0.17809524, 0.22222222, 0.14507936, 0.20555556,\n",
       "       0.04444445, 0.1447619 , 0.08888889, 0.50555555, 0.42333334,\n",
       "       0.19444444, 0.19460318, 0.08333333, 0.28333333, 0.3204762 ,\n",
       "       0.08333333, 0.35      , 0.08333333, 0.08333333, 0.08333333,\n",
       "       0.08333333, 0.4       , 0.08333333, 0.08333333, 0.28333333,\n",
       "       0.08333333, 0.21666666, 0.08333333, 0.08333333, 0.31666666,\n",
       "       0.08333333, 0.3       , 0.11666666, 0.08333333, 0.08333333,\n",
       "       0.08333333, 0.23888889, 0.08333333, 0.08333333, 0.13888888,\n",
       "       0.10555555, 0.28333333, 0.08333333, 0.08333333, 0.08333333,\n",
       "       0.08333333, 0.11666666, 0.08333333, 0.19444445, 0.08333333,\n",
       "       0.18333333, 0.38333333, 0.27761905, 0.0947619 , 0.28333333,\n",
       "       0.51666666, 0.34444444, 0.48333333, 0.08333333, 0.31666666,\n",
       "       0.08333333, 0.48333333, 0.1       , 0.08333333, 0.08333333,\n",
       "       0.08333333, 0.43333333, 0.4       , 0.08333333, 0.28333333,\n",
       "       0.08333333, 0.61666666, 0.18333333, 0.08333333, 0.08333333,\n",
       "       0.08333333, 0.31666666, 0.0947619 , 0.28333333, 0.08333333,\n",
       "       0.08333333, 0.3       , 0.28333333, 0.08333333, 0.08333333,\n",
       "       0.08333333, 0.19444445, 0.18333333, 0.08333333, 0.27222222,\n",
       "       0.08333333, 0.31777778, 0.12333333, 0.08333333, 0.14444444])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c60d92f-dce8-4ef8-a571-1bea91b8b710",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 70,  55,  80, 130, 128,  37, 158, 125, 123,  92, 114,  47,  77,\n",
       "       112, 110, 109,  57, 108, 107,  98, 103, 102,  99, 100, 133,  27,\n",
       "        90, 135,   2, 155, 153,  88,  87,  93,  85, 150,  84,  12, 149,\n",
       "        95, 148, 145, 144, 129,  82, 138, 139,  17, 140,  83,  72,  32,\n",
       "       118, 142, 127,  41, 105,  33,  53,  36,   7, 111,  52,  18,  42,\n",
       "        58,  97, 157,  30, 104, 159,  21,  71,  68,  29,  63,   5,  66,\n",
       "       137, 152, 115,  26,   6,  48,  75, 113, 151,  76,  39,  69,  11,\n",
       "         0,  91,   1,  67,  35,  31,  38,  13,  28,  61, 101,  49,  20,\n",
       "        40,  44, 154, 117,  25,  22, 143, 147, 119, 134,  89,  78, 106,\n",
       "        16,  96, 146,  23,  15,  94, 124, 141,  54, 156,  79,  24,  34,\n",
       "        64,   4,   9,  10,  14, 121,  81,   3,  65,  62, 116,  50,  46,\n",
       "       132,  86,  51,  60,  56,  19,  74, 131,  43,  59,  45, 126, 122,\n",
       "        73, 120,   8, 136], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "mean_score = np.array(grid_cv_result[\"mean_test_score\"])\n",
    "mean_score_rank = mean_score.argsort()\n",
    "mean_score_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7007dd26-7b7f-4aed-93c4-06599a91cbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 28,  60,  76,  82,  91,  93, 131, 132, 137, 158], dtype=int64),)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(mean_score_rank < 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7b20315-8198-4bb2-90f9-fe2670768a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 70,  36,  41,  74,  55,   6, 105, 118, 142,  72,  21, 157,  47,\n",
       "       112, 109, 114, 108, 107, 103, 123, 102, 110,  99, 100,  82,  98,\n",
       "        95,  93,  92,  90,  88,  87,  77,  85, 158,  80,  84,  57, 125,\n",
       "        83, 144, 139, 138, 148, 149, 135, 133, 150, 140, 153, 145, 130,\n",
       "       129, 128, 155,   2,  12,  37,  27,  17,  30,  71,   5,  32,  38,\n",
       "        61,  45,  25,  65, 127,  40,  29,  66,  53,   1,  16,   0,  33,\n",
       "       101,  34,  20,  75,  63,  24,  79,  39, 137, 115, 152,  11,  46,\n",
       "        42, 111,  52,  58,  97,  18,   7,  68,  76, 151, 113,  48,  67,\n",
       "         4,  13,  64,  91,  15,   9, 104,  14,  54, 159,  26,  28,  10,\n",
       "        31,  35,  69,  49, 131,  56, 154,  19,  50,   3, 156,  62, 136,\n",
       "        44,  43,  51,  60, 116, 117, 121,   8, 143, 147,  78, 119, 134,\n",
       "        89, 106,  22,  23, 146,  96,  59,  94, 141, 124,  73,  81, 120,\n",
       "       126, 122, 132,  86], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_score = np.array(grid_cv_result[\"std_test_score\"])\n",
    "std_score_rank = std_score.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e2ce88e-d566-45a9-b4a7-96e522e7538e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_hidden_layer_sizes</th>\n",
       "      <th>param_nb_epoch</th>\n",
       "      <th>param_optimizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.461793</td>\n",
       "      <td>0.097312</td>\n",
       "      <td>0.137249</td>\n",
       "      <td>0.006131</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>(50, 50, 50, 50)</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 5, 'hidde...</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.212063</td>\n",
       "      <td>0.211065</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.391384</td>\n",
       "      <td>0.014835</td>\n",
       "      <td>0.185101</td>\n",
       "      <td>0.090780</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>(50, 50, 50, 50)</td>\n",
       "      <td>10</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 5, 'hidde...</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.209644</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.414888</td>\n",
       "      <td>0.090011</td>\n",
       "      <td>0.134258</td>\n",
       "      <td>0.005419</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>(50, 50, 50, 50)</td>\n",
       "      <td>10</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 5, 'hidde...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.605624</td>\n",
       "      <td>0.075675</td>\n",
       "      <td>0.131512</td>\n",
       "      <td>0.006313</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>(50, 50, 50, 50)</td>\n",
       "      <td>10</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 5, 'hidde...</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>0.366582</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.441564</td>\n",
       "      <td>0.074478</td>\n",
       "      <td>0.127249</td>\n",
       "      <td>0.006882</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>(50, 50, 50, 50)</td>\n",
       "      <td>10</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 5, 'hidde...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.334286</td>\n",
       "      <td>0.264682</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.333192</td>\n",
       "      <td>0.015942</td>\n",
       "      <td>0.132204</td>\n",
       "      <td>0.010513</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>10</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>25</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'sigmoid', 'batch_size': 10, 'h...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.311577</td>\n",
       "      <td>0.036243</td>\n",
       "      <td>0.174430</td>\n",
       "      <td>0.094056</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>10</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>25</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>{'activation': 'sigmoid', 'batch_size': 10, 'h...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.317778</td>\n",
       "      <td>0.370492</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.345889</td>\n",
       "      <td>0.103304</td>\n",
       "      <td>0.130649</td>\n",
       "      <td>0.004031</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>10</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>25</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'sigmoid', 'batch_size': 10, 'h...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.123333</td>\n",
       "      <td>0.165865</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.395710</td>\n",
       "      <td>0.039170</td>\n",
       "      <td>0.128640</td>\n",
       "      <td>0.006577</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>10</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>25</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>{'activation': 'sigmoid', 'batch_size': 10, 'h...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.370805</td>\n",
       "      <td>0.105115</td>\n",
       "      <td>0.125998</td>\n",
       "      <td>0.006001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>10</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>25</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>{'activation': 'sigmoid', 'batch_size': 10, 'h...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144444</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0         0.461793      0.097312         0.137249        0.006131   \n",
       "1         0.391384      0.014835         0.185101        0.090780   \n",
       "2         0.414888      0.090011         0.134258        0.005419   \n",
       "3         0.605624      0.075675         0.131512        0.006313   \n",
       "4         0.441564      0.074478         0.127249        0.006882   \n",
       "..             ...           ...              ...             ...   \n",
       "155       0.333192      0.015942         0.132204        0.010513   \n",
       "156       0.311577      0.036243         0.174430        0.094056   \n",
       "157       0.345889      0.103304         0.130649        0.004031   \n",
       "158       0.395710      0.039170         0.128640        0.006577   \n",
       "159       0.370805      0.105115         0.125998        0.006001   \n",
       "\n",
       "    param_activation param_batch_size param_hidden_layer_sizes param_nb_epoch  \\\n",
       "0               relu                5         (50, 50, 50, 50)             10   \n",
       "1               relu                5         (50, 50, 50, 50)             10   \n",
       "2               relu                5         (50, 50, 50, 50)             10   \n",
       "3               relu                5         (50, 50, 50, 50)             10   \n",
       "4               relu                5         (50, 50, 50, 50)             10   \n",
       "..               ...              ...                      ...            ...   \n",
       "155          sigmoid               10                    (50,)             25   \n",
       "156          sigmoid               10                    (50,)             25   \n",
       "157          sigmoid               10                    (50,)             25   \n",
       "158          sigmoid               10                    (50,)             25   \n",
       "159          sigmoid               10                    (50,)             25   \n",
       "\n",
       "    param_optimizer                                             params  \\\n",
       "0              adam  {'activation': 'relu', 'batch_size': 5, 'hidde...   \n",
       "1           adagrad  {'activation': 'relu', 'batch_size': 5, 'hidde...   \n",
       "2               sgd  {'activation': 'relu', 'batch_size': 5, 'hidde...   \n",
       "3           RMSprop  {'activation': 'relu', 'batch_size': 5, 'hidde...   \n",
       "4            Adamax  {'activation': 'relu', 'batch_size': 5, 'hidde...   \n",
       "..              ...                                                ...   \n",
       "155            adam  {'activation': 'sigmoid', 'batch_size': 10, 'h...   \n",
       "156         adagrad  {'activation': 'sigmoid', 'batch_size': 10, 'h...   \n",
       "157             sgd  {'activation': 'sigmoid', 'batch_size': 10, 'h...   \n",
       "158         RMSprop  {'activation': 'sigmoid', 'batch_size': 10, 'h...   \n",
       "159          Adamax  {'activation': 'sigmoid', 'batch_size': 10, 'h...   \n",
       "\n",
       "     split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0             0.027778           0.000000           0.583333   \n",
       "1             0.055556           0.361111           0.555556   \n",
       "2             0.000000           0.000000           0.416667   \n",
       "3             0.361111           0.000000           0.416667   \n",
       "4             0.000000           0.722222           0.555556   \n",
       "..                 ...                ...                ...   \n",
       "155           0.000000           0.000000           0.416667   \n",
       "156           0.000000           1.000000           0.388889   \n",
       "157           0.000000           0.000000           0.416667   \n",
       "158           0.000000           0.000000           0.416667   \n",
       "159           0.000000           0.000000           0.722222   \n",
       "\n",
       "     split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0             0.277778           0.171429         0.212063        0.211065   \n",
       "1             0.111111           0.000000         0.216667        0.209644   \n",
       "2             0.000000           0.000000         0.083333        0.166667   \n",
       "3             1.000000           0.000000         0.355556        0.366582   \n",
       "4             0.222222           0.171429         0.334286        0.264682   \n",
       "..                 ...                ...              ...             ...   \n",
       "155           0.000000           0.000000         0.083333        0.166667   \n",
       "156           0.000000           0.200000         0.317778        0.370492   \n",
       "157           0.000000           0.200000         0.123333        0.165865   \n",
       "158           0.000000           0.000000         0.083333        0.166667   \n",
       "159           0.000000           0.000000         0.144444        0.288889   \n",
       "\n",
       "     rank_test_score  \n",
       "0                 69  \n",
       "1                 67  \n",
       "2                111  \n",
       "3                 23  \n",
       "4                 29  \n",
       "..               ...  \n",
       "155              111  \n",
       "156               34  \n",
       "157               93  \n",
       "158              111  \n",
       "159               90  \n",
       "\n",
       "[160 rows x 18 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(grid_cv_result)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b7c1d9a-1664-4fc2-b6f0-a6fb1e09d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = df.sort_values(\"mean_test_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a399708b-bd5f-4618-baaa-f16c348ad937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.371184</td>\n",
       "      <td>{'activation': 'sigmoid', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50), 'nb_epoch': 25, 'optimizer': 'adagrad'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.561111</td>\n",
       "      <td>0.392208</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 5, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 25, 'optimizer': 'RMSprop'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.448454</td>\n",
       "      <td>{'activation': 'sigmoid', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 10, 'optimizer': 'adam'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.505556</td>\n",
       "      <td>0.414773</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10, 'hidden_layer_sizes': (50,), 'nb_epoch': 10, 'optimizer': 'RMSprop'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.448454</td>\n",
       "      <td>{'activation': 'sigmoid', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 10, 'optimizer': 'sgd'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.448454</td>\n",
       "      <td>{'activation': 'sigmoid', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 25, 'optimizer': 'adagrad'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.475556</td>\n",
       "      <td>0.190471</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 25, 'optimizer': 'adam'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.408777</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50), 'nb_epoch': 25, 'optimizer': 'Adamax'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.437143</td>\n",
       "      <td>0.372905</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 10, 'optimizer': 'RMSprop'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.330824</td>\n",
       "      <td>{'activation': 'sigmoid', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50), 'nb_epoch': 10, 'optimizer': 'adagrad'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>{'activation': 'sigmoid', 'batch_size': 5, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 10, 'optimizer': 'sgd'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>{'activation': 'sigmoid', 'batch_size': 5, 'hidden_layer_sizes': (50,), 'nb_epoch': 10, 'optimizer': 'Adamax'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10, 'hidden_layer_sizes': (50,), 'nb_epoch': 25, 'optimizer': 'sgd'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50), 'nb_epoch': 25, 'optimizer': 'sgd'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>{'activation': 'sigmoid', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 10, 'optimizer': 'RMSprop'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>{'activation': 'sigmoid', 'batch_size': 5, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 25, 'optimizer': 'RMSprop'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 25, 'optimizer': 'sgd'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>{'activation': 'sigmoid', 'batch_size': 5, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 10, 'optimizer': 'adam'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50), 'nb_epoch': 25, 'optimizer': 'adam'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.064788</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 10, 'hidden_layer_sizes': (50,), 'nb_epoch': 10, 'optimizer': 'adam'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_test_score  std_test_score  \\\n",
       "136         0.616667        0.371184   \n",
       "8           0.561111        0.392208   \n",
       "120         0.516667        0.448454   \n",
       "73          0.505556        0.414773   \n",
       "122         0.483333        0.448454   \n",
       "126         0.483333        0.448454   \n",
       "45          0.475556        0.190471   \n",
       "59          0.466667        0.408777   \n",
       "43          0.437143        0.372905   \n",
       "131         0.433333        0.330824   \n",
       "..               ...             ...   \n",
       "82          0.083333        0.166667   \n",
       "114         0.083333        0.166667   \n",
       "77          0.083333        0.166667   \n",
       "57          0.083333        0.166667   \n",
       "123         0.083333        0.166667   \n",
       "88          0.083333        0.166667   \n",
       "47          0.083333        0.166667   \n",
       "80          0.083333        0.166667   \n",
       "55          0.077778        0.155556   \n",
       "70          0.044444        0.064788   \n",
       "\n",
       "                                                                                                                          params  \n",
       "136      {'activation': 'sigmoid', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50), 'nb_epoch': 25, 'optimizer': 'adagrad'}  \n",
       "8        {'activation': 'relu', 'batch_size': 5, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 25, 'optimizer': 'RMSprop'}  \n",
       "120     {'activation': 'sigmoid', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 10, 'optimizer': 'adam'}  \n",
       "73                 {'activation': 'relu', 'batch_size': 10, 'hidden_layer_sizes': (50,), 'nb_epoch': 10, 'optimizer': 'RMSprop'}  \n",
       "122      {'activation': 'sigmoid', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 10, 'optimizer': 'sgd'}  \n",
       "126  {'activation': 'sigmoid', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 25, 'optimizer': 'adagrad'}  \n",
       "45         {'activation': 'relu', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 25, 'optimizer': 'adam'}  \n",
       "59           {'activation': 'relu', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50), 'nb_epoch': 25, 'optimizer': 'Adamax'}  \n",
       "43      {'activation': 'relu', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 10, 'optimizer': 'RMSprop'}  \n",
       "131      {'activation': 'sigmoid', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50), 'nb_epoch': 10, 'optimizer': 'adagrad'}  \n",
       "..                                                                                                                           ...  \n",
       "82        {'activation': 'sigmoid', 'batch_size': 5, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 10, 'optimizer': 'sgd'}  \n",
       "114               {'activation': 'sigmoid', 'batch_size': 5, 'hidden_layer_sizes': (50,), 'nb_epoch': 10, 'optimizer': 'Adamax'}  \n",
       "77                     {'activation': 'relu', 'batch_size': 10, 'hidden_layer_sizes': (50,), 'nb_epoch': 25, 'optimizer': 'sgd'}  \n",
       "57              {'activation': 'relu', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50), 'nb_epoch': 25, 'optimizer': 'sgd'}  \n",
       "123  {'activation': 'sigmoid', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 10, 'optimizer': 'RMSprop'}  \n",
       "88    {'activation': 'sigmoid', 'batch_size': 5, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 25, 'optimizer': 'RMSprop'}  \n",
       "47          {'activation': 'relu', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 25, 'optimizer': 'sgd'}  \n",
       "80       {'activation': 'sigmoid', 'batch_size': 5, 'hidden_layer_sizes': (50, 50, 50, 50), 'nb_epoch': 10, 'optimizer': 'adam'}  \n",
       "55             {'activation': 'relu', 'batch_size': 10, 'hidden_layer_sizes': (50, 50, 50), 'nb_epoch': 25, 'optimizer': 'adam'}  \n",
       "70                    {'activation': 'relu', 'batch_size': 10, 'hidden_layer_sizes': (50,), 'nb_epoch': 10, 'optimizer': 'adam'}  \n",
       "\n",
       "[160 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth=160\n",
    "pd.options.display.min_rows=20\n",
    "sdf[[\"mean_test_score\", \"std_test_score\", \"params\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8ed84bb9-32c4-4ddf-b16f-ad6eb5a27c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = sdf[[\"mean_test_score\", \"std_test_score\", \"params\"]].params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e5164548-9b2c-457a-9aa0-9c5aaebfbc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "480/480 [==============================] - 12s 16ms/step - loss: 1.1132 - accuracy: 0.3379\n",
      "Epoch 2/10\n",
      "480/480 [==============================] - 8s 17ms/step - loss: 1.1565 - accuracy: 0.3446\n",
      "Epoch 3/10\n",
      "480/480 [==============================] - 8s 17ms/step - loss: 1.0663 - accuracy: 0.4121\n",
      "Epoch 4/10\n",
      "480/480 [==============================] - 8s 17ms/step - loss: 1.1270 - accuracy: 0.4596\n",
      "Epoch 5/10\n",
      "480/480 [==============================] - 8s 17ms/step - loss: 0.9957 - accuracy: 0.4871\n",
      "Epoch 6/10\n",
      "480/480 [==============================] - 8s 17ms/step - loss: 0.9533 - accuracy: 0.5192\n",
      "Epoch 7/10\n",
      "480/480 [==============================] - 8s 16ms/step - loss: 0.9304 - accuracy: 0.5333\n",
      "Epoch 8/10\n",
      "480/480 [==============================] - 8s 16ms/step - loss: 0.8945 - accuracy: 0.5471\n",
      "Epoch 9/10\n",
      "480/480 [==============================] - 8s 16ms/step - loss: 0.8575 - accuracy: 0.5671\n",
      "Epoch 10/10\n",
      "480/480 [==============================] - 8s 16ms/step - loss: 0.8255 - accuracy: 0.5700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x223ea5343d0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model10 = language_model(activation=params['activation'], \n",
    "                   optimizer=params['optimizer'], \n",
    "                   hidden_layer_sizes=params['hidden_layer_sizes'])\n",
    "\n",
    "model10.fit(X_train, y_train, epochs=params['nb_epoch'], batch_size=params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "afecf1c8-70ae-4080-ba7f-694d5df09683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 1s 18ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5416666666666666"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(np.argmax(model10.predict(X_test), axis=1), np.argmax(y_test, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a916459c-b116-4b98-a95d-f3f041685e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034b29a9-6040-4748-bf64-70e3f971bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model11 = language_model(activation=params2['activation'], \n",
    "                   optimizer=params2['optimizer'], \n",
    "                   hidden_layer_sizes=params2['hidden_layer_sizes'])\n",
    "\n",
    "model11.fit(X_train, y_train, epochs=params2['nb_epoch'], batch_size=params2['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5b5e3e82-5f11-4f16-a794-f3342af73201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "480/480 [==============================] - 10s 16ms/step - loss: 1.1249 - accuracy: 0.3600\n",
      "Epoch 2/10\n",
      "480/480 [==============================] - 8s 16ms/step - loss: 1.0069 - accuracy: 0.4792\n",
      "Epoch 3/10\n",
      "480/480 [==============================] - 8s 17ms/step - loss: 0.9949 - accuracy: 0.5021\n",
      "Epoch 4/10\n",
      "480/480 [==============================] - 8s 16ms/step - loss: 0.9360 - accuracy: 0.5221\n",
      "Epoch 5/10\n",
      "480/480 [==============================] - 8s 16ms/step - loss: 0.9115 - accuracy: 0.5283\n",
      "Epoch 6/10\n",
      "480/480 [==============================] - 8s 16ms/step - loss: 0.8741 - accuracy: 0.5521\n",
      "Epoch 7/10\n",
      "480/480 [==============================] - 8s 16ms/step - loss: 0.8290 - accuracy: 0.5767\n",
      "Epoch 8/10\n",
      "480/480 [==============================] - 8s 16ms/step - loss: 0.7977 - accuracy: 0.5854\n",
      "Epoch 9/10\n",
      "480/480 [==============================] - 8s 16ms/step - loss: 0.7677 - accuracy: 0.5983\n",
      "Epoch 10/10\n",
      "480/480 [==============================] - 8s 16ms/step - loss: 0.8009 - accuracy: 0.6062\n",
      "19/19 [==============================] - 1s 16ms/step\n",
      "0: 0.5383333333333333\n",
      "Epoch 1/10\n",
      "480/480 [==============================] - 17s 29ms/step - loss: 1.0987 - accuracy: 0.3350\n",
      "Epoch 2/10\n",
      "480/480 [==============================] - 14s 29ms/step - loss: 1.0986 - accuracy: 0.3392\n",
      "Epoch 3/10\n",
      "480/480 [==============================] - 14s 29ms/step - loss: 1.0986 - accuracy: 0.3367\n",
      "Epoch 4/10\n",
      "480/480 [==============================] - 14s 28ms/step - loss: 1.0985 - accuracy: 0.3371\n",
      "Epoch 5/10\n",
      "480/480 [==============================] - 13s 28ms/step - loss: 1.0985 - accuracy: 0.3329\n",
      "Epoch 6/10\n",
      "480/480 [==============================] - 13s 28ms/step - loss: 1.0985 - accuracy: 0.3375\n",
      "Epoch 7/10\n",
      "480/480 [==============================] - 13s 28ms/step - loss: 1.0985 - accuracy: 0.3371\n",
      "Epoch 8/10\n",
      "480/480 [==============================] - 13s 28ms/step - loss: 1.0985 - accuracy: 0.3396\n",
      "Epoch 9/10\n",
      "480/480 [==============================] - 13s 28ms/step - loss: 1.0985 - accuracy: 0.3371\n",
      "Epoch 10/10\n",
      "480/480 [==============================] - 13s 28ms/step - loss: 1.0985 - accuracy: 0.3371\n",
      "19/19 [==============================] - 1s 22ms/step\n",
      "1: 0.31833333333333336\n",
      "Epoch 1/10\n",
      "480/480 [==============================] - 18s 32ms/step - loss: 1.0994 - accuracy: 0.3246\n",
      "Epoch 2/10\n",
      "480/480 [==============================] - 15s 31ms/step - loss: 1.0993 - accuracy: 0.3113\n",
      "Epoch 3/10\n",
      "480/480 [==============================] - 14s 30ms/step - loss: 1.0992 - accuracy: 0.3258\n",
      "Epoch 4/10\n",
      "480/480 [==============================] - 14s 30ms/step - loss: 1.0991 - accuracy: 0.3300\n",
      "Epoch 5/10\n",
      "480/480 [==============================] - 14s 30ms/step - loss: 1.0990 - accuracy: 0.3404\n",
      "Epoch 6/10\n",
      "480/480 [==============================] - 14s 30ms/step - loss: 1.0991 - accuracy: 0.3388\n",
      "Epoch 7/10\n",
      "480/480 [==============================] - 14s 30ms/step - loss: 1.0989 - accuracy: 0.3413\n",
      "Epoch 8/10\n",
      "480/480 [==============================] - 14s 30ms/step - loss: 1.0993 - accuracy: 0.3258\n",
      "Epoch 9/10\n",
      "480/480 [==============================] - 14s 30ms/step - loss: 1.0992 - accuracy: 0.3204\n",
      "Epoch 10/10\n",
      "480/480 [==============================] - 14s 30ms/step - loss: 1.0992 - accuracy: 0.3292\n",
      "19/19 [==============================] - 1s 23ms/step\n",
      "2: 0.36\n",
      "Epoch 1/10\n",
      "480/480 [==============================] - 18s 30ms/step - loss: 1.0799 - accuracy: 0.4067\n",
      "Epoch 2/10\n",
      "480/480 [==============================] - 15s 31ms/step - loss: 1.0173 - accuracy: 0.4708\n",
      "Epoch 3/10\n",
      "480/480 [==============================] - 14s 30ms/step - loss: 0.9788 - accuracy: 0.5100\n",
      "Epoch 4/10\n",
      "480/480 [==============================] - 14s 29ms/step - loss: 0.9694 - accuracy: 0.5267\n",
      "Epoch 5/10\n",
      "480/480 [==============================] - 15s 31ms/step - loss: 0.9011 - accuracy: 0.5433\n",
      "Epoch 6/10\n",
      "480/480 [==============================] - 15s 31ms/step - loss: 0.9141 - accuracy: 0.5696\n",
      "Epoch 7/10\n",
      "480/480 [==============================] - 15s 31ms/step - loss: 1.0021 - accuracy: 0.5788\n",
      "Epoch 8/10\n",
      "480/480 [==============================] - 15s 32ms/step - loss: 0.7964 - accuracy: 0.5958\n",
      "Epoch 9/10\n",
      "480/480 [==============================] - 15s 32ms/step - loss: 0.8368 - accuracy: 0.6050\n",
      "Epoch 10/10\n",
      "480/480 [==============================] - 14s 30ms/step - loss: 0.7534 - accuracy: 0.6146\n",
      "19/19 [==============================] - 1s 23ms/step\n",
      "3: 0.5316666666666666\n",
      "Epoch 1/10\n",
      "480/480 [==============================] - 12s 19ms/step - loss: 1.0765 - accuracy: 0.3604\n",
      "Epoch 2/10\n",
      "480/480 [==============================] - 9s 19ms/step - loss: 1.0155 - accuracy: 0.4638\n",
      "Epoch 3/10\n",
      "480/480 [==============================] - 9s 19ms/step - loss: 0.9956 - accuracy: 0.4804\n",
      "Epoch 4/10\n",
      "480/480 [==============================] - 9s 19ms/step - loss: 0.9780 - accuracy: 0.4979\n",
      "Epoch 5/10\n",
      "480/480 [==============================] - 9s 19ms/step - loss: 0.9637 - accuracy: 0.5004\n",
      "Epoch 6/10\n",
      "480/480 [==============================] - 9s 19ms/step - loss: 0.9421 - accuracy: 0.5142\n",
      "Epoch 7/10\n",
      "480/480 [==============================] - 9s 19ms/step - loss: 0.9201 - accuracy: 0.5221\n",
      "Epoch 8/10\n",
      "480/480 [==============================] - 9s 19ms/step - loss: 0.9006 - accuracy: 0.5446\n",
      "Epoch 9/10\n",
      "480/480 [==============================] - 9s 19ms/step - loss: 0.8826 - accuracy: 0.5475\n",
      "Epoch 10/10\n",
      "480/480 [==============================] - 9s 19ms/step - loss: 0.8554 - accuracy: 0.5625\n",
      "19/19 [==============================] - 1s 18ms/step\n",
      "4: 0.495\n",
      "Epoch 1/25\n",
      "480/480 [==============================] - 23s 43ms/step - loss: 1.1276 - accuracy: 0.3346\n",
      "Epoch 2/25\n",
      "480/480 [==============================] - 20s 42ms/step - loss: 1.1027 - accuracy: 0.3771\n",
      "Epoch 3/25\n",
      "480/480 [==============================] - 20s 42ms/step - loss: 1.0616 - accuracy: 0.4437\n",
      "Epoch 4/25\n",
      "480/480 [==============================] - 20s 42ms/step - loss: 1.0368 - accuracy: 0.4721\n",
      "Epoch 5/25\n",
      "480/480 [==============================] - 20s 42ms/step - loss: 0.9673 - accuracy: 0.4996\n",
      "Epoch 6/25\n",
      "480/480 [==============================] - 20s 41ms/step - loss: 0.9189 - accuracy: 0.5312\n",
      "Epoch 7/25\n",
      "480/480 [==============================] - 20s 42ms/step - loss: 0.8769 - accuracy: 0.5479\n",
      "Epoch 8/25\n",
      "480/480 [==============================] - 20s 41ms/step - loss: 0.8428 - accuracy: 0.5596\n",
      "Epoch 9/25\n",
      "480/480 [==============================] - 20s 41ms/step - loss: 0.8127 - accuracy: 0.5813\n",
      "Epoch 10/25\n",
      "480/480 [==============================] - 20s 41ms/step - loss: 0.7892 - accuracy: 0.5975\n",
      "Epoch 11/25\n",
      "480/480 [==============================] - 20s 42ms/step - loss: 0.7629 - accuracy: 0.5946\n",
      "Epoch 12/25\n",
      "480/480 [==============================] - 20s 41ms/step - loss: 0.7122 - accuracy: 0.6221\n",
      "Epoch 13/25\n",
      "480/480 [==============================] - 20s 41ms/step - loss: 0.7077 - accuracy: 0.6154\n",
      "Epoch 14/25\n",
      "480/480 [==============================] - 20s 41ms/step - loss: 0.6812 - accuracy: 0.6267\n",
      "Epoch 15/25\n",
      "480/480 [==============================] - 20s 42ms/step - loss: 0.6976 - accuracy: 0.6442\n",
      "Epoch 16/25\n",
      "480/480 [==============================] - 20s 41ms/step - loss: 0.6353 - accuracy: 0.6521\n",
      "Epoch 17/25\n",
      "480/480 [==============================] - 20s 41ms/step - loss: 0.6382 - accuracy: 0.6458\n",
      "Epoch 18/25\n",
      "480/480 [==============================] - 20s 41ms/step - loss: 0.5998 - accuracy: 0.6596\n",
      "Epoch 19/25\n",
      "480/480 [==============================] - 20s 42ms/step - loss: 0.5935 - accuracy: 0.6742\n",
      "Epoch 20/25\n",
      "480/480 [==============================] - 20s 41ms/step - loss: 0.5934 - accuracy: 0.6900\n",
      "Epoch 21/25\n",
      "480/480 [==============================] - 20s 41ms/step - loss: 0.5674 - accuracy: 0.6958\n",
      "Epoch 22/25\n",
      "480/480 [==============================] - 20s 41ms/step - loss: 0.5383 - accuracy: 0.7050\n",
      "Epoch 23/25\n",
      "480/480 [==============================] - 20s 42ms/step - loss: 0.5345 - accuracy: 0.7154\n",
      "Epoch 24/25\n",
      "480/480 [==============================] - 20s 41ms/step - loss: 0.5235 - accuracy: 0.7275\n",
      "Epoch 25/25\n",
      "480/480 [==============================] - 20s 42ms/step - loss: 0.5311 - accuracy: 0.7329\n",
      "19/19 [==============================] - 1s 33ms/step\n",
      "5: 0.5683333333333334\n",
      "Epoch 1/25\n",
      "480/480 [==============================] - 26s 48ms/step - loss: 1.0987 - accuracy: 0.3204\n",
      "Epoch 2/25\n",
      "480/480 [==============================] - 22s 46ms/step - loss: 1.0986 - accuracy: 0.3117\n",
      "Epoch 3/25\n",
      "480/480 [==============================] - 22s 46ms/step - loss: 1.0986 - accuracy: 0.3358\n",
      "Epoch 4/25\n",
      "480/480 [==============================] - 22s 46ms/step - loss: 1.0986 - accuracy: 0.3383\n",
      "Epoch 5/25\n",
      "480/480 [==============================] - 22s 47ms/step - loss: 1.0986 - accuracy: 0.3325\n",
      "Epoch 6/25\n",
      "480/480 [==============================] - 22s 45ms/step - loss: 1.0986 - accuracy: 0.3408\n",
      "Epoch 7/25\n",
      "480/480 [==============================] - 22s 46ms/step - loss: 1.0985 - accuracy: 0.3404\n",
      "Epoch 8/25\n",
      "480/480 [==============================] - 22s 46ms/step - loss: 1.0986 - accuracy: 0.3429\n",
      "Epoch 9/25\n",
      "480/480 [==============================] - 22s 46ms/step - loss: 1.0986 - accuracy: 0.3346\n",
      "Epoch 10/25\n",
      "480/480 [==============================] - 22s 46ms/step - loss: 1.0985 - accuracy: 0.3367\n",
      "Epoch 11/25\n",
      "480/480 [==============================] - 22s 46ms/step - loss: 1.0985 - accuracy: 0.3375\n",
      "Epoch 12/25\n",
      "480/480 [==============================] - 22s 46ms/step - loss: 1.0985 - accuracy: 0.3371\n",
      "Epoch 13/25\n",
      "480/480 [==============================] - 22s 46ms/step - loss: 1.0985 - accuracy: 0.3371\n",
      "Epoch 14/25\n",
      "480/480 [==============================] - 22s 45ms/step - loss: 1.0985 - accuracy: 0.3396\n",
      "Epoch 15/25\n",
      "480/480 [==============================] - 22s 46ms/step - loss: 1.0985 - accuracy: 0.3358\n",
      "Epoch 16/25\n",
      "480/480 [==============================] - 22s 46ms/step - loss: 1.0985 - accuracy: 0.3383\n",
      "Epoch 17/25\n",
      "480/480 [==============================] - 22s 46ms/step - loss: 1.0985 - accuracy: 0.3371\n",
      "Epoch 18/25\n",
      "480/480 [==============================] - 22s 46ms/step - loss: 1.0985 - accuracy: 0.3375\n",
      "Epoch 19/25\n",
      "480/480 [==============================] - 22s 46ms/step - loss: 1.0985 - accuracy: 0.3367\n",
      "Epoch 20/25\n",
      "480/480 [==============================] - 22s 46ms/step - loss: 1.0985 - accuracy: 0.3375\n",
      "Epoch 21/25\n",
      "480/480 [==============================] - 22s 47ms/step - loss: 1.0985 - accuracy: 0.3379\n",
      "Epoch 22/25\n",
      "  7/480 [..............................] - ETA: 21s - loss: 1.0989 - accuracy: 0.2857"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [90]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m params \u001b[38;5;241m=\u001b[39m sdf[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_test_score\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd_test_score\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mparams[i]\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m language_model(activation\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m      5\u001b[0m                optimizer\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m      6\u001b[0m                hidden_layer_sizes\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_layer_sizes\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnb_epoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i, accuracy_score(np\u001b[38;5;241m.\u001b[39margmax(model\u001b[38;5;241m.\u001b[39mpredict(X_test), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), np\u001b[38;5;241m.\u001b[39margmax(y_test, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    params = sdf[[\"mean_test_score\", \"std_test_score\", \"params\"]].params[i]\n",
    "    \n",
    "    model = language_model(activation=params['activation'], \n",
    "                   optimizer=params['optimizer'], \n",
    "                   hidden_layer_sizes=params['hidden_layer_sizes'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=params['nb_epoch'], batch_size=params['batch_size'])\n",
    "    \n",
    "    print(\"{}: {}\".format(i, accuracy_score(np.argmax(model.predict(X_test), axis=1), np.argmax(y_test, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bb87e3d3-55ed-47d8-9b32-a1b924b257b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "480/480 [==============================] - 18s 32ms/step - loss: 1.1172 - accuracy: 0.3487\n",
      "Epoch 2/10\n",
      "480/480 [==============================] - 16s 32ms/step - loss: 1.0453 - accuracy: 0.4529\n",
      "Epoch 3/10\n",
      "480/480 [==============================] - 16s 32ms/step - loss: 0.9904 - accuracy: 0.4958\n",
      "Epoch 4/10\n",
      "480/480 [==============================] - 15s 32ms/step - loss: 0.9472 - accuracy: 0.5188\n",
      "Epoch 5/10\n",
      "480/480 [==============================] - 16s 32ms/step - loss: 0.9039 - accuracy: 0.5412\n",
      "Epoch 6/10\n",
      "480/480 [==============================] - 16s 33ms/step - loss: 0.8700 - accuracy: 0.5663\n",
      "Epoch 7/10\n",
      "480/480 [==============================] - 16s 34ms/step - loss: 0.8365 - accuracy: 0.5758\n",
      "Epoch 8/10\n",
      "480/480 [==============================] - 16s 33ms/step - loss: 0.8708 - accuracy: 0.5608\n",
      "Epoch 9/10\n",
      "480/480 [==============================] - 16s 34ms/step - loss: 0.8235 - accuracy: 0.5829\n",
      "Epoch 10/10\n",
      "480/480 [==============================] - 16s 33ms/step - loss: 0.7630 - accuracy: 0.6079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22383f82220>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model112 = language_model(activation=params['activation'], \n",
    "                   optimizer=params['optimizer'], \n",
    "                   hidden_layer_sizes=params['hidden_layer_sizes'])\n",
    "\n",
    "model112.fit(X_train, y_train, epochs=params['nb_epoch'], batch_size=params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4c91977a-6628-48d2-b48b-cb504c38654a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 1s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_y = np.argmax(model112.predict(X_test), axis=1)\n",
    "test_y = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e13b691a-a639-4bac-afae-a4d3f11db4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 15,  10,   0],\n",
       "       [159, 153,  54],\n",
       "       [ 34,  28, 147]], dtype=int64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cm = confusion_matrix(pred_y, test_y)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(cm, cmap='plasma')\n",
    "plt.colorbar()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.xticks(np.arange(3), [\"Beginner\", \"Intermediate\", \"Advanced\"])\n",
    "plt.yticks(np.arange(3), [\"Beginner\", \"Intermediate\", \"Advanced\"])\n",
    "\n",
    "plt.show()\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4c3535c8-2dcc-43ec-8382-d96ee51af664",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = data2[data2[\"cat\"] != \"上級\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e5763ced-03d6-44f8-ada1-6caf9a66cf39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((400, 2), (1600, 2))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = data3.sample(frac=0.2, random_state=200)\n",
    "train2 = data3.drop(test2.index)\n",
    "\n",
    "test2.shape, train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0837fdd9-9def-4d23-a072-e09bc5f37eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600, 15, 300), (1600, 2))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "le2 = preprocessing.LabelEncoder()\n",
    "ohe2 = preprocessing.OneHotEncoder()\n",
    "\n",
    "le2.fit(data3.cat)\n",
    "y_train2 = le2.transform(train2.cat).reshape(-1, 1)\n",
    "ohe2.fit(y_train2)\n",
    "y_train2 = ohe2.transform(y_train2).todense()\n",
    "\n",
    "X_train2 = np.array([x for x in train2.value])\n",
    "\n",
    "X_train2.shape, y_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "990f3ecc-f3c8-4950-9951-8792be741a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((400, 15, 300), (400, 2))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le2 = preprocessing.LabelEncoder()\n",
    "ohe2 = preprocessing.OneHotEncoder()\n",
    "\n",
    "le2.fit(data3.cat)\n",
    "y_test2 = le2.transform(test2.cat).reshape(-1, 1)\n",
    "ohe2.fit(y_test2)\n",
    "y_test2 = ohe2.transform(y_test2).todense()\n",
    "\n",
    "X_test2 = np.array([x for x in test2.value])\n",
    "\n",
    "X_test2.shape, y_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "661130d1-6b10-4e56-a451-a1a974fff027",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "12917fef-029a-41b5-8f8d-cbc17a4c71d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "320/320 [==============================] - 10s 22ms/step - loss: 0.6736 - accuracy: 0.5462\n",
      "Epoch 2/10\n",
      "320/320 [==============================] - 7s 22ms/step - loss: 0.8589 - accuracy: 0.5612\n",
      "Epoch 3/10\n",
      "320/320 [==============================] - 7s 22ms/step - loss: 0.6865 - accuracy: 0.5919\n",
      "Epoch 4/10\n",
      "320/320 [==============================] - 7s 23ms/step - loss: 1.9573 - accuracy: 0.5406\n",
      "Epoch 5/10\n",
      "320/320 [==============================] - 7s 22ms/step - loss: 0.6883 - accuracy: 0.5831\n",
      "Epoch 6/10\n",
      "320/320 [==============================] - 7s 22ms/step - loss: 0.6867 - accuracy: 0.6212\n",
      "Epoch 7/10\n",
      "320/320 [==============================] - 7s 22ms/step - loss: 0.6093 - accuracy: 0.6506\n",
      "Epoch 8/10\n",
      "320/320 [==============================] - 7s 23ms/step - loss: 0.5849 - accuracy: 0.6725\n",
      "Epoch 9/10\n",
      "320/320 [==============================] - 7s 22ms/step - loss: 0.5488 - accuracy: 0.6994\n",
      "Epoch 10/10\n",
      "320/320 [==============================] - 8s 24ms/step - loss: 0.5610 - accuracy: 0.7006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x223898658b0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model112 = language_model(activation=params['activation'], \n",
    "                   optimizer=params['optimizer'], \n",
    "                   hidden_layer_sizes=params['hidden_layer_sizes'])\n",
    "\n",
    "model112.fit(X_train2, y_train2, epochs=params['nb_epoch'], batch_size=params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3d88d326-8ba7-447c-a749-67798f12709a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 1s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_y2 = np.argmax(model112.predict(X_test2), axis=1)\n",
    "test_y2 = np.argmax(y_test2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f4b04f1d-0330-42e0-801b-7e01a291b464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[167, 123],\n",
       "       [ 31,  79]], dtype=int64)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cm = confusion_matrix(pred_y2, test_y2)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(cm, cmap='plasma')\n",
    "plt.colorbar()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.xticks(np.arange(2), [\"Beginner\", \"Intermediate\"])\n",
    "plt.yticks(np.arange(2), [\"Beginner\", \"Intermediate\"])\n",
    "\n",
    "plt.show()\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1ffccd7b-8c84-4293-8cc8-434b1bc7ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns \n",
    "from sklearn.metrics import roc_curve  \n",
    "\n",
    "def plot_roc_cur(fper, tper):  \n",
    "    plt.plot(fper, tper, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6869dc0c-e377-4325-8283-37408963d0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fper, tper, thresholds = roc_curve(test_y2, pred_y2) \n",
    "plot_roc_cur(fper, tper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e36c5e-427a-43b6-832e-9cdd237c3d56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
