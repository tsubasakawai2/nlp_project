{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13b60299-c959-479a-8d50-c3cb97e49d40",
      "metadata": {
        "id": "13b60299-c959-479a-8d50-c3cb97e49d40",
        "outputId": "f0fb5718-2fe6-41e2-db04-6818ea514abe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>学習者のID</th>\n",
              "      <th>学習者の性別</th>\n",
              "      <th>学習環境</th>\n",
              "      <th>作文テーマ</th>\n",
              "      <th>学習者の母語</th>\n",
              "      <th>日本語学習履歴</th>\n",
              "      <th>日本語レベル</th>\n",
              "      <th>テスト成績（文字語彙）</th>\n",
              "      <th>テスト成績（文法）</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CG009</td>\n",
              "      <td>女</td>\n",
              "      <td>国外</td>\n",
              "      <td>外国語がうまくなる方法について</td>\n",
              "      <td>中国語</td>\n",
              "      <td>2年未満</td>\n",
              "      <td>初級</td>\n",
              "      <td>B</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CG011</td>\n",
              "      <td>女</td>\n",
              "      <td>国外</td>\n",
              "      <td>外国語がうまくなる方法について</td>\n",
              "      <td>中国語</td>\n",
              "      <td>2年未満</td>\n",
              "      <td>初級</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CG013</td>\n",
              "      <td>男</td>\n",
              "      <td>国外</td>\n",
              "      <td>外国語がうまくなる方法について</td>\n",
              "      <td>中国語</td>\n",
              "      <td>2年未満</td>\n",
              "      <td>上級</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CG015</td>\n",
              "      <td>男</td>\n",
              "      <td>国外</td>\n",
              "      <td>外国語がうまくなる方法について</td>\n",
              "      <td>中国語</td>\n",
              "      <td>2年未満</td>\n",
              "      <td>初級</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CG017</td>\n",
              "      <td>女</td>\n",
              "      <td>国外</td>\n",
              "      <td>外国語がうまくなる方法について</td>\n",
              "      <td>中国語</td>\n",
              "      <td>2年未満</td>\n",
              "      <td>中級</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>KN303</td>\n",
              "      <td>女</td>\n",
              "      <td>国内</td>\n",
              "      <td>外国語がうまくなる方法について</td>\n",
              "      <td>韓国語</td>\n",
              "      <td>2年未満</td>\n",
              "      <td>中級</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>KN307</td>\n",
              "      <td>女</td>\n",
              "      <td>国内</td>\n",
              "      <td>外国語がうまくなる方法について</td>\n",
              "      <td>韓国語</td>\n",
              "      <td>2年以上5年未満</td>\n",
              "      <td>上級</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>KN312</td>\n",
              "      <td>女</td>\n",
              "      <td>国内</td>\n",
              "      <td>外国語がうまくなる方法について</td>\n",
              "      <td>韓国語</td>\n",
              "      <td>2年以上5年未満</td>\n",
              "      <td>上級</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>KN313</td>\n",
              "      <td>男</td>\n",
              "      <td>国内</td>\n",
              "      <td>外国語がうまくなる方法について</td>\n",
              "      <td>韓国語</td>\n",
              "      <td>2年以上5年未満</td>\n",
              "      <td>上級</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>303</th>\n",
              "      <td>KN316</td>\n",
              "      <td>女</td>\n",
              "      <td>国内</td>\n",
              "      <td>外国語がうまくなる方法について</td>\n",
              "      <td>韓国語</td>\n",
              "      <td>5年以上</td>\n",
              "      <td>上級</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>304 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    学習者のID 学習者の性別 学習環境            作文テーマ 学習者の母語   日本語学習履歴 日本語レベル テスト成績（文字語彙）  \\\n",
              "0    CG009      女   国外  外国語がうまくなる方法について    中国語      2年未満     初級           B   \n",
              "1    CG011      女   国外  外国語がうまくなる方法について    中国語      2年未満     初級           B   \n",
              "2    CG013      男   国外  外国語がうまくなる方法について    中国語      2年未満     上級           A   \n",
              "3    CG015      男   国外  外国語がうまくなる方法について    中国語      2年未満     初級           B   \n",
              "4    CG017      女   国外  外国語がうまくなる方法について    中国語      2年未満     中級           B   \n",
              "..     ...    ...  ...              ...    ...       ...    ...         ...   \n",
              "299  KN303      女   国内  外国語がうまくなる方法について    韓国語      2年未満     中級           X   \n",
              "300  KN307      女   国内  外国語がうまくなる方法について    韓国語  2年以上5年未満     上級           X   \n",
              "301  KN312      女   国内  外国語がうまくなる方法について    韓国語  2年以上5年未満     上級           X   \n",
              "302  KN313      男   国内  外国語がうまくなる方法について    韓国語  2年以上5年未満     上級           X   \n",
              "303  KN316      女   国内  外国語がうまくなる方法について    韓国語      5年以上     上級           X   \n",
              "\n",
              "    テスト成績（文法）  \n",
              "0           C  \n",
              "1           B  \n",
              "2           A  \n",
              "3           B  \n",
              "4           A  \n",
              "..        ...  \n",
              "299         X  \n",
              "300         X  \n",
              "301         X  \n",
              "302         X  \n",
              "303         X  \n",
              "\n",
              "[304 rows x 9 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from os import listdir\n",
        "import os.path as osp\n",
        "import pandas as pd\n",
        "\n",
        "path = \"data/register.xls\"\n",
        "labels = pd.read_excel(path)\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e5a45fc-5165-4705-a4ab-b9cd5bbd5da3",
      "metadata": {
        "id": "9e5a45fc-5165-4705-a4ab-b9cd5bbd5da3",
        "outputId": "2f2f1efb-5051-4732-bfc5-bb78eebc8868"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'初級': 31, '上級': 124, '中級': 149})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import Counter\n",
        "a = Counter(labels.日本語レベル)\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98583a0b-403d-4212-8f28-f019e4a13f37",
      "metadata": {
        "id": "98583a0b-403d-4212-8f28-f019e4a13f37",
        "outputId": "ec537b9d-a0f1-4ed2-d868-832a2f3c34d7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>学習者のID</th>\n",
              "      <th>学習者の性別</th>\n",
              "      <th>学習環境</th>\n",
              "      <th>作文テーマ</th>\n",
              "      <th>学習者の母語</th>\n",
              "      <th>日本語学習履歴</th>\n",
              "      <th>日本語レベル</th>\n",
              "      <th>テスト成績（文字語彙）</th>\n",
              "      <th>テスト成績（文法）</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CG009</td>\n",
              "      <td>女</td>\n",
              "      <td>国外</td>\n",
              "      <td>外国語がうまくなる方法について</td>\n",
              "      <td>中国語</td>\n",
              "      <td>2年未満</td>\n",
              "      <td>初級</td>\n",
              "      <td>B</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CG011</td>\n",
              "      <td>女</td>\n",
              "      <td>国外</td>\n",
              "      <td>外国語がうまくなる方法について</td>\n",
              "      <td>中国語</td>\n",
              "      <td>2年未満</td>\n",
              "      <td>初級</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CG013</td>\n",
              "      <td>男</td>\n",
              "      <td>国外</td>\n",
              "      <td>外国語がうまくなる方法について</td>\n",
              "      <td>中国語</td>\n",
              "      <td>2年未満</td>\n",
              "      <td>上級</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CG015</td>\n",
              "      <td>男</td>\n",
              "      <td>国外</td>\n",
              "      <td>外国語がうまくなる方法について</td>\n",
              "      <td>中国語</td>\n",
              "      <td>2年未満</td>\n",
              "      <td>初級</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CG017</td>\n",
              "      <td>女</td>\n",
              "      <td>国外</td>\n",
              "      <td>外国語がうまくなる方法について</td>\n",
              "      <td>中国語</td>\n",
              "      <td>2年未満</td>\n",
              "      <td>中級</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>KN303</td>\n",
              "      <td>女</td>\n",
              "      <td>国内</td>\n",
              "      <td>外国語がうまくなる方法について</td>\n",
              "      <td>韓国語</td>\n",
              "      <td>2年未満</td>\n",
              "      <td>中級</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>KN307</td>\n",
              "      <td>女</td>\n",
              "      <td>国内</td>\n",
              "      <td>外国語がうまくなる方法について</td>\n",
              "      <td>韓国語</td>\n",
              "      <td>2年以上5年未満</td>\n",
              "      <td>上級</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>KN312</td>\n",
              "      <td>女</td>\n",
              "      <td>国内</td>\n",
              "      <td>外国語がうまくなる方法について</td>\n",
              "      <td>韓国語</td>\n",
              "      <td>2年以上5年未満</td>\n",
              "      <td>上級</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>KN313</td>\n",
              "      <td>男</td>\n",
              "      <td>国内</td>\n",
              "      <td>外国語がうまくなる方法について</td>\n",
              "      <td>韓国語</td>\n",
              "      <td>2年以上5年未満</td>\n",
              "      <td>上級</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>303</th>\n",
              "      <td>KN316</td>\n",
              "      <td>女</td>\n",
              "      <td>国内</td>\n",
              "      <td>外国語がうまくなる方法について</td>\n",
              "      <td>韓国語</td>\n",
              "      <td>5年以上</td>\n",
              "      <td>上級</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>192 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    学習者のID 学習者の性別 学習環境            作文テーマ 学習者の母語   日本語学習履歴 日本語レベル テスト成績（文字語彙）  \\\n",
              "0    CG009      女   国外  外国語がうまくなる方法について    中国語      2年未満     初級           B   \n",
              "1    CG011      女   国外  外国語がうまくなる方法について    中国語      2年未満     初級           B   \n",
              "2    CG013      男   国外  外国語がうまくなる方法について    中国語      2年未満     上級           A   \n",
              "3    CG015      男   国外  外国語がうまくなる方法について    中国語      2年未満     初級           B   \n",
              "4    CG017      女   国外  外国語がうまくなる方法について    中国語      2年未満     中級           B   \n",
              "..     ...    ...  ...              ...    ...       ...    ...         ...   \n",
              "299  KN303      女   国内  外国語がうまくなる方法について    韓国語      2年未満     中級           X   \n",
              "300  KN307      女   国内  外国語がうまくなる方法について    韓国語  2年以上5年未満     上級           X   \n",
              "301  KN312      女   国内  外国語がうまくなる方法について    韓国語  2年以上5年未満     上級           X   \n",
              "302  KN313      男   国内  外国語がうまくなる方法について    韓国語  2年以上5年未満     上級           X   \n",
              "303  KN316      女   国内  外国語がうまくなる方法について    韓国語      5年以上     上級           X   \n",
              "\n",
              "    テスト成績（文法）  \n",
              "0           C  \n",
              "1           B  \n",
              "2           A  \n",
              "3           B  \n",
              "4           A  \n",
              "..        ...  \n",
              "299         X  \n",
              "300         X  \n",
              "301         X  \n",
              "302         X  \n",
              "303         X  \n",
              "\n",
              "[192 rows x 9 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "label1 = labels.loc[labels.作文テーマ == \"外国語がうまくなる方法について\",:]\n",
        "label1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ae02239-03db-4330-b29d-d68fc8164afd",
      "metadata": {
        "id": "2ae02239-03db-4330-b29d-d68fc8164afd",
        "outputId": "f22d7756-73b8-440e-e4c8-29f42411c6b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'初級': 31, '上級': 61, '中級': 100})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b = Counter(label1.日本語レベル)\n",
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "834e01d4-9aac-47c2-97bb-142ca7a152b9",
      "metadata": {
        "id": "834e01d4-9aac-47c2-97bb-142ca7a152b9",
        "outputId": "23c1cfc9-7a11-4765-df73-95583e4530b4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>学習者のID</th>\n",
              "      <th>学習者の性別</th>\n",
              "      <th>学習環境</th>\n",
              "      <th>作文テーマ</th>\n",
              "      <th>学習者の母語</th>\n",
              "      <th>日本語学習履歴</th>\n",
              "      <th>日本語レベル</th>\n",
              "      <th>テスト成績（文字語彙）</th>\n",
              "      <th>テスト成績（文法）</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>CG101</td>\n",
              "      <td>男</td>\n",
              "      <td>国外</td>\n",
              "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
              "      <td>中国語</td>\n",
              "      <td>2年以上5年未満</td>\n",
              "      <td>中級</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>CG102</td>\n",
              "      <td>女</td>\n",
              "      <td>国外</td>\n",
              "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
              "      <td>中国語</td>\n",
              "      <td>2年以上5年未満</td>\n",
              "      <td>中級</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>CG103</td>\n",
              "      <td>女</td>\n",
              "      <td>国外</td>\n",
              "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
              "      <td>中国語</td>\n",
              "      <td>2年以上5年未満</td>\n",
              "      <td>中級</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>CG104</td>\n",
              "      <td>女</td>\n",
              "      <td>国外</td>\n",
              "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
              "      <td>中国語</td>\n",
              "      <td>2年以上5年未満</td>\n",
              "      <td>中級</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>CG105</td>\n",
              "      <td>女</td>\n",
              "      <td>国外</td>\n",
              "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
              "      <td>中国語</td>\n",
              "      <td>2年以上5年未満</td>\n",
              "      <td>上級</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>KG151</td>\n",
              "      <td>女</td>\n",
              "      <td>国外</td>\n",
              "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
              "      <td>韓国語</td>\n",
              "      <td>5年以上</td>\n",
              "      <td>上級</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>KG152</td>\n",
              "      <td>女</td>\n",
              "      <td>国外</td>\n",
              "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
              "      <td>韓国語</td>\n",
              "      <td>2年以上5年未満</td>\n",
              "      <td>上級</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294</th>\n",
              "      <td>KG153</td>\n",
              "      <td>女</td>\n",
              "      <td>国外</td>\n",
              "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
              "      <td>韓国語</td>\n",
              "      <td>2年以上5年未満</td>\n",
              "      <td>上級</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>KG154</td>\n",
              "      <td>女</td>\n",
              "      <td>国外</td>\n",
              "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
              "      <td>韓国語</td>\n",
              "      <td>2年未満</td>\n",
              "      <td>中級</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>KG155</td>\n",
              "      <td>女</td>\n",
              "      <td>国外</td>\n",
              "      <td>インターネット時代に新聞や雑誌は必要か</td>\n",
              "      <td>韓国語</td>\n",
              "      <td>5年以上</td>\n",
              "      <td>上級</td>\n",
              "      <td>X</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>112 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    学習者のID 学習者の性別 学習環境                作文テーマ 学習者の母語   日本語学習履歴 日本語レベル  \\\n",
              "76   CG101      男   国外  インターネット時代に新聞や雑誌は必要か    中国語  2年以上5年未満     中級   \n",
              "77   CG102      女   国外  インターネット時代に新聞や雑誌は必要か    中国語  2年以上5年未満     中級   \n",
              "78   CG103      女   国外  インターネット時代に新聞や雑誌は必要か    中国語  2年以上5年未満     中級   \n",
              "79   CG104      女   国外  インターネット時代に新聞や雑誌は必要か    中国語  2年以上5年未満     中級   \n",
              "80   CG105      女   国外  インターネット時代に新聞や雑誌は必要か    中国語  2年以上5年未満     上級   \n",
              "..     ...    ...  ...                  ...    ...       ...    ...   \n",
              "292  KG151      女   国外  インターネット時代に新聞や雑誌は必要か    韓国語      5年以上     上級   \n",
              "293  KG152      女   国外  インターネット時代に新聞や雑誌は必要か    韓国語  2年以上5年未満     上級   \n",
              "294  KG153      女   国外  インターネット時代に新聞や雑誌は必要か    韓国語  2年以上5年未満     上級   \n",
              "295  KG154      女   国外  インターネット時代に新聞や雑誌は必要か    韓国語      2年未満     中級   \n",
              "296  KG155      女   国外  インターネット時代に新聞や雑誌は必要か    韓国語      5年以上     上級   \n",
              "\n",
              "    テスト成績（文字語彙） テスト成績（文法）  \n",
              "76            X         X  \n",
              "77            X         X  \n",
              "78            X         X  \n",
              "79            X         X  \n",
              "80            X         X  \n",
              "..          ...       ...  \n",
              "292           X         X  \n",
              "293           X         X  \n",
              "294           X         X  \n",
              "295           X         X  \n",
              "296           X         X  \n",
              "\n",
              "[112 rows x 9 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label2 = labels.loc[labels.作文テーマ == \"インターネット時代に新聞や雑誌は必要か\",:]\n",
        "label2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2be1bc8-8312-4010-b717-64244a03c21b",
      "metadata": {
        "id": "d2be1bc8-8312-4010-b717-64244a03c21b",
        "outputId": "3331d0b3-6928-48de-bbb9-e15434734d7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'中級': 49, '上級': 63})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "c = Counter(label2.日本語レベル)\n",
        "c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1ec06dd-dcd9-4ff8-be26-c2eb5e3feef9",
      "metadata": {
        "id": "c1ec06dd-dcd9-4ff8-be26-c2eb5e3feef9",
        "outputId": "6f4b5e7b-4319-4bc8-9c06-80dd41a3f749"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array(['初級', '初級', '上級', '初級', '中級', '中級', '中級', '上級', '中級', '中級', '中級',\n",
              "        '上級', '中級', '中級', '中級', '初級', '中級', '上級', '初級', '初級', '中級', '中級',\n",
              "        '初級', '中級', '中級', '上級', '中級', '中級', '初級', '上級', '上級', '初級', '中級',\n",
              "        '中級', '上級', '中級', '中級', '初級', '初級', '中級', '上級', '中級', '上級', '中級',\n",
              "        '上級', '中級', '初級', '中級', '中級', '初級', '上級', '初級', '中級', '中級', '上級',\n",
              "        '初級', '初級', '上級', '上級', '中級', '中級', '中級', '上級', '中級', '中級', '上級',\n",
              "        '中級', '中級', '中級', '初級', '中級', '初級', '中級', '中級', '上級', '上級', '初級',\n",
              "        '上級', '中級', '中級', '上級', '中級', '中級', '中級', '中級', '中級', '中級', '中級',\n",
              "        '中級', '中級', '上級', '中級', '中級', '初級', '上級', '上級', '上級', '上級', '上級',\n",
              "        '中級', '中級', '中級', '中級', '初級', '中級', '中級', '初級', '中級', '中級', '中級',\n",
              "        '上級', '中級', '上級', '上級', '上級', '上級', '上級', '上級', '上級', '中級', '中級',\n",
              "        '上級', '中級', '上級', '上級', '中級', '上級', '上級', '中級', '中級', '初級', '中級',\n",
              "        '中級', '上級', '中級', '上級', '上級', '上級', '中級', '上級', '中級', '上級', '上級',\n",
              "        '上級', '中級', '初級', '中級', '中級', '上級', '上級', '中級', '中級', '上級', '上級',\n",
              "        '上級', '中級', '初級', '中級', '中級', '中級', '上級', '中級', '初級', '中級', '中級',\n",
              "        '中級', '上級', '初級', '上級', '中級', '中級', '中級', '初級', '中級', '中級', '中級',\n",
              "        '中級', '初級', '中級', '初級', '初級', '中級', '中級', '中級', '中級', '上級', '中級',\n",
              "        '中級', '上級', '上級', '上級', '上級'], dtype=object),\n",
              " 192)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "level1 = np.array(label1.日本語レベル)\n",
        "level1, len(level1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c847ac2a-1c01-4872-b656-12ebc25ad33c",
      "metadata": {
        "id": "c847ac2a-1c01-4872-b656-12ebc25ad33c",
        "outputId": "19ad7cfe-19ac-4ced-c51d-f757ac485475"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array(['中級', '中級', '中級', '中級', '上級', '中級', '中級', '中級', '上級', '中級', '中級',\n",
              "        '中級', '中級', '上級', '上級', '中級', '中級', '中級', '上級', '中級', '中級', '中級',\n",
              "        '中級', '上級', '中級', '中級', '中級', '中級', '中級', '上級', '中級', '中級', '中級',\n",
              "        '上級', '中級', '中級', '中級', '中級', '中級', '中級', '中級', '上級', '中級', '中級',\n",
              "        '中級', '中級', '上級', '中級', '中級', '上級', '中級', '中級', '中級', '上級', '中級',\n",
              "        '上級', '上級', '上級', '上級', '中級', '上級', '上級', '上級', '中級', '上級', '上級',\n",
              "        '中級', '上級', '上級', '上級', '中級', '上級', '上級', '上級', '上級', '上級', '上級',\n",
              "        '上級', '上級', '上級', '上級', '上級', '上級', '中級', '上級', '上級', '上級', '上級',\n",
              "        '上級', '上級', '上級', '上級', '上級', '上級', '上級', '上級', '上級', '上級', '上級',\n",
              "        '上級', '上級', '上級', '上級', '上級', '上級', '上級', '上級', '上級', '上級', '上級',\n",
              "        '中級', '上級'], dtype=object),\n",
              " 112)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "level2 = np.array(label2.日本語レベル)\n",
        "level2, len(level2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8c86efb-bf58-4252-be6f-ca08ba4da7a8",
      "metadata": {
        "id": "c8c86efb-bf58-4252-be6f-ca08ba4da7a8"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "txt_path = \"data/txt/\"\n",
        "txt_topics = listdir(txt_path)\n",
        "\n",
        "txt_gaigo_path = txt_path + txt_topics[0]\n",
        "txt_internet_path = txt_path + txt_topics[1]\n",
        "\n",
        "txt_gaigo_files = listdir(txt_gaigo_path)\n",
        "txt_internet_files = listdir(txt_internet_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5889ef08-8537-4546-a485-ddfdd2aa65b1",
      "metadata": {
        "id": "5889ef08-8537-4546-a485-ddfdd2aa65b1",
        "outputId": "cd5c8377-b367-41b5-ce16-e3365ac939f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "192"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import os.path as osp\n",
        "# import re\n",
        "# gaigo_files = [f for f in listdir(osp.join(txt_path, txt_topics[0])) if f.endswith('.txt')]\n",
        "# gaigo = []\n",
        "# for gaigo_file in gaigo_files:\n",
        "#     # print(gaigo_file)\n",
        "#     with open(osp.join(txt_path, txt_topics[0], gaigo_file), \"r\", encoding=\"utf-8\") as f:\n",
        "#         lines = f.read()\n",
        "#         # type(lines)\n",
        "#         lines = re.sub('\\n', '', lines)\n",
        "#         lines = re.sub('\\u3000', '', lines)\n",
        "#     gaigo.append(lines)\n",
        "# # type(gaigo[0])\n",
        "# # gaigo[0][0]\n",
        "\n",
        "import os.path as osp\n",
        "import re\n",
        "gaigo_files = [f for f in listdir(osp.join(txt_path, txt_topics[0])) if f.endswith('.txt')]\n",
        "gaigo = []\n",
        "for gaigo_file in gaigo_files:\n",
        "    # print(gaigo_file)\n",
        "    with open(osp.join(txt_path, txt_topics[0], gaigo_file), \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.read()\n",
        "        # type(lines)\n",
        "        lines = re.sub(r'[0-9０-９a-zA-Zａ-ｚＡ-Ｚ]+', \" \", lines)\n",
        "        lines = re.sub(r'[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒—●★☆〇◎◆▼◇△□(：〜～＋=)／*&^%$#@!~`){}・［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕〈〉？、。・,\\./『』【】「」→←○《》≪≫\\n\\u3000]+', \"\", lines)\n",
        "        lines = lines.replace(\" \", \"\")\n",
        "\n",
        "\n",
        "    gaigo.append(lines)\n",
        "# type(gaigo[0])\n",
        "len(gaigo)\n",
        "\n",
        "# 注意。注釈は削除していない。記号のみ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec8cc9c2-1357-4ff9-90ce-b5585ec755b5",
      "metadata": {
        "id": "ec8cc9c2-1357-4ff9-90ce-b5585ec755b5",
        "outputId": "7c42c261-a99a-4a4d-bce3-8af6bc4a7ab9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "112"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import re\n",
        "# internet_files = [f for f in listdir(osp.join(txt_path, txt_topics[1])) if f.endswith('.txt')]\n",
        "# internet = []\n",
        "# for internet_file in internet_files:\n",
        "#     with open(osp.join(txt_path, txt_topics[1], internet_file), \"r\", encoding=\"utf-8\") as f:\n",
        "#         lines = f.read()\n",
        "#         # print(lines)\n",
        "#         lines = lines.strip('\\ufeff□')\n",
        "#         lines = re.sub('■', '', lines)\n",
        "#         lines = re.sub('\\n', '', lines)\n",
        "#         lines = re.sub('□', '', lines)\n",
        "#         lines = re.sub('\\u3000', '', lines)\n",
        "        \n",
        "#     internet.append(lines)\n",
        "\n",
        "# internet[0]\n",
        "\n",
        "import re\n",
        "internet_files = [f for f in listdir(osp.join(txt_path, txt_topics[1])) if f.endswith('.txt')]\n",
        "internet = []\n",
        "for internet_file in internet_files:\n",
        "    with open(osp.join(txt_path, txt_topics[1], internet_file), \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.read()\n",
        "        # print(lines)\n",
        "        lines = lines.strip('\\ufeff□')\n",
        "        lines = re.sub(r'[0-9０-９a-zA-Zａ-ｚＡ-Ｚ]+', \" \", lines)\n",
        "        lines = re.sub(r'[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒—●★☆〇◎◆▼◇△□(：〜～＋=)／*&^%$#@!~`){}・［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕〈〉？、。・,\\./『』【】「」→←○《》≪≫\\n\\u3000]+', \"\", lines)\n",
        "        lines = lines.replace(\" \", \"\")\n",
        "        \n",
        "    internet.append(lines)\n",
        "\n",
        "len(internet)\n",
        "# + delete ・・・"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6080253b-8225-4834-b212-f6da98da0835",
      "metadata": {
        "id": "6080253b-8225-4834-b212-f6da98da0835",
        "outputId": "8410c43a-6e38-4cdb-ed1c-33148c572912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31\n",
            "124\n",
            "149\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict as dd\n",
        "text_tagged = dd(list)\n",
        "for level, txt in zip(level1, gaigo):\n",
        "    text_tagged[level].append(txt)\n",
        "\n",
        "for level, txt in zip(level2, internet):\n",
        "    text_tagged[level].append(txt)\n",
        "\n",
        "text_tagged_dict = dict(text_tagged)\n",
        "# text_tagged_dict\n",
        "for level in text_tagged_dict.keys():\n",
        "    print(len(text_tagged_dict[level]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efc35f04-43f2-49e7-929c-e29f49948bd4",
      "metadata": {
        "id": "efc35f04-43f2-49e7-929c-e29f49948bd4"
      },
      "outputs": [],
      "source": [
        "for level in text_tagged_dict.keys():\n",
        "    text_tagged_dict[level] = \"\".join(text_tagged_dict[level])\n",
        "        \n",
        "# text_tagged_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec0de867-5c50-4c98-b8d6-43dd0b155c73",
      "metadata": {
        "id": "ec0de867-5c50-4c98-b8d6-43dd0b155c73"
      },
      "outputs": [],
      "source": [
        "import pickle \n",
        "# with open('text_tagged_dict2_joined.pickle', 'wb') as f:\n",
        "#     pickle.dump(text_tagged_dict, f)\n",
        "    \n",
        "text_tagged_dict2_joined = pickle.load(open('text_tagged_dict2_joined.pickle', 'rb'))\n",
        "# text_tagged_dict2_joined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cd19413-0a78-491a-a043-5725826708be",
      "metadata": {
        "id": "9cd19413-0a78-491a-a043-5725826708be"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# text_length_dist = [len(text) for level in text_tagged_dict2.keys() for text in text_tagged_dict2[level]]\n",
        "# # [len(text) for text in text_tagged_dict2[\"初級\"]]\n",
        "# # text_length_dist\n",
        "\n",
        "# # np.max(text_length_dist) = 914\n",
        "# # to determine how many paddings we should add \n",
        "# plt.hist(text_length_dist, bins = np.arange(0, 1000, 10))\n",
        "# plt.show()\n",
        "# # we can determine the max length of texts as 1000 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7df0bec8-4824-42bf-bc32-0adffa428ae8",
      "metadata": {
        "id": "7df0bec8-4824-42bf-bc32-0adffa428ae8",
        "outputId": "436e34e9-78f1-43df-8526-e27d1f627d8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'私は大学日本語科の一'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_tagged_dict2_joined[\"初級\"][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1354c976-0341-4a4d-96d5-57a866b755ff",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "1354c976-0341-4a4d-96d5-57a866b755ff"
      },
      "outputs": [],
      "source": [
        "text_tagged_joined2 = {\"初級\": list(), \"上級\": list(), \"中級\": list()}\n",
        "text_tagged_joined2[\"初級\"].append(text_tagged_dict2_joined[\"初級\"])\n",
        "text_len_advanced = len(text_tagged_dict2_joined[\"上級\"])\n",
        "text_len_intermediate = len(text_tagged_dict2_joined[\"中級\"])\n",
        "\n",
        "for i in range(5):\n",
        "    text_tagged_joined2[\"上級\"].append(text_tagged_dict2_joined[\"上級\"][int(i * text_len_advanced / 5) : int((i + 1) * text_len_advanced / 5)])\n",
        "\n",
        "for i in range(6):\n",
        "    text_tagged_joined2[\"中級\"].append(text_tagged_dict2_joined[\"中級\"][int(i * text_len_intermediate / 6) : int((i + 1) * text_len_intermediate / 6)])\n",
        "\n",
        "# text_tagged_joined2[\"中級\"][5]\n",
        "# text_tagged_dict2_joined[\"中級\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6bd5880-8a56-437d-9d98-1b45a7244c85",
      "metadata": {
        "id": "e6bd5880-8a56-437d-9d98-1b45a7244c85",
        "outputId": "84930939-1e3c-4194-edff-fd0faed69fcc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "72690"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_len_advanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5650580b-c46f-4f42-956f-103d996a6ec6",
      "metadata": {
        "id": "5650580b-c46f-4f42-956f-103d996a6ec6",
        "outputId": "db46d106-150f-47bc-fa97-46f9a8763527"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "84307"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_len_intermediate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ff682c6-ce89-44ad-a2e1-38eb842e4883",
      "metadata": {
        "id": "0ff682c6-ce89-44ad-a2e1-38eb842e4883",
        "outputId": "8aa7c5ea-48c4-48a4-d3d8-decb17379039"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13407\n",
            "14538\n",
            "14051\n"
          ]
        }
      ],
      "source": [
        "for text in text_tagged_joined2.values():\n",
        "    print(len(text[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68d1c9a7-b298-42ed-9bde-70d575e3d680",
      "metadata": {
        "id": "68d1c9a7-b298-42ed-9bde-70d575e3d680",
        "outputId": "c300876b-4fba-4b8f-dce3-b0ae00707834"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:30<00:00, 10.10s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('ja_ginza')\n",
        "\n",
        "text_vectorized = {}\n",
        "for level in tqdm(text_tagged_joined2.keys()):\n",
        "    text_vectorized[level] = []\n",
        "    for text in text_tagged_joined2[level]:\n",
        "        tokenized = nlp(text)\n",
        "        for token in tokenized:\n",
        "            text_vectorized[level].append(token.vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfc63193-6fab-4181-92dd-3d0452c35c2c",
      "metadata": {
        "id": "cfc63193-6fab-4181-92dd-3d0452c35c2c",
        "outputId": "a669c4b7-a458-479d-eb24-856477e598b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7655\n",
            "41449\n",
            "48524\n"
          ]
        }
      ],
      "source": [
        "# for i in text_vectorized.values():\n",
        "#     print(len(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a2682a-7ded-4e4e-840e-64046523a06a",
      "metadata": {
        "id": "a1a2682a-7ded-4e4e-840e-64046523a06a"
      },
      "outputs": [],
      "source": [
        "import pickle \n",
        "# with open('text_vectorized_joined.pickle', 'wb') as f:\n",
        "#     pickle.dump(text_vectorized, f)\n",
        "    \n",
        "text_vectorized2 = pickle.load(open('text_vectorized_joined.pickle', 'rb'))\n",
        "# text_vectorized2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dd96490-8457-4531-83a1-fbfa68c1e4dc",
      "metadata": {
        "tags": [],
        "id": "1dd96490-8457-4531-83a1-fbfa68c1e4dc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "window_size = 15\n",
        "n_sample = 1000 # less than or equal to (len(text_vectorized2[\"初級\"]) - window_size)\n",
        "\n",
        "\n",
        "text_transformed = {}\n",
        "for level, vector in text_vectorized2.items():\n",
        "    text_transformed[level] = []\n",
        "    start_index_list = np.random.permutation(list(range(len(text_vectorized2[level]) - window_size)))\n",
        "    for i in range(n_sample):\n",
        "        start_index = start_index_list[i]\n",
        "        # print(start_index)\n",
        "        chunks = []\n",
        "        for index in range(start_index, start_index + window_size):\n",
        "            chunks.append(text_vectorized2[level][index])\n",
        "        text_transformed[level].append(chunks)\n",
        "    text_transformed[level] = np.array(text_transformed[level])\n",
        "    \n",
        "# print(text_transformed)\n",
        "# text_transformed[\"初級\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93216d7f-f969-4d44-ab58-64c423366e29",
      "metadata": {
        "id": "93216d7f-f969-4d44-ab58-64c423366e29",
        "outputId": "b2e65f72-ac3e-4b49-d3a9-75bfdf504901"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "初級 (1000, 15, 300)\n",
            "上級 (1000, 15, 300)\n",
            "中級 (1000, 15, 300)\n"
          ]
        }
      ],
      "source": [
        "for level, value in text_transformed.items():\n",
        "    print(level, value.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dgIN64tnlFd",
        "outputId": "6470714f-9119-44ff-961c-aa2bcb475219"
      },
      "id": "0dgIN64tnlFd",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e5468d86-f1de-477c-be4c-758fa8981c2b",
      "metadata": {
        "id": "e5468d86-f1de-477c-be4c-758fa8981c2b"
      },
      "outputs": [],
      "source": [
        "import pickle \n",
        "# with open('text_transformed_window_size15_sample_size1000.pickle', 'wb') as f:\n",
        "#     pickle.dump(text_transformed, f)\n",
        "    \n",
        "text_transformed2 = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/nlp_project/text_transformed_window_size15_sample_size1000.pickle', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "751d8c39-6f9d-4150-8ac9-2c152a81d4c7",
      "metadata": {
        "id": "751d8c39-6f9d-4150-8ac9-2c152a81d4c7"
      },
      "outputs": [],
      "source": [
        "category_vectors = []\n",
        "for level, vectors in text_transformed2.items():\n",
        "    for vector in vectors:\n",
        "        category_vectors.append((level, vector))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "24dc7fa5-a9b2-4559-9d34-bf2279a54528",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "24dc7fa5-a9b2-4559-9d34-bf2279a54528",
        "outputId": "426c8c11-63d4-4e47-a38c-33f2d6f862f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  cat                                              value\n",
              "0  初級  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "1  初級  [[0.47779918, -0.290014, 0.35474735, -0.094679...\n",
              "2  初級  [[-0.11235832, -0.04813227, -0.2808526, 0.0092...\n",
              "3  初級  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
              "4  初級  [[-0.084042855, -0.03873583, 0.013529049, -0.1..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8e7c5022-1d54-4307-8b51-927a6bdb4c06\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cat</th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>初級</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>初級</td>\n",
              "      <td>[[0.47779918, -0.290014, 0.35474735, -0.094679...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>初級</td>\n",
              "      <td>[[-0.11235832, -0.04813227, -0.2808526, 0.0092...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>初級</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>初級</td>\n",
              "      <td>[[-0.084042855, -0.03873583, 0.013529049, -0.1...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e7c5022-1d54-4307-8b51-927a6bdb4c06')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8e7c5022-1d54-4307-8b51-927a6bdb4c06 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8e7c5022-1d54-4307-8b51-927a6bdb4c06');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "key, value = zip(*category_vectors)\n",
        "data2 = pd.DataFrame({'cat': key, 'value': value})\n",
        "\n",
        "data2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "cf9d0590-7607-4165-8ab9-7254250d7d16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf9d0590-7607-4165-8ab9-7254250d7d16",
        "outputId": "ddbe3c8e-befa-4acf-8dc6-3c059b3d2186"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.0239743 ,  0.01262705,  0.02503493, -0.25055572, -0.00230157,\n",
              "        0.12320483, -0.24744102,  0.19238903, -0.24068387,  0.07185157,\n",
              "        0.12069177,  0.01762237, -0.08773331, -0.04897095, -0.22160628,\n",
              "       -0.17144617, -0.16414927, -0.11867727, -0.23981866,  0.04262594,\n",
              "       -0.31085443,  0.39227724,  0.00928467,  0.03484426, -0.01108476,\n",
              "       -0.01912511, -0.2160065 , -0.08861484,  0.19086163,  0.03391807,\n",
              "        0.02015168, -0.04170526,  0.2553586 ,  0.17107923,  0.10361891,\n",
              "        0.01196844, -0.10293758,  0.27045432, -0.09230511, -0.03678845,\n",
              "        0.37546104, -0.01957306, -0.09742833, -0.35306036, -0.28462362,\n",
              "       -0.15244167, -0.03874411,  0.00315991, -0.04927937,  0.13574249,\n",
              "       -0.12446789, -0.08645909,  0.03322214,  0.02640729,  0.04219476,\n",
              "        0.05602331,  0.11965343,  0.27279937,  0.01554249,  0.01614439,\n",
              "       -0.11878366,  0.04147785,  0.13128603, -0.05654456, -0.05122675,\n",
              "       -0.06118861, -0.07684646,  0.22377348,  0.24619152, -0.08158962,\n",
              "        0.0446958 ,  0.29018402, -0.21988262, -0.03134859,  0.14315976,\n",
              "       -0.26347217,  0.4352744 ,  0.01394558,  0.08699139,  0.16615725,\n",
              "        0.05094247, -0.22797051,  0.14706281, -0.01875144, -0.11322079,\n",
              "        0.21501613, -0.17008398,  0.09581278,  0.01105864, -0.012698  ,\n",
              "       -0.19812371, -0.09280668, -0.06243419,  0.13035034,  0.03231481,\n",
              "        0.08170561,  0.09568913, -0.19996   , -0.20375828,  0.1102109 ,\n",
              "        0.2700572 , -0.22923128,  0.23323098, -0.02231162, -0.16096988,\n",
              "       -0.13470174,  0.30878872, -0.03357483,  0.2793544 , -0.11335952,\n",
              "        0.17044185, -0.24824052, -0.03182455, -0.00577084, -0.07647344,\n",
              "       -0.13862638, -0.07179846, -0.15278721,  0.11252756,  0.05706834,\n",
              "       -0.19557694, -0.13512594, -0.03472605, -0.11154974,  0.07853797,\n",
              "       -0.242282  , -0.21703291, -0.04220525,  0.00079433, -0.11839225,\n",
              "       -0.25316057, -0.29269683, -0.10500497, -0.295238  , -0.25281382,\n",
              "        0.08656443,  0.02474758,  0.09212983, -0.18788163,  0.02076911,\n",
              "       -0.2097765 , -0.08586384, -0.15965988,  0.17289172, -0.06660441,\n",
              "       -0.05782462, -0.18491855, -0.37078103,  0.11714315, -0.05513881,\n",
              "       -0.1293844 , -0.08997952,  0.35132077, -0.15383011, -0.05214553,\n",
              "        0.31239977, -0.06668464, -0.04255849, -0.1628003 ,  0.10601627,\n",
              "        0.19490924, -0.10263088, -0.05020073, -0.0035387 ,  0.17330359,\n",
              "       -0.00929459, -0.27283534, -0.07605983,  0.00530626, -0.24268126,\n",
              "        0.22885048, -0.17224948,  0.05874478, -0.13016455, -0.07099041,\n",
              "        0.11081818,  0.05715383,  0.28738433,  0.37253043, -0.13353206,\n",
              "        0.07575694, -0.105288  ,  0.04868092, -0.07574604,  0.03016348,\n",
              "        0.05522351,  0.2588987 , -0.0241141 , -0.13989195,  0.04892332,\n",
              "        0.08168682, -0.10931995, -0.00869416, -0.14725932, -0.09070682,\n",
              "       -0.08476355,  0.02247556,  0.00794374,  0.16917613,  0.07600649,\n",
              "        0.10997821, -0.3419397 , -0.07662439, -0.00333167, -0.04747627,\n",
              "        0.03552777, -0.02760449,  0.0201211 ,  0.19523387,  0.05651283,\n",
              "       -0.10055898, -0.03165564,  0.20466444, -0.15533133,  0.04621852,\n",
              "       -0.08016416, -0.09735517, -0.04690261,  0.18381757, -0.13739185,\n",
              "       -0.08320837, -0.04270866,  0.09985469,  0.16442402,  0.00998202,\n",
              "       -0.0999382 , -0.00377265, -0.02578402, -0.03502601, -0.10311011,\n",
              "        0.09421798, -0.15727876, -0.19565219,  0.16500385,  0.09692209,\n",
              "        0.10686832, -0.11394666,  0.06778985, -0.253045  , -0.04350044,\n",
              "        0.26257432,  0.00608369,  0.05373707, -0.00622181, -0.17440921,\n",
              "        0.21241365, -0.2736177 , -0.12017059, -0.15798289,  0.09085845,\n",
              "        0.13573949,  0.03920018,  0.0037802 , -0.00820611, -0.26012146,\n",
              "       -0.0431843 ,  0.04523035, -0.3415437 ,  0.1969927 , -0.08818417,\n",
              "       -0.06942731, -0.23516285,  0.10983916, -0.00713349, -0.15120167,\n",
              "        0.02595977, -0.15150274, -0.05534105,  0.06908288, -0.06254087,\n",
              "       -0.05551073,  0.42564517, -0.03673264, -0.08363806, -0.01990834,\n",
              "        0.11324922,  0.12172341,  0.11478925, -0.03111299,  0.11487505,\n",
              "       -0.00552405,  0.16992226, -0.06122524,  0.04596264,  0.07082573,\n",
              "       -0.11657621,  0.16424964, -0.25042444,  0.04935465, -0.00570252,\n",
              "       -0.06607542,  0.06595723,  0.20027041, -0.16055797, -0.01730892,\n",
              "        0.32721677, -0.12105057, -0.10999383,  0.07808415, -0.06817568],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "data2.iloc[0][1][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e631c80f-2b73-4a17-a3a1-94d696166343",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e631c80f-2b73-4a17-a3a1-94d696166343",
        "outputId": "d975ddf2-168f-41ae-df5f-704acd955e65"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((600, 2), (2400, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "test = data2.sample(frac=0.2, random_state=200)\n",
        "train = data2.drop(test.index)\n",
        "\n",
        "test.shape, train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5c8a5af5-616a-4893-831b-aaeb36cb07ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c8a5af5-616a-4893-831b-aaeb36cb07ed",
        "outputId": "7bdae589-c131-405b-f1da-1a3eaba5167e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2400, 15, 300), (2400, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "ohe = preprocessing.OneHotEncoder()\n",
        "\n",
        "le.fit(data2.cat)\n",
        "y_train = le.transform(train.cat).reshape(-1, 1)\n",
        "ohe.fit(y_train)\n",
        "y_train = ohe.transform(y_train).todense()\n",
        "\n",
        "X_train = np.array([x for x in train.value])\n",
        "\n",
        "X_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "55ef04c3-43f6-4792-9ae1-ebe94318b5e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55ef04c3-43f6-4792-9ae1-ebe94318b5e7",
        "outputId": "f2537b3d-bfab-4c66-be38-2d16bd4a1c8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 256)               570368    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 300)               77100     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 903       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 648,371\n",
            "Trainable params: 648,371\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "\n",
        "n_rnn = window_size = 15 # 時系列の数\n",
        "batch_size = 128\n",
        "epochs = 20  #epochsは、多いほど、精密に学習するが、重くなるため今回は小さくしている\n",
        "n_mid = 256  # 中間層のニューロン数\n",
        "data_dim = 300\n",
        "\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(LSTM(n_mid, input_shape=(n_rnn, data_dim)))\n",
        "model_lstm.add(Dense(data_dim, activation=\"relu\"))\n",
        "model_lstm.add(Dense(3, activation=\"softmax\"))\n",
        "model_lstm.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n",
        "print(model_lstm.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "13267e31-ab0e-4cac-83fd-49d99eccb2e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13267e31-ab0e-4cac-83fd-49d99eccb2e2",
        "outputId": "a5f2555a-23a7-4cb9-a33f-5eca8745e1f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "75/75 [==============================] - 7s 5ms/step - loss: 1.0316 - accuracy: 0.4296\n",
            "Epoch 2/20\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.9746 - accuracy: 0.4988\n",
            "Epoch 3/20\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.9704 - accuracy: 0.5179\n",
            "Epoch 4/20\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.9227 - accuracy: 0.5242\n",
            "Epoch 5/20\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 0.8715 - accuracy: 0.5596\n",
            "Epoch 6/20\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.8555 - accuracy: 0.5683\n",
            "Epoch 7/20\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.7873 - accuracy: 0.6183\n",
            "Epoch 8/20\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.7521 - accuracy: 0.6408\n",
            "Epoch 9/20\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.6869 - accuracy: 0.6837\n",
            "Epoch 10/20\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.6299 - accuracy: 0.7117\n",
            "Epoch 11/20\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.5538 - accuracy: 0.7558\n",
            "Epoch 12/20\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.5093 - accuracy: 0.7729\n",
            "Epoch 13/20\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.4197 - accuracy: 0.8233\n",
            "Epoch 14/20\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.3691 - accuracy: 0.8450\n",
            "Epoch 15/20\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.2910 - accuracy: 0.8742\n",
            "Epoch 16/20\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.2181 - accuracy: 0.9167\n",
            "Epoch 17/20\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.1848 - accuracy: 0.9275\n",
            "Epoch 18/20\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.1469 - accuracy: 0.9496\n",
            "Epoch 19/20\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.1033 - accuracy: 0.9667\n",
            "Epoch 20/20\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 0.0963 - accuracy: 0.9675\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f37002cad30>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model_lstm.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "model_lstm.fit(X_train, y_train, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a8060b53-2c09-4e0d-a201-b22ffff3ca61",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8060b53-2c09-4e0d-a201-b22ffff3ca61",
        "outputId": "aba1eeab-419e-48ae-df04-7715749dbe40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((600, 15, 300), (600, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "ohe = preprocessing.OneHotEncoder()\n",
        "\n",
        "le.fit(data2.cat)\n",
        "y_test = le.transform(test.cat).reshape(-1, 1)\n",
        "ohe.fit(y_test)\n",
        "y_test = ohe.transform(y_test).todense()\n",
        "\n",
        "X_test = np.array([x for x in test.value])\n",
        "\n",
        "X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4f229a42-0179-479b-afd1-d0f07718eb15",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f229a42-0179-479b-afd1-d0f07718eb15",
        "outputId": "60adf6ea-7246-4cf8-e0da-92759555c73d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/19 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.48833333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(np.argmax(model_lstm.predict(X_test), axis=1), np.argmax(y_test, axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0684f0ce-434c-42bf-b956-95e385e56f95",
      "metadata": {
        "id": "0684f0ce-434c-42bf-b956-95e385e56f95"
      },
      "source": [
        "The accuracy was 50.7%. This needs fine-tuning. Possible suggestions for fine-tuning are grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "daec05e5-f8d7-4983-b186-ba4f1b6dff06",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daec05e5-f8d7-4983-b186-ba4f1b6dff06",
        "outputId": "b80c22b5-b9f7-4768-d65e-79142700be7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/19 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 1, 1, 2, 0, 2, 0, 2, 1, 0, 1, 0, 1, 0, 2, 2, 1, 2, 0, 2, 0,\n",
              "       0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0, 1, 1, 2, 2, 1, 1, 1, 1,\n",
              "       0, 2, 2, 0, 0, 2, 2, 2, 0, 2, 0, 0, 2, 0, 1, 2, 1, 0, 1, 0, 0, 0,\n",
              "       2, 1, 0, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 0, 0, 1, 0, 1, 2, 2, 2,\n",
              "       0, 2, 2, 0, 1, 1, 0, 0, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 0, 0, 1, 2,\n",
              "       1, 1, 1, 1, 0, 0, 2, 2, 1, 0, 0, 2, 1, 0, 1, 2, 2, 1, 1, 1, 2, 1,\n",
              "       1, 1, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 0, 2, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "       1, 2, 0, 1, 0, 0, 0, 0, 1, 2, 0, 1, 0, 1, 2, 1, 1, 1, 1, 0, 1, 1,\n",
              "       0, 1, 1, 0, 2, 1, 0, 1, 2, 1, 0, 0, 1, 0, 2, 1, 1, 2, 1, 1, 0, 0,\n",
              "       0, 2, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 1, 0, 1,\n",
              "       0, 0, 2, 0, 0, 0, 1, 2, 2, 1, 2, 2, 2, 1, 0, 2, 2, 0, 1, 0, 0, 0,\n",
              "       0, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 0, 0, 1, 2, 2, 1, 1, 0, 2, 0, 0,\n",
              "       0, 0, 1, 2, 2, 0, 1, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 2, 1, 0, 0,\n",
              "       0, 0, 2, 0, 1, 0, 0, 0, 1, 2, 1, 0, 0, 2, 0, 0, 0, 1, 2, 1, 1, 1,\n",
              "       1, 1, 2, 2, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 1, 2, 1, 0, 2, 2, 1, 1,\n",
              "       2, 1, 1, 2, 0, 0, 1, 0, 0, 0, 1, 2, 0, 2, 1, 1, 0, 1, 0, 1, 1, 2,\n",
              "       0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n",
              "       2, 2, 1, 2, 2, 1, 1, 1, 0, 1, 0, 0, 2, 2, 1, 0, 0, 0, 0, 2, 0, 1,\n",
              "       1, 2, 1, 1, 0, 1, 1, 2, 2, 1, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       2, 0, 1, 0, 0, 0, 1, 2, 0, 1, 0, 1, 1, 2, 1, 0, 2, 0, 1, 1, 1, 2,\n",
              "       1, 2, 1, 2, 0, 0, 1, 1, 1, 1, 0, 2, 0, 1, 0, 0, 1, 2, 2, 1, 1, 1,\n",
              "       1, 0, 1, 1, 1, 1, 0, 2, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 2, 2, 2, 2,\n",
              "       2, 0, 2, 2, 2, 1, 1, 2, 0, 2, 2, 0, 0, 1, 1, 2, 0, 1, 1, 1, 1, 0,\n",
              "       0, 1, 1, 0, 1, 0, 1, 2, 0, 0, 0, 2, 1, 1, 1, 1, 2, 0, 2, 0, 1, 1,\n",
              "       1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 0, 0, 1, 2, 1, 2, 1,\n",
              "       2, 2, 0, 0, 2, 2, 1, 2, 0, 0, 1, 2, 2, 2, 0, 2, 0, 0, 2, 0, 1, 1,\n",
              "       1, 0, 1, 1, 2, 0, 1, 1, 2, 0, 0, 1, 0, 1, 2, 1, 2, 0, 1, 2, 2, 1,\n",
              "       1, 2, 0, 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "np.argmax(model_lstm.predict(X_test), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "eec2547b-32ce-4d9a-871d-80c38b53aa8f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eec2547b-32ce-4d9a-871d-80c38b53aa8f",
        "outputId": "a90f8cf2-4a91-4213-f21e-2fb2d363c584"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[1, 1, 2, 1, 2, 2, 2, 0, 1, 0, 1, 2, 0, 0, 1, 2, 1, 0, 1, 2, 2,\n",
              "         0, 1, 1, 0, 1, 1, 2, 0, 1, 0, 2, 1, 1, 2, 0, 1, 0, 2, 2, 0, 2,\n",
              "         0, 1, 0, 1, 2, 2, 1, 2, 2, 0, 0, 2, 2, 0, 2, 0, 1, 2, 2, 0, 2,\n",
              "         0, 0, 0, 2, 0, 0, 2, 2, 0, 2, 2, 1, 2, 0, 2, 2, 1, 0, 0, 2, 1,\n",
              "         0, 2, 2, 2, 0, 2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 0, 2, 1, 1, 1, 1,\n",
              "         1, 1, 2, 0, 2, 0, 1, 0, 0, 1, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "         2, 2, 0, 2, 0, 0, 0, 2, 1, 0, 2, 0, 0, 2, 1, 1, 1, 0, 2, 2, 2,\n",
              "         1, 1, 1, 2, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 2, 0,\n",
              "         2, 1, 1, 0, 1, 1, 0, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 2, 0, 1, 0,\n",
              "         2, 0, 0, 2, 2, 1, 0, 0, 1, 0, 2, 1, 0, 2, 1, 1, 0, 1, 0, 2, 2,\n",
              "         0, 2, 0, 1, 0, 2, 1, 1, 1, 0, 1, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2,\n",
              "         1, 0, 2, 1, 2, 2, 2, 0, 1, 2, 0, 0, 1, 1, 2, 1, 0, 0, 2, 2, 2,\n",
              "         2, 0, 1, 0, 2, 2, 1, 1, 0, 2, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2,\n",
              "         1, 0, 1, 2, 2, 1, 0, 2, 1, 1, 2, 2, 1, 0, 0, 2, 1, 0, 0, 0, 1,\n",
              "         0, 0, 2, 0, 0, 2, 1, 0, 0, 2, 2, 2, 0, 1, 1, 0, 2, 2, 0, 1, 1,\n",
              "         0, 1, 0, 1, 0, 0, 2, 1, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 1, 1,\n",
              "         1, 0, 0, 2, 1, 2, 0, 2, 2, 2, 0, 1, 2, 1, 1, 2, 0, 0, 1, 2, 1,\n",
              "         0, 0, 1, 0, 0, 2, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 0, 1, 2,\n",
              "         2, 0, 0, 2, 2, 1, 0, 1, 2, 2, 2, 1, 0, 0, 0, 2, 1, 1, 1, 0, 1,\n",
              "         2, 1, 1, 2, 2, 2, 2, 1, 1, 0, 2, 2, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "         0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 2, 0, 2, 0, 0, 2, 2,\n",
              "         0, 2, 2, 0, 0, 1, 0, 2, 1, 0, 0, 1, 2, 2, 0, 2, 2, 2, 2, 1, 1,\n",
              "         2, 0, 0, 1, 1, 1, 1, 2, 1, 1, 2, 2, 2, 0, 1, 2, 0, 0, 1, 2, 2,\n",
              "         2, 0, 1, 2, 2, 0, 1, 2, 2, 0, 2, 2, 1, 0, 1, 0, 2, 0, 1, 2, 0,\n",
              "         0, 1, 0, 1, 1, 0, 1, 0, 0, 2, 1, 1, 0, 2, 1, 2, 0, 2, 2, 0, 2,\n",
              "         1, 0, 2, 2, 0, 1, 0, 2, 2, 1, 0, 1, 0, 2, 0, 0, 2, 2, 0, 0, 1,\n",
              "         2, 2, 2, 2, 2, 0, 0, 2, 1, 2, 1, 2, 1, 0, 0, 1, 2, 0, 0, 0, 0,\n",
              "         1, 2, 1, 1, 2, 1, 2, 0, 1, 2, 0, 0, 2, 1, 0, 1, 2, 0, 2, 2, 2,\n",
              "         1, 0, 0, 2, 2, 0, 1, 1, 1, 0, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "np.argmax(y_test, axis=1).reshape(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e587ef6d-a7fa-4464-b8fb-5af9921fbb8c",
      "metadata": {
        "id": "e587ef6d-a7fa-4464-b8fb-5af9921fbb8c"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "188b9a25-9234-4569-a440-03b0b59439b8",
      "metadata": {
        "id": "188b9a25-9234-4569-a440-03b0b59439b8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "# from keras.layers.core import Dense, Activation\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.layers import Dense, LSTM, Dropout\n",
        "# from keras.optimizers import Adam, Adagrad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "09d1519d-ab7c-48f1-b0d1-9b58d6628412",
      "metadata": {
        "id": "09d1519d-ab7c-48f1-b0d1-9b58d6628412"
      },
      "outputs": [],
      "source": [
        "window_size = 15\n",
        "n_sample = 1000 # less than or equal to (len(text_vectorized2[\"初級\"]) - window_size)\n",
        "\n",
        "n_rnn = window_size  # 時系列の数\n",
        "batch_size = 128\n",
        "epochs = 20  #epochsは、多いほど、精密に学習するが、重くなるため今回は小さくしている\n",
        "n_mid = 256  # 中間層のニューロン数\n",
        "data_dim = 300\n",
        "\n",
        "input_dim = (n_rnn, data_dim)\n",
        "output_dim = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "eb800270-72b4-45d2-9528-96c6bf6c8d4a",
      "metadata": {
        "id": "eb800270-72b4-45d2-9528-96c6bf6c8d4a"
      },
      "outputs": [],
      "source": [
        "def language_model(activation=\"relu\", optimizer=\"adam\", hidden_layer_sizes=(100, 100)):\n",
        "    model = Sequential()\n",
        "    firstflag = True\n",
        "    for dim in hidden_layer_sizes:\n",
        "        if firstflag:\n",
        "            model.add(LSTM(n_mid, input_shape=(n_rnn, data_dim), return_sequences=True, activation=activation))\n",
        "            model.add(Dropout(0.2))\n",
        "            firstflag = False\n",
        "        else:\n",
        "            model.add(LSTM(n_mid, return_sequences=True, activation=activation))\n",
        "            model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(LSTM(n_mid, activation=activation))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(output_dim, activation=\"softmax\"))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d75a5336-7e0b-4aac-8835-c2fb553779d5",
      "metadata": {
        "id": "d75a5336-7e0b-4aac-8835-c2fb553779d5"
      },
      "outputs": [],
      "source": [
        "# activation = [\"relu\", \"tanh\"]\n",
        "optimizer = [\"adam\", \"adagrad\", \"sgd\", \"RMSprop\", \"Adamax\"]\n",
        "hidden_layer_sizes = [(50, 50, 50, 50), (50, 50, 50), (50, 50), (50, ) ]\n",
        "nb_epoch = [20, 25]\n",
        "batch_size = [128, 256]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9b9751e1-4c58-4acd-9068-f7ca0ab799a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b9751e1-4c58-4acd-9068-f7ca0ab799a6",
        "outputId": "597b80c6-ef70-451a-c7e6-8672d5e7b024"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-c64ef7c3159e>:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model2 = KerasClassifier(build_fn=language_model, verbose=0)\n"
          ]
        }
      ],
      "source": [
        "model2 = KerasClassifier(build_fn=language_model, verbose=0)\n",
        "param_grid = dict(activation=activation, \n",
        "                  optimizer=optimizer, \n",
        "                  hidden_layer_sizes=hidden_layer_sizes, \n",
        "                  nb_epoch=nb_epoch, \n",
        "                  batch_size=batch_size,)\n",
        "grid = GridSearchCV(estimator=model2, param_grid=param_grid, verbose=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d7763d3e-26f0-4cf9-8298-bf378664feb3",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7763d3e-26f0-4cf9-8298-bf378664feb3",
        "outputId": "77e7cd85-d5d9-4007-f9fe-77a492b6d3de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_161 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_162 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_163 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_164 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_165 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_166 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_167 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_168 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_169 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_170 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_171 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_172 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_173 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_174 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_175 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_176 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_177 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_178 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_179 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_180 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_181 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_182 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_183 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   5.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_184 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_185 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_186 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_187 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_188 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   5.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_189 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_190 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_191 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_192 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_193 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   5.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_194 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_195 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_196 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_197 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_198 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   5.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_199 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_200 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_201 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_202 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_203 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   5.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_204 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_205 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_206 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_207 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_208 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_209 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_210 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_211 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_212 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_213 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_214 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_215 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_216 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_217 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_218 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_219 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_220 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_221 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_222 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_223 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_224 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_225 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_226 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_227 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_228 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_229 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_230 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_231 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_232 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_233 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_234 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_235 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_236 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_237 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_238 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_239 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_240 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_241 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_242 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_243 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_244 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_245 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_246 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_247 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_248 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_249 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_250 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_251 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_252 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_253 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_254 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_255 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_256 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_257 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   4.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_258 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_259 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_260 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_261 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   4.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_262 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_263 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_264 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_265 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   4.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_266 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_267 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_268 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_269 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_270 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_271 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_272 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_273 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   4.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_274 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_275 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_276 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_277 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_278 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_279 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_280 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_281 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_282 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_283 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_284 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_285 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_286 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_287 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_288 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_289 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_290 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_291 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_292 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_293 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   4.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_294 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_295 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_296 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_297 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_298 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_299 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_300 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_301 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_302 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_303 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_304 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_305 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_306 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_307 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_308 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_309 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_310 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_311 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_312 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_313 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_314 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_315 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_316 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_317 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_318 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_319 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_320 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_321 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   4.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_322 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_323 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_324 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   4.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_325 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_326 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_327 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_328 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_329 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_330 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_331 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_332 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_333 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_334 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_335 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_336 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_337 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_338 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_339 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_340 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_341 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_342 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_343 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_344 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_345 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_346 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_347 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_348 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_349 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   4.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_350 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_351 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_352 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_353 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_354 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_355 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_356 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_357 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   4.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_358 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_359 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_360 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_361 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_362 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_363 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_364 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_365 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   4.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_366 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_367 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_368 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_369 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   4.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_370 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_371 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_372 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_373 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_374 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_375 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_376 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_377 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_378 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_379 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_380 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_381 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_382 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_383 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_384 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_385 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_386 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_387 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_388 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_389 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_390 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_391 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_392 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_393 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_394 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_395 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_396 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_397 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_398 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_399 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_400 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_401 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   4.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_402 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_403 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_404 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_405 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_406 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_407 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_408 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_409 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   4.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_410 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_411 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_412 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_413 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_414 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_415 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_416 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_417 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   4.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_418 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_419 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_420 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_421 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_422 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_423 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_424 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_425 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   4.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_426 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_427 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_428 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_429 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_430 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_431 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_432 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_433 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_434 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_435 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_436 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_437 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   4.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_438 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_439 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_440 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_441 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   4.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_442 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_443 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_444 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_445 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_446 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_447 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_448 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_449 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   4.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_450 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_451 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_452 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_453 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_454 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_455 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_456 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   3.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_457 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_458 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_459 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_460 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_461 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_462 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_463 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_464 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_465 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   3.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_466 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_467 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_468 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_469 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_470 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_471 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_472 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_473 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_474 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   3.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_475 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_476 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_477 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_478 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_479 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_480 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_481 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_482 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_483 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   3.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_484 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_485 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_486 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_487 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_488 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_489 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   3.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_490 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_491 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_492 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   3.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_493 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_494 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_495 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_496 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_497 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_498 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_499 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_500 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_501 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   4.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_502 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_503 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_504 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   3.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_505 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_506 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_507 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   4.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_508 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_509 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_510 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   3.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_511 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_512 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_513 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   3.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_514 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_515 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_516 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   3.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_517 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_518 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_519 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_520 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_521 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_522 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_523 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_524 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_525 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   3.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_526 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_527 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_528 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_529 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_530 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_531 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_532 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_533 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_534 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   3.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_535 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_536 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_537 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_538 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_539 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_540 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_541 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_542 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_543 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   3.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_544 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_545 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_546 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_547 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_548 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_549 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_550 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_551 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_552 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_553 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_554 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_555 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   3.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_556 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_557 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_558 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_559 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_560 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_561 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_562 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_563 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_564 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   3.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_565 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_566 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_567 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_568 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_569 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_570 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_571 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_572 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_573 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   3.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_574 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_575 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_576 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   3.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_577 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_578 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_579 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   3.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_580 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_581 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_582 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   3.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_583 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_584 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_585 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   3.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_586 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_587 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   3.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_588 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_589 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_590 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_591 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_592 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_593 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_594 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   3.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_595 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_596 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_597 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   3.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_598 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_599 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_600 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_601 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_602 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_603 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_604 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_605 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_606 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_607 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_608 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_609 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_610 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_611 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_612 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_613 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_614 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   2.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_615 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_616 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_617 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_618 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_619 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_620 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_621 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   2.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_622 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_623 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_624 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   2.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_625 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_626 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_627 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_628 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   2.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_629 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_630 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   2.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_631 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_632 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   2.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_633 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_634 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_635 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_636 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_637 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_638 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_639 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_640 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_641 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_642 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_643 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_644 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_645 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_646 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   2.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_647 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_648 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_649 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_650 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_651 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_652 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_653 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_654 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_655 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_656 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   2.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_657 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_658 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_659 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_660 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_661 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_662 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_663 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_664 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   2.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_665 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_666 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_667 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_668 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   2.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_669 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_670 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   2.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_671 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_672 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   2.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_673 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_674 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_675 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_676 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   2.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_677 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_678 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   2.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_679 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_680 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   2.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_681 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_682 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_683 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_684 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_685 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_686 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_687 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_688 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_689 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_690 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_691 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_692 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_693 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_694 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_695 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_696 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   2.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_697 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_698 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_699 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_700 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_701 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_702 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_703 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_704 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_705 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_706 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_707 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_708 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_709 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_710 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_711 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_712 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_713 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_714 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_715 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f36ab695820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:Layer lstm_716 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_717 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_718 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_719 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_720 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f36abcde040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:Layer lstm_721 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_722 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_723 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_724 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_725 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_726 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_727 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_728 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_729 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_730 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_731 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_732 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_733 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   6.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_734 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_735 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_736 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_737 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_738 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_739 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_740 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_741 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_742 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_743 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_744 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_745 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_746 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_747 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_748 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_749 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_750 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_751 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_752 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_753 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_754 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_755 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_756 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_757 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_758 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_759 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_760 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_761 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_762 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_763 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_764 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_765 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_766 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_767 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_768 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   6.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_769 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_770 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_771 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_772 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_773 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_774 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_775 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_776 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_777 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_778 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   6.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_779 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_780 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_781 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_782 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_783 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_784 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_785 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_786 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_787 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_788 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_789 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_790 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_791 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_792 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_793 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_794 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_795 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_796 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_797 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_798 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_799 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_800 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_801 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_802 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_803 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_804 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_805 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_806 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_807 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_808 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_809 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_810 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_811 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_812 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_813 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_814 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_815 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_816 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_817 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_818 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_819 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_820 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_821 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_822 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_823 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_824 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_825 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_826 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_827 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_828 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_829 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_830 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_831 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_832 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   6.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_833 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_834 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_835 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_836 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_837 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_838 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_839 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_840 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_841 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_842 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_843 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_844 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_845 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_846 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_847 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_848 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_849 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_850 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_851 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_852 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_853 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_854 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_855 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_856 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_857 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_858 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_859 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_860 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_861 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_862 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_863 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_864 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_865 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_866 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_867 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_868 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_869 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_870 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_871 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_872 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_873 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_874 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_875 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_876 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_877 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_878 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_879 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_880 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_881 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_882 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_883 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_884 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_885 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_886 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_887 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_888 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_889 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_890 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_891 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_892 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_893 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_894 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_895 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_896 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_897 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_898 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_899 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_900 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_901 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_902 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_903 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   6.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_904 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_905 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_906 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_907 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_908 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_909 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_910 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_911 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_912 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_913 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_914 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_915 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_916 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_917 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_918 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_919 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_920 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_921 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_922 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_923 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_924 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_925 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_926 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_927 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_928 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_929 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_930 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_931 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_932 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_933 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_934 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_935 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_936 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_937 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_938 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_939 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_940 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_941 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_942 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_943 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_944 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_945 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_946 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_947 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_948 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_949 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_950 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_951 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_952 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_953 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_954 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_955 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_956 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_957 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   5.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_958 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_959 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_960 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_961 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   5.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_962 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_963 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_964 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_965 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   5.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_966 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_967 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_968 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_969 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   6.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_970 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_971 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_972 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_973 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   5.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_974 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_975 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_976 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_977 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_978 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_979 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_980 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_981 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_982 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_983 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_984 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_985 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_986 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_987 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_988 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_989 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   6.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_990 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_991 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_992 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_993 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_994 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_995 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_996 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_997 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   5.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_998 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_999 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1000 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   5.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1001 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1002 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1003 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1004 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1005 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   6.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1006 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1007 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1008 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1009 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   5.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1010 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1011 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1012 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1013 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   5.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1014 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1015 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1016 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1017 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1018 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1019 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1020 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1021 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1022 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1023 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1024 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1025 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1026 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1027 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1028 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1029 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1030 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1031 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1032 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1033 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1034 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1035 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1036 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1037 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1038 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1039 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1040 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1041 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1042 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1043 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1044 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1045 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1046 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1047 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1048 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1049 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1050 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1051 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1052 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1053 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1054 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1055 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1056 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1057 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   5.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1058 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1059 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1060 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1061 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   5.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1062 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1063 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1064 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1065 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   6.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1066 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1067 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1068 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1069 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   5.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1070 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1071 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1072 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1073 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   5.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1074 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1075 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1076 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1077 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1078 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1079 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1080 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1081 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1082 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1083 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1084 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1085 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1086 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1087 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1088 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1089 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1090 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1091 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1092 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1093 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1094 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1095 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   5.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1096 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1097 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1098 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1099 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1100 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1101 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   6.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1102 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1103 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1104 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1105 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   5.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1106 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1107 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1108 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1109 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   5.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1110 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1111 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   5.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1112 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1113 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1114 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1115 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1116 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1117 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1118 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1119 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1120 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1121 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1122 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1123 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1124 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1125 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1126 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1127 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1128 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1129 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1130 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1131 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1132 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1133 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1134 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1135 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1136 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1137 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1138 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1139 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1140 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1141 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1142 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1143 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1144 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1145 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1146 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1147 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1148 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1149 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1150 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1151 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1152 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1153 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1154 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1155 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1156 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1157 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1158 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1159 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1160 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1161 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1162 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1163 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1164 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1165 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   5.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1166 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1167 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1168 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   5.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1169 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1170 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1171 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   4.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1172 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1173 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1174 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1175 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1176 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1177 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1178 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1179 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1180 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1181 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1182 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1183 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   4.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1184 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1185 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1186 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   4.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1187 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1188 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1189 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1190 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1191 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1192 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1193 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1194 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1195 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   4.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1196 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1197 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1198 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   5.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1199 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1200 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1201 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1202 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1203 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1204 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   4.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1205 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1206 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1207 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   4.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1208 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1209 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1210 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1211 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1212 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1213 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   4.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1214 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1215 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1216 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   4.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1217 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1218 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1219 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1220 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1221 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1222 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   4.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1223 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1224 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1225 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1226 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1227 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1228 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1229 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1230 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1231 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1232 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1233 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1234 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1235 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1236 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1237 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   4.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1238 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1239 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1240 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1241 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1242 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1243 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   4.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1244 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1245 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1246 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   4.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1247 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1248 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1249 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1250 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1251 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1252 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1253 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1254 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1255 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   4.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1256 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1257 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1258 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1259 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1260 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1261 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   4.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1262 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1263 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1264 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   4.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1265 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1266 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1267 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1268 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1269 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1270 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   4.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1271 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1272 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1273 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   4.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1274 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1275 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1276 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   4.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1277 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1278 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   4.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1279 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1280 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1281 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1282 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   4.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1283 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1284 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1285 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   4.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1286 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1287 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1288 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1289 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1290 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1291 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1292 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1293 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1294 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   4.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1295 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1296 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1297 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1298 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1299 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1300 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   4.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1301 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1302 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1303 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1304 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1305 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1306 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1307 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1308 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   3.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1309 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1310 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1311 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1312 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1313 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1314 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1315 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1316 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1317 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1318 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1319 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1320 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1321 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1322 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1323 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1324 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1325 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1326 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1327 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1328 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1329 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1330 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1331 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1332 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1333 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1334 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1335 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1336 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1337 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1338 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1339 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1340 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1341 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1342 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1343 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1344 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1345 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1346 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1347 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1348 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1349 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1350 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1351 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1352 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1353 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1354 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1355 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1356 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1357 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1358 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1359 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1360 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1361 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1362 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1363 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1364 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1365 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1366 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1367 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1368 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1369 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1370 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1371 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1372 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1373 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1374 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1375 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1376 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1377 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1378 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1379 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1380 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1381 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1382 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1383 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1384 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1385 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1386 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1387 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1388 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1389 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1390 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1391 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1392 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1393 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1394 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1395 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1396 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1397 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1398 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1399 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1400 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.1s\n",
            "[CV] END activation=relu, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   9.2s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=  10.1s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.6s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   9.0s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   9.0s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.9s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.2s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.9s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.1s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.0s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.8s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.1s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.1s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.1s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.1s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.0s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.9s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.9s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.9s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.9s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.2s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.9s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   9.1s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   9.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   9.5s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   9.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   9.0s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   9.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   9.0s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.0s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.8s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.5s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   6.9s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.5s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.1s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   6.9s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.1s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.9s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.6s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.5s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   6.9s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.5s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   6.9s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.2s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   6.9s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.2s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   6.8s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.8s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.6s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.6s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.0s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.6s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   5.5s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   5.8s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   5.8s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   5.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.0s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.6s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   6.0s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.0s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.1s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   5.9s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   5.5s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.0s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.0s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.2s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.9s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.0s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.6s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.0s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   5.6s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   5.5s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.0s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   5.5s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.6s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.5s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.5s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.8s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.2s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.2s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.1s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.5s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.9s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.0s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.8s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.2s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.1s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.6s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.2s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   3.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.1s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   3.8s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.2s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   3.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.1s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   3.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.0s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   3.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.1s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.6s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.0s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.2s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.9s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.8s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.5s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.8s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.1s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   3.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.1s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   3.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   3.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.8s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.1s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.1s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   3.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.1s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   3.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   3.8s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.2s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   3.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.0s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   3.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.0s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   3.7s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.9s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.9s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.9s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.2s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.8s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.4s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.3s\n",
            "[CV] END activation=tanh, batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.8s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   9.9s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   9.7s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   9.8s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   9.8s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   9.5s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=  10.3s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   9.6s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   9.4s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=  10.2s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   9.8s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.6s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.7s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.8s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.7s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.8s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=  10.2s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=  10.3s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=  10.2s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=  10.2s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=  11.6s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=  10.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.9s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.9s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.9s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=  10.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.9s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.9s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.8s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=  10.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=  10.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   9.5s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=  10.2s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   9.7s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   9.8s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   9.8s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   9.7s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   9.8s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   9.3s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=  10.2s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   9.7s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=  11.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=  10.2s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=  10.3s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   9.7s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=  10.6s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=  10.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=  10.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   9.9s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=  10.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=  10.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.7s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.9s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.9s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.9s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.8s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.6s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.9s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.4s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.2s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.4s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.4s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.3s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.5s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.8s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.9s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.6s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.9s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.8s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.9s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.5s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.9s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.4s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.2s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.3s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.4s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.4s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.3s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.8s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.5s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.3s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.5s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.4s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.6s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   6.3s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   6.5s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.6s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   6.2s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.3s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   5.7s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.3s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.2s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.6s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.7s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.2s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.5s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.8s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.2s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.8s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.2s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.3s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.4s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.7s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.5s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.3s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.7s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.2s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   6.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.8s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   6.5s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   6.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.7s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.4s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.7s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.3s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.8s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.8s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.3s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.6s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.4s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   5.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.5s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.4s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.5s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.4s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.4s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.9s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.8s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.2s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.6s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.4s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.3s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.6s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.4s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.5s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.4s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.5s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.5s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.3s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.4s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.4s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.4s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.2s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.6s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.1s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.6s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.3s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.4s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.5s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.0s\n",
            "[CV] END activation=tanh, batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.7s\n"
          ]
        }
      ],
      "source": [
        "grid_result = grid.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "d46d01e3-41a5-4369-84e2-6dc3f5b1976e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d46d01e3-41a5-4369-84e2-6dc3f5b1976e",
        "outputId": "5fba39c9-1745-4a22-db57-c72fc8f7ae5e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([4.94819379, 4.92124991, 4.98324423, 5.22386875, 4.77248521,\n",
              "        4.75072613, 4.67256632, 4.61011233, 5.24086461, 5.00736651,\n",
              "        3.94882131, 3.89164729, 3.84395447, 4.23850641, 3.9753469 ,\n",
              "        3.9534544 , 3.8687252 , 3.82825971, 4.40876946, 3.92900877,\n",
              "        3.07817745, 2.99073162, 2.89260201, 3.29045901, 2.98132415,\n",
              "        2.92830539, 2.96136379, 2.99288259, 3.13214078, 3.16562767,\n",
              "        2.18534522, 2.10531325, 2.11448932, 2.29335666, 2.02930427,\n",
              "        2.20381064, 2.12618113, 2.05535188, 2.27125916, 2.11817064,\n",
              "        6.9550415 , 6.73892193, 6.47470922, 7.32712417, 7.14411292,\n",
              "        7.08559074, 6.32410965, 6.71722054, 6.97709999, 7.10189133,\n",
              "        5.12124715, 5.1014461 , 5.15415907, 5.84869766, 5.11562009,\n",
              "        5.02660708, 5.0588623 , 5.14128861, 5.63733301, 5.30480008,\n",
              "        4.36234236, 4.33965964, 4.05626006, 4.34725108, 3.93282399,\n",
              "        4.23629298, 4.13673615, 4.04082627, 4.35143161, 4.23468914,\n",
              "        2.75466423, 2.89158816, 2.6426569 , 2.93391261, 2.7086132 ,\n",
              "        2.82291341, 2.66874838, 2.67332182, 2.89869084, 2.85379529,\n",
              "        7.22828813, 7.09227085, 7.05980096, 7.49937401, 7.09053464,\n",
              "        7.37044687, 7.11842585, 7.05161858, 7.50609341, 7.23551011,\n",
              "        6.07059922, 5.86767306, 5.84408975, 6.19585681, 5.97620101,\n",
              "        5.90424638, 6.14238043, 5.82799978, 6.36126518, 5.91726222,\n",
              "        4.5785181 , 4.58062534, 4.74396853, 4.8968564 , 4.63207192,\n",
              "        4.63105221, 4.5046474 , 4.43844409, 4.78367009, 4.85311046,\n",
              "        3.22178092, 3.01541905, 3.08952818, 3.23945956, 3.05375648,\n",
              "        3.24802771, 3.07297626, 3.01897244, 3.31590576, 3.19481025,\n",
              "        7.82100534, 8.11679621, 7.65705099, 8.50513535, 7.94210243,\n",
              "        7.81555719, 7.86963878, 7.79764967, 8.50899982, 8.35786729,\n",
              "        6.64758039, 6.66963177, 6.8316515 , 6.96389461, 6.60628405,\n",
              "        6.6362977 , 6.59319844, 6.6221149 , 7.02753863, 6.58788295,\n",
              "        5.22958837, 5.02030611, 5.01176763, 5.57041473, 4.94591007,\n",
              "        5.02972898, 5.00276999, 4.83394642, 5.56133094, 5.02116661,\n",
              "        3.74298606, 3.46606617, 3.34673467, 3.66197171, 3.48966603,\n",
              "        3.56590772, 3.35862861, 3.52146378, 3.64991345, 3.54357648]),\n",
              " 'std_fit_time': array([0.17228004, 0.40925488, 0.403983  , 0.19810373, 0.19524084,\n",
              "        0.18971121, 0.19497464, 0.21624066, 0.23382504, 0.28574477,\n",
              "        0.20348934, 0.16847113, 0.1912235 , 0.22030041, 0.18085687,\n",
              "        0.21113148, 0.21087336, 0.16826628, 0.53898239, 0.1981874 ,\n",
              "        0.20387628, 0.19323189, 0.16919152, 0.17655491, 0.18929969,\n",
              "        0.02270238, 0.20369636, 0.19788467, 0.0400491 , 0.42060355,\n",
              "        0.24156604, 0.14741788, 0.18834888, 0.16276355, 0.03039905,\n",
              "        0.17583578, 0.1713409 , 0.15138565, 0.15348717, 0.14925017,\n",
              "        0.49336773, 0.71196999, 0.49426476, 0.62239554, 0.66988387,\n",
              "        0.71179007, 0.1978513 , 0.60687352, 0.37253817, 0.45215352,\n",
              "        0.16153674, 0.16252047, 0.26327026, 0.82493761, 0.2494992 ,\n",
              "        0.1594498 , 0.29361948, 0.29078787, 0.25326642, 0.2133087 ,\n",
              "        0.5805235 , 0.45771727, 0.38466279, 0.38511895, 0.22618052,\n",
              "        0.04175178, 0.39265073, 0.16234739, 0.17144703, 0.06534441,\n",
              "        0.21997481, 0.4345463 , 0.17523482, 0.20413321, 0.12065273,\n",
              "        0.20896741, 0.1445389 , 0.19119516, 0.20907444, 0.1503363 ,\n",
              "        0.65696697, 0.22284846, 0.22821847, 0.21419341, 0.25034965,\n",
              "        0.18374872, 0.19890089, 0.21942948, 0.2533987 , 0.43888863,\n",
              "        0.27144221, 0.27939967, 0.18911229, 0.28623955, 0.16522271,\n",
              "        0.24732734, 0.50382947, 0.21595482, 0.03770367, 0.22101654,\n",
              "        0.25234517, 0.20224394, 0.28147497, 0.24813567, 0.24370907,\n",
              "        0.28896348, 0.1735082 , 0.2128184 , 0.23095953, 0.265424  ,\n",
              "        0.19588815, 0.14583962, 0.19417157, 0.2408365 , 0.16412144,\n",
              "        0.21147286, 0.17626156, 0.14074509, 0.22492262, 0.23069631,\n",
              "        0.17235981, 0.39436309, 0.0663165 , 0.72224454, 0.20184894,\n",
              "        0.08516845, 0.28443236, 0.27877225, 0.4134897 , 0.03191641,\n",
              "        0.16123705, 0.08486345, 0.46415802, 0.1899065 , 0.20905014,\n",
              "        0.15851668, 0.16953313, 0.18310157, 0.07841463, 0.37419635,\n",
              "        0.1915791 , 0.33541106, 0.20783193, 0.34592037, 0.21211502,\n",
              "        0.15986904, 0.31652171, 0.32809035, 0.18485024, 0.20031093,\n",
              "        0.35012173, 0.20512334, 0.16357563, 0.23944995, 0.22795382,\n",
              "        0.22841594, 0.17653614, 0.19319034, 0.19765582, 0.22596752]),\n",
              " 'mean_score_time': array([0.73408332, 0.76565909, 0.78421106, 0.76572223, 0.74590616,\n",
              "        0.82091031, 0.75102806, 0.81743226, 0.69612079, 0.75153837,\n",
              "        0.58958368, 0.58286309, 0.58403425, 0.58764114, 0.58856483,\n",
              "        0.58476872, 0.59264054, 0.5908299 , 0.58694463, 0.59241867,\n",
              "        0.48062   , 0.48390017, 0.47416887, 0.47602906, 0.55172219,\n",
              "        0.61415377, 0.48829021, 0.47880449, 0.53162446, 0.57056623,\n",
              "        0.37863374, 0.37143226, 0.37479067, 0.38549137, 0.43829079,\n",
              "        0.37331271, 0.36968474, 0.37333941, 0.37718749, 0.38267226,\n",
              "        0.69584551, 0.73263712, 0.75382724, 0.70732012, 0.67910123,\n",
              "        0.67483411, 0.69526529, 0.71201892, 0.75563183, 0.69052749,\n",
              "        0.6503068 , 0.70780058, 0.62835541, 0.73606591, 0.64997358,\n",
              "        0.74080205, 0.63649974, 0.63558865, 0.57935901, 0.56924996,\n",
              "        0.48154764, 0.48130593, 0.4738749 , 0.47941332, 0.54929528,\n",
              "        0.47576327, 0.47345028, 0.48444514, 0.48066802, 0.58611546,\n",
              "        0.36839843, 0.37156458, 0.36682601, 0.38413043, 0.3597939 ,\n",
              "        0.3774684 , 0.37409978, 0.38007121, 0.37051806, 0.36691523,\n",
              "        1.69861245, 1.63571944, 1.62565222, 1.68324294, 1.63418546,\n",
              "        1.64298625, 1.62982283, 1.63876796, 1.68431039, 1.64171662,\n",
              "        1.50962439, 1.39947987, 1.39463334, 1.31602249, 1.41220779,\n",
              "        1.31436334, 1.38400097, 1.30763569, 1.32189956, 1.40801821,\n",
              "        1.08347521, 1.04421883, 1.16972942, 1.10296526, 1.05618858,\n",
              "        1.0910316 , 1.08490505, 1.10583768, 1.0974297 , 1.08639059,\n",
              "        0.76207423, 0.80634985, 0.7351337 , 0.81132503, 0.81375251,\n",
              "        0.74018393, 0.81230092, 0.80977616, 0.75348387, 0.8994503 ,\n",
              "        1.92543716, 1.73819089, 2.07112212, 2.00150299, 1.99404931,\n",
              "        2.10923476, 1.92557034, 1.92524176, 1.88025355, 1.6314404 ,\n",
              "        1.30056291, 1.30942268, 1.29236312, 1.3836081 , 1.30141664,\n",
              "        1.38933234, 1.29646692, 1.47267103, 1.30556207, 1.38591728,\n",
              "        1.01063375, 1.00812712, 1.07423697, 1.08263245, 1.07848368,\n",
              "        1.14815168, 1.0810914 , 1.14299798, 1.02259851, 1.06849928,\n",
              "        0.79483867, 0.7364603 , 0.80230951, 0.77763314, 0.72741966,\n",
              "        0.72836714, 0.79679723, 0.73535662, 0.7215344 , 0.77612038]),\n",
              " 'std_score_time': array([0.03184307, 0.13268543, 0.14078678, 0.11243947, 0.10618252,\n",
              "        0.15266714, 0.09457137, 0.1594785 , 0.01851949, 0.12583813,\n",
              "        0.01900107, 0.00926509, 0.0132836 , 0.01415452, 0.009145  ,\n",
              "        0.01158458, 0.01350013, 0.00406997, 0.01030474, 0.01630067,\n",
              "        0.01150435, 0.00837561, 0.01363368, 0.00748613, 0.13217721,\n",
              "        0.17965931, 0.02779825, 0.00848747, 0.11871665, 0.12052568,\n",
              "        0.0133562 , 0.01036272, 0.01537662, 0.01174798, 0.1284906 ,\n",
              "        0.01340016, 0.0110881 , 0.01810278, 0.00969789, 0.00624144,\n",
              "        0.01806148, 0.12278135, 0.1181565 , 0.02630314, 0.01644281,\n",
              "        0.01974054, 0.01661166, 0.04227115, 0.13430084, 0.00556019,\n",
              "        0.16486307, 0.16417643, 0.10353238, 0.19062049, 0.17269664,\n",
              "        0.19187772, 0.10339639, 0.09899747, 0.00587817, 0.00685294,\n",
              "        0.00649587, 0.01118985, 0.01461136, 0.01040242, 0.13857356,\n",
              "        0.01365891, 0.01617411, 0.0195512 , 0.00780688, 0.12493422,\n",
              "        0.0186272 , 0.01743393, 0.02156901, 0.02398893, 0.00805724,\n",
              "        0.01489395, 0.0171031 , 0.01626191, 0.01538508, 0.02323387,\n",
              "        0.12708794, 0.11400674, 0.0908741 , 0.09431268, 0.08291795,\n",
              "        0.13588848, 0.09741517, 0.08714449, 0.09747496, 0.0876956 ,\n",
              "        0.2457015 , 0.1282802 , 0.14644587, 0.01561997, 0.15126066,\n",
              "        0.01853153, 0.13638414, 0.02120462, 0.02491419, 0.14906655,\n",
              "        0.12137789, 0.04253828, 0.22913399, 0.1094201 , 0.02492666,\n",
              "        0.12377036, 0.12503841, 0.11237566, 0.12508459, 0.0318948 ,\n",
              "        0.04038044, 0.13863604, 0.01864786, 0.14465761, 0.14711367,\n",
              "        0.00928167, 0.15258327, 0.14351637, 0.00974728, 0.29592753,\n",
              "        0.18814015, 0.1994006 , 0.03222193, 0.18176962, 0.1846199 ,\n",
              "        0.04238176, 0.20246431, 0.19437254, 0.2425087 , 0.02136771,\n",
              "        0.0234706 , 0.01290291, 0.01617148, 0.14040523, 0.01601339,\n",
              "        0.14793959, 0.01417998, 0.18909975, 0.01530564, 0.15607979,\n",
              "        0.00967208, 0.02069508, 0.13832348, 0.14401866, 0.1323269 ,\n",
              "        0.15540068, 0.12990499, 0.15333725, 0.00925871, 0.13405139,\n",
              "        0.11413403, 0.00634077, 0.15273198, 0.12142294, 0.02111266,\n",
              "        0.01416916, 0.12138321, 0.01076499, 0.00762891, 0.08241422]),\n",
              " 'param_activation': masked_array(data=['relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
              "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
              "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
              "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
              "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
              "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
              "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
              "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
              "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
              "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
              "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
              "                    'relu', 'relu', 'relu', 'tanh', 'tanh', 'tanh', 'tanh',\n",
              "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
              "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
              "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
              "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
              "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
              "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
              "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
              "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
              "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
              "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
              "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_batch_size': masked_array(data=[128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
              "                    128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
              "                    128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
              "                    128, 128, 128, 128, 128, 128, 128, 256, 256, 256, 256,\n",
              "                    256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256,\n",
              "                    256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256,\n",
              "                    256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256,\n",
              "                    256, 256, 256, 128, 128, 128, 128, 128, 128, 128, 128,\n",
              "                    128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
              "                    128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
              "                    128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 256,\n",
              "                    256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256,\n",
              "                    256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256,\n",
              "                    256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256,\n",
              "                    256, 256, 256, 256, 256, 256],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_hidden_layer_sizes': masked_array(data=[(50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
              "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
              "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
              "                    (50, 50, 50, 50), (50, 50, 50), (50, 50, 50),\n",
              "                    (50, 50, 50), (50, 50, 50), (50, 50, 50), (50, 50, 50),\n",
              "                    (50, 50, 50), (50, 50, 50), (50, 50, 50), (50, 50, 50),\n",
              "                    (50, 50), (50, 50), (50, 50), (50, 50), (50, 50),\n",
              "                    (50, 50), (50, 50), (50, 50), (50, 50), (50, 50),\n",
              "                    (50,), (50,), (50,), (50,), (50,), (50,), (50,), (50,),\n",
              "                    (50,), (50,), (50, 50, 50, 50), (50, 50, 50, 50),\n",
              "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
              "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
              "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50),\n",
              "                    (50, 50, 50), (50, 50, 50), (50, 50, 50), (50, 50, 50),\n",
              "                    (50, 50, 50), (50, 50, 50), (50, 50, 50), (50, 50, 50),\n",
              "                    (50, 50, 50), (50, 50), (50, 50), (50, 50), (50, 50),\n",
              "                    (50, 50), (50, 50), (50, 50), (50, 50), (50, 50),\n",
              "                    (50, 50), (50,), (50,), (50,), (50,), (50,), (50,),\n",
              "                    (50,), (50,), (50,), (50,), (50, 50, 50, 50),\n",
              "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
              "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
              "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
              "                    (50, 50, 50), (50, 50, 50), (50, 50, 50), (50, 50, 50),\n",
              "                    (50, 50, 50), (50, 50, 50), (50, 50, 50), (50, 50, 50),\n",
              "                    (50, 50, 50), (50, 50, 50), (50, 50), (50, 50),\n",
              "                    (50, 50), (50, 50), (50, 50), (50, 50), (50, 50),\n",
              "                    (50, 50), (50, 50), (50, 50), (50,), (50,), (50,),\n",
              "                    (50,), (50,), (50,), (50,), (50,), (50,), (50,),\n",
              "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
              "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
              "                    (50, 50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50),\n",
              "                    (50, 50, 50, 50), (50, 50, 50), (50, 50, 50),\n",
              "                    (50, 50, 50), (50, 50, 50), (50, 50, 50), (50, 50, 50),\n",
              "                    (50, 50, 50), (50, 50, 50), (50, 50, 50), (50, 50, 50),\n",
              "                    (50, 50), (50, 50), (50, 50), (50, 50), (50, 50),\n",
              "                    (50, 50), (50, 50), (50, 50), (50, 50), (50, 50),\n",
              "                    (50,), (50,), (50,), (50,), (50,), (50,), (50,), (50,),\n",
              "                    (50,), (50,)],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_nb_epoch': masked_array(data=[20, 20, 20, 20, 20, 25, 25, 25, 25, 25, 20, 20, 20, 20,\n",
              "                    20, 25, 25, 25, 25, 25, 20, 20, 20, 20, 20, 25, 25, 25,\n",
              "                    25, 25, 20, 20, 20, 20, 20, 25, 25, 25, 25, 25, 20, 20,\n",
              "                    20, 20, 20, 25, 25, 25, 25, 25, 20, 20, 20, 20, 20, 25,\n",
              "                    25, 25, 25, 25, 20, 20, 20, 20, 20, 25, 25, 25, 25, 25,\n",
              "                    20, 20, 20, 20, 20, 25, 25, 25, 25, 25, 20, 20, 20, 20,\n",
              "                    20, 25, 25, 25, 25, 25, 20, 20, 20, 20, 20, 25, 25, 25,\n",
              "                    25, 25, 20, 20, 20, 20, 20, 25, 25, 25, 25, 25, 20, 20,\n",
              "                    20, 20, 20, 25, 25, 25, 25, 25, 20, 20, 20, 20, 20, 25,\n",
              "                    25, 25, 25, 25, 20, 20, 20, 20, 20, 25, 25, 25, 25, 25,\n",
              "                    20, 20, 20, 20, 20, 25, 25, 25, 25, 25, 20, 20, 20, 20,\n",
              "                    20, 25, 25, 25, 25, 25],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_optimizer': masked_array(data=['adam', 'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax', 'adam',\n",
              "                    'adagrad', 'sgd', 'RMSprop', 'Adamax'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'params': [{'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'relu',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 128,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50, 50),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 20,\n",
              "   'optimizer': 'Adamax'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adam'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'adagrad'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'sgd'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'RMSprop'},\n",
              "  {'activation': 'tanh',\n",
              "   'batch_size': 256,\n",
              "   'hidden_layer_sizes': (50,),\n",
              "   'nb_epoch': 25,\n",
              "   'optimizer': 'Adamax'}],\n",
              " 'split0_test_score': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.09375   , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.23125   , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.01041667, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.0125    , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.20416667, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.08125   , 0.00625   , 0.        , 0.00416667, 0.        ,\n",
              "        0.20625   , 0.01875   , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.00416667,\n",
              "        0.        , 0.01666667, 0.        , 0.02291667, 0.01666667,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.02291667, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.00416667, 0.01041667, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.98958331, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.00416667, 0.        , 0.        , 0.        ]),\n",
              " 'split1_test_score': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.32291666, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.33125001, 0.        , 0.03333334, 0.        ,\n",
              "        0.        , 0.20416667, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.03333334, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.20208333, 0.        , 0.14375   , 0.        ,\n",
              "        0.00625   , 0.52916664, 0.00416667, 0.33541667, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.28958333, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.32499999, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.21458334, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.22083333, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.24583334, 0.28541666, 0.        , 0.        ,\n",
              "        0.20416667, 0.00208333, 0.        , 0.33750001, 0.10625   ,\n",
              "        0.01458333, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.38958332, 0.00208333, 0.        , 0.00208333, 0.33750001,\n",
              "        0.16875   , 0.00208333, 0.        , 0.01666667, 0.23125   ,\n",
              "        0.36458334, 0.0125    , 0.        , 0.24375001, 0.04375   ,\n",
              "        0.18958333, 0.33541667, 0.02083333, 0.08333334, 0.34375   ,\n",
              "        0.40208334, 0.23958333, 0.        , 0.02708333, 0.19583334,\n",
              "        0.63125002, 0.03333334, 0.        , 0.01041667, 0.00416667,\n",
              "        0.33333334, 0.075     , 0.        , 0.        , 0.25208333,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.30000001,\n",
              "        0.38333333, 0.30208334, 0.        , 0.        , 0.00625   ,\n",
              "        0.00625   , 0.0875    , 0.0125    , 0.        , 0.        ,\n",
              "        0.0125    , 0.05      , 0.10208333, 0.        , 0.30833334,\n",
              "        0.20625   , 0.23541667, 0.        , 0.33333334, 0.18541667,\n",
              "        0.0125    , 0.        , 0.00208333, 0.02916667, 0.17916666,\n",
              "        0.03333334, 0.33333334, 0.08541667, 0.07083333, 0.03958333]),\n",
              " 'split2_test_score': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.00208333, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.00208333, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.23958333, 0.00416667, 0.        , 0.        ,\n",
              "        0.        , 0.70833331, 0.06875   , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.00208333, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.19791667, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.0125    , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.01041667, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.00416667, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.30000001, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.175     , 0.        , 0.        , 0.        ]),\n",
              " 'split3_test_score': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.3125    , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.02708333, 0.19583334, 0.00208333, 0.25208333, 0.        ,\n",
              "        0.11666667, 0.07291666, 0.        , 0.26458332, 0.        ,\n",
              "        0.00833333, 0.29583332, 0.30208334, 0.07916667, 0.12708333,\n",
              "        0.2375    , 0.31458333, 0.36250001, 0.08333334, 0.00416667,\n",
              "        0.        , 0.025     , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.00208333, 0.        , 0.20208333, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.31666666, 0.00625   , 0.30625001, 0.        ,\n",
              "        0.        , 0.        , 0.01875   , 0.27500001, 0.        ,\n",
              "        0.        , 0.00416667, 0.00208333, 0.08958333, 0.        ,\n",
              "        0.19166666, 0.27291667, 0.06458333, 0.06666667, 0.        ,\n",
              "        0.1       , 0.28333333, 0.        , 0.38333333, 0.13333334,\n",
              "        0.0625    , 0.        , 0.        , 0.2375    , 0.25833333,\n",
              "        0.00208333, 0.08125   , 0.        , 0.15625   , 0.33125001,\n",
              "        0.09375   , 0.06458333, 0.        , 0.00416667, 0.25416666,\n",
              "        0.1875    , 0.01666667, 0.        , 0.02083333, 0.09791667,\n",
              "        0.21041666, 0.15000001, 0.        , 0.27916667, 0.22291666,\n",
              "        0.21875   , 0.00625   , 0.00416667, 0.22916667, 0.175     ,\n",
              "        0.10208333, 0.00625   , 0.0125    , 0.2       , 0.13333334,\n",
              "        0.03541667, 0.        , 0.        , 0.        , 0.00416667,\n",
              "        0.15416667, 0.11458334, 0.        , 0.        , 0.12291667,\n",
              "        0.27083334, 0.54583335, 0.        , 0.05833333, 0.09166667,\n",
              "        0.18333334, 0.37916666, 0.        , 0.1       , 0.00208333,\n",
              "        0.28541666, 0.31458333, 0.00416667, 0.06666667, 0.01041667,\n",
              "        0.21250001, 0.25416666, 0.02916667, 0.        , 0.01458333,\n",
              "        0.12708333, 0.31458333, 0.35624999, 0.0375    , 0.17291667,\n",
              "        0.18333334, 0.34166667, 0.17083333, 0.02916667, 0.06875   ]),\n",
              " 'split4_test_score': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.75625002, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.0125    , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.15833333, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.03541667, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.00416667, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.24791667, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.02708333, 0.00208333, 0.        , 0.        ]),\n",
              " 'mean_test_score': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.06458333, 0.        , 0.0625    , 0.        ,\n",
              "        0.        , 0.06625   , 0.        , 0.00666667, 0.        ,\n",
              "        0.00541667, 0.08      , 0.00041667, 0.05041667, 0.        ,\n",
              "        0.02333333, 0.02125   , 0.        , 0.05291666, 0.        ,\n",
              "        0.00166667, 0.25125   , 0.06041667, 0.04458333, 0.02541667,\n",
              "        0.04875   , 0.19041666, 0.07333334, 0.08375   , 0.00083333,\n",
              "        0.        , 0.005     , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.05833333, 0.        , 0.04041667, 0.        ,\n",
              "        0.        , 0.065     , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.06333333, 0.00125   , 0.06125   , 0.        ,\n",
              "        0.        , 0.13708333, 0.00458333, 0.055     , 0.        ,\n",
              "        0.        , 0.21833333, 0.01416667, 0.01791667, 0.        ,\n",
              "        0.03833333, 0.11291667, 0.07      , 0.01333333, 0.        ,\n",
              "        0.06083333, 0.05708333, 0.        , 0.14416667, 0.04791667,\n",
              "        0.01833333, 0.        , 0.        , 0.0475    , 0.05166667,\n",
              "        0.11916666, 0.01666667, 0.        , 0.03166667, 0.13375   ,\n",
              "        0.0525    , 0.01333333, 0.        , 0.00416667, 0.09708333,\n",
              "        0.12666667, 0.00708333, 0.        , 0.05375   , 0.02833333,\n",
              "        0.12125   , 0.10083334, 0.00416667, 0.0725    , 0.11333333,\n",
              "        0.16375   , 0.04916667, 0.00083333, 0.05125   , 0.075     ,\n",
              "        0.14666667, 0.01375   , 0.0025    , 0.04666667, 0.03083333,\n",
              "        0.07375   , 0.015     , 0.        , 0.        , 0.05125   ,\n",
              "        0.03291667, 0.02291667, 0.        , 0.00083333, 0.08458334,\n",
              "        0.13083333, 0.16958334, 0.        , 0.01166667, 0.01958333,\n",
              "        0.03791667, 0.09875   , 0.0025    , 0.02      , 0.00041667,\n",
              "        0.05958333, 0.07291667, 0.02125   , 0.01333333, 0.06375   ,\n",
              "        0.08458333, 0.14958333, 0.00583333, 0.06666667, 0.04      ,\n",
              "        0.02791667, 0.32083333, 0.07166666, 0.01333333, 0.07041667,\n",
              "        0.04333333, 0.17625   , 0.05166667, 0.02      , 0.02166667]),\n",
              " 'std_test_score': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.12916666, 0.        , 0.125     , 0.        ,\n",
              "        0.        , 0.1325    , 0.        , 0.01333333, 0.        ,\n",
              "        0.01083333, 0.09801502, 0.00083333, 0.10083333, 0.        ,\n",
              "        0.04666667, 0.02887954, 0.        , 0.10583333, 0.        ,\n",
              "        0.00333333, 0.27735921, 0.12083334, 0.05829761, 0.05083333,\n",
              "        0.09440604, 0.20329148, 0.14459234, 0.12990649, 0.00166667,\n",
              "        0.        , 0.01      , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.11562781, 0.        , 0.08083333, 0.        ,\n",
              "        0.        , 0.13      , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.12666667, 0.0025    , 0.1225    , 0.        ,\n",
              "        0.        , 0.11221724, 0.00726483, 0.11      , 0.        ,\n",
              "        0.        , 0.2597087 , 0.02730359, 0.03583333, 0.        ,\n",
              "        0.07666667, 0.12044046, 0.11057457, 0.02666667, 0.        ,\n",
              "        0.08146233, 0.11312788, 0.        , 0.17716126, 0.05930735,\n",
              "        0.02279894, 0.        , 0.        , 0.095     , 0.10333333,\n",
              "        0.15649924, 0.03230174, 0.        , 0.06229689, 0.16382155,\n",
              "        0.06853375, 0.0256377 , 0.        , 0.00645497, 0.11912295,\n",
              "        0.1374457 , 0.00666667, 0.        , 0.09531017, 0.03869844,\n",
              "        0.09924541, 0.13001469, 0.00833333, 0.10825638, 0.14396783,\n",
              "        0.15140889, 0.0952391 , 0.00166667, 0.08957461, 0.09040803,\n",
              "        0.24549625, 0.01130388, 0.005     , 0.07712949, 0.05161288,\n",
              "        0.13051448, 0.03      , 0.        , 0.        , 0.10042963,\n",
              "        0.06075909, 0.04583333, 0.        , 0.00166667, 0.11775976,\n",
              "        0.16413917, 0.22153819, 0.        , 0.02333333, 0.03612286,\n",
              "        0.07274862, 0.14367208, 0.005     , 0.04      , 0.00083333,\n",
              "        0.1130204 , 0.12237522, 0.04044887, 0.02666667, 0.1223582 ,\n",
              "        0.10192249, 0.11808219, 0.01166667, 0.13333334, 0.07292738,\n",
              "        0.04981912, 0.36154327, 0.14229395, 0.01654119, 0.08626509,\n",
              "        0.07118052, 0.14415463, 0.06802522, 0.02781387, 0.02809335]),\n",
              " 'rank_test_score': array([109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109,  37, 109,\n",
              "         40, 109, 109,  35, 109,  93, 109,  95,  25, 107,  55, 109,  73,\n",
              "         77, 109,  49, 109, 102,   2,  43,  61,  72,  57,   4,  28,  24,\n",
              "        104, 109,  96, 109, 109, 109, 109, 109, 109, 109, 109, 109,  45,\n",
              "        109,  63, 109, 109,  36, 109, 109, 109, 109,  39, 103,  41, 109,\n",
              "        109,  11,  97,  47, 109, 109,   3,  85,  82, 109,  65,  18,  33,\n",
              "         87, 109,  42,  46, 109,  10,  58,  81, 109, 109,  59,  52,  16,\n",
              "         83, 109,  68,  12,  50,  90, 109,  98,  21,  14,  92, 109,  48,\n",
              "         70,  15,  19,  99,  30,  17,   7,  56, 104,  53,  26,   9,  86,\n",
              "        100,  60,  69,  27,  84, 109, 109,  54,  67,  74, 109, 104,  22,\n",
              "         13,   6, 109,  91,  80,  66,  20, 100,  78, 107,  44,  29,  76,\n",
              "         87,  38,  23,   8,  94,  34,  64,  71,   1,  31,  89,  32,  62,\n",
              "          5,  51,  79,  75], dtype=int32)}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "grid_result.cv_results_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = grid_result.cv_results_['std_test_score'] < 0.1\n",
        "np.argmax(grid_result.cv_results_['mean_test_score'][mask])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZiUqBJ2CP8g",
        "outputId": "ecf01cff-eb2d-4926-dd09-855e8af623de"
      },
      "id": "zZiUqBJ2CP8g",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "218e1f12-84e4-46fa-a483-7b7052858bd4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "218e1f12-84e4-46fa-a483-7b7052858bd4",
        "outputId": "ec0e0ea6-54ee-46fb-9683-472b36837cc2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.00041666668839752675,\n",
              " 0.00041666668839752675,\n",
              " 0.0008333333767950535,\n",
              " 0.0008333333767950535,\n",
              " 0.0008333333767950535,\n",
              " 0.0012500000186264515,\n",
              " 0.001666666753590107,\n",
              " 0.002500000037252903,\n",
              " 0.002500000037252903,\n",
              " 0.00416666679084301,\n",
              " 0.004166666883975267,\n",
              " 0.004583333525806666,\n",
              " 0.005000000074505806,\n",
              " 0.005416666716337204,\n",
              " 0.005833333358168602,\n",
              " 0.006666667014360428,\n",
              " 0.007083333563059568,\n",
              " 0.011666666716337204,\n",
              " 0.013333332957699895,\n",
              " 0.013333333656191826,\n",
              " 0.013333334028720856,\n",
              " 0.013333334028720856,\n",
              " 0.013750000577419996,\n",
              " 0.01416666698642075,\n",
              " 0.015000000596046448,\n",
              " 0.01666666609235108,\n",
              " 0.017916665971279146,\n",
              " 0.018333333404734732,\n",
              " 0.019583333749324083,\n",
              " 0.019999999925494193,\n",
              " 0.020000000298023225,\n",
              " 0.02124999985098839,\n",
              " 0.021249999944120645,\n",
              " 0.021666666865348815,\n",
              " 0.02291666716337204,\n",
              " 0.023333333432674408,\n",
              " 0.02541666626930237,\n",
              " 0.02791666630655527,\n",
              " 0.028333333879709245,\n",
              " 0.03083333494141698,\n",
              " 0.03166666668839753,\n",
              " 0.03291666712611914,\n",
              " 0.03791666748002172,\n",
              " 0.038333332538604735,\n",
              " 0.04000000040978193,\n",
              " 0.04041666686534882,\n",
              " 0.04333333447575569,\n",
              " 0.044583332538604734,\n",
              " 0.04666666742414236,\n",
              " 0.04749999940395355,\n",
              " 0.04791666865348816,\n",
              " 0.04874999942258,\n",
              " 0.04916666569188237,\n",
              " 0.05041666626930237,\n",
              " 0.05124999964609742,\n",
              " 0.051250001043081285,\n",
              " 0.051666665077209475,\n",
              " 0.05166666698642075,\n",
              " 0.052500000596046446,\n",
              " 0.05291666388511658,\n",
              " 0.05375000135973096,\n",
              " 0.0550000011920929,\n",
              " 0.05708333295769989,\n",
              " 0.058333331765607,\n",
              " 0.05958333257585764,\n",
              " 0.06041666865348816,\n",
              " 0.06083333343267441,\n",
              " 0.061250001192092896,\n",
              " 0.0625,\n",
              " 0.06333333253860474,\n",
              " 0.06375000085681677,\n",
              " 0.06458333134651184,\n",
              " 0.06499999761581421,\n",
              " 0.0662500023841858,\n",
              " 0.06666666865348816,\n",
              " 0.06999999880790711,\n",
              " 0.07041666507720948,\n",
              " 0.07166666430421173,\n",
              " 0.07250000089406967,\n",
              " 0.07291666641831399,\n",
              " 0.07333333576098085,\n",
              " 0.07375000193715095,\n",
              " 0.07500000083819032,\n",
              " 0.0800000011920929,\n",
              " 0.08375000208616257,\n",
              " 0.0845833339728415,\n",
              " 0.08458333611488342,\n",
              " 0.09708333313465119,\n",
              " 0.09874999905005097,\n",
              " 0.10083333626389504,\n",
              " 0.1129166690632701,\n",
              " 0.11333333253860474,\n",
              " 0.11916666370816528,\n",
              " 0.1212499976158142,\n",
              " 0.1266666680574417,\n",
              " 0.13083333373069764,\n",
              " 0.1337500035762787,\n",
              " 0.13708333373069764,\n",
              " 0.14416666626930236,\n",
              " 0.14666667133569716,\n",
              " 0.14958333279937505,\n",
              " 0.16375000178813934,\n",
              " 0.1695833384990692,\n",
              " 0.1762500018812716,\n",
              " 0.19041666076518596,\n",
              " 0.21833332860842347,\n",
              " 0.2512500022072345,\n",
              " 0.3208333313465118]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "sorted(grid_result.cv_results_['mean_test_score'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfr2aPpdAlQp",
        "outputId": "43899fa3-7f7c-4e8c-ab2f-043c7993ad68"
      },
      "id": "kfr2aPpdAlQp",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'tanh',\n",
              " 'batch_size': 128,\n",
              " 'hidden_layer_sizes': (50, 50),\n",
              " 'nb_epoch': 20,\n",
              " 'optimizer': 'adam'}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "03f0a4b8-17e1-4569-bb6e-f3578e83f3f9",
      "metadata": {
        "id": "03f0a4b8-17e1-4569-bb6e-f3578e83f3f9"
      },
      "outputs": [],
      "source": [
        "import pickle \n",
        "with open('/content/drive/MyDrive/Colab Notebooks/nlp_project/grid_result_best_params1.pickle', 'wb') as f:\n",
        "    pickle.dump(grid_result.best_params_, f)\n",
        "    \n",
        "# text_transformed2 = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/nlp_project/text_transformed_window_size15_sample_size1000.pickle', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = language_model(activation=grid_result.best_params_['activation'], \n",
        "                   optimizer=grid_result.best_params_['optimizer'], \n",
        "                   hidden_layer_sizes=grid_result.best_params_['hidden_layer_sizes'])\n",
        "\n",
        "model2.fit(X_train, y_train, epochs=grid_result.best_params_['nb_epoch'], batch_size=grid_result.best_params_['batch_size'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EllPxuj68BQu",
        "outputId": "e202cb57-ed17-432d-ad6a-366baf97bb42"
      },
      "id": "EllPxuj68BQu",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "10/10 [==============================] - 3s 19ms/step - loss: 1.1005 - accuracy: 0.3179\n",
            "Epoch 2/20\n",
            "10/10 [==============================] - 0s 15ms/step - loss: 1.0997 - accuracy: 0.3196\n",
            "Epoch 3/20\n",
            "10/10 [==============================] - 0s 13ms/step - loss: 1.1007 - accuracy: 0.3142\n",
            "Epoch 4/20\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 1.0998 - accuracy: 0.3271\n",
            "Epoch 5/20\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 1.0988 - accuracy: 0.3425\n",
            "Epoch 6/20\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 1.0994 - accuracy: 0.3212\n",
            "Epoch 7/20\n",
            "10/10 [==============================] - 0s 13ms/step - loss: 1.0993 - accuracy: 0.3317\n",
            "Epoch 8/20\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 1.0989 - accuracy: 0.3283\n",
            "Epoch 9/20\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 1.0986 - accuracy: 0.3363\n",
            "Epoch 10/20\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 1.0991 - accuracy: 0.3329\n",
            "Epoch 11/20\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 1.0994 - accuracy: 0.3279\n",
            "Epoch 12/20\n",
            "10/10 [==============================] - 0s 13ms/step - loss: 1.0984 - accuracy: 0.3363\n",
            "Epoch 13/20\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 1.0981 - accuracy: 0.3429\n",
            "Epoch 14/20\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 1.0982 - accuracy: 0.3421\n",
            "Epoch 15/20\n",
            "10/10 [==============================] - 0s 13ms/step - loss: 1.0981 - accuracy: 0.3542\n",
            "Epoch 16/20\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 1.0987 - accuracy: 0.3379\n",
            "Epoch 17/20\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 1.0980 - accuracy: 0.3408\n",
            "Epoch 18/20\n",
            "10/10 [==============================] - 0s 13ms/step - loss: 1.0978 - accuracy: 0.3433\n",
            "Epoch 19/20\n",
            "10/10 [==============================] - 0s 13ms/step - loss: 1.0980 - accuracy: 0.3479\n",
            "Epoch 20/20\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 1.0984 - accuracy: 0.3338\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f36ad5b6460>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(np.argmax(model2.predict(X_test), axis=1), np.argmax(y_test, axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq2yb0Fz8bKG",
        "outputId": "db4e570c-dbbb-4819-b2e5-2ed2e86c08d4"
      },
      "id": "hq2yb0Fz8bKG",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/19 [==============================] - 1s 4ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3466666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5P1UFOt8QMN",
        "outputId": "1cce32f0-c0f3-4786-9027-bf770437ab58"
      },
      "id": "K5P1UFOt8QMN",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/19 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.32899114, 0.34335256, 0.32765627],\n",
              "       [0.32988042, 0.3293508 , 0.34076878],\n",
              "       [0.3271588 , 0.34085882, 0.33198237],\n",
              "       ...,\n",
              "       [0.32931495, 0.34006554, 0.3306195 ],\n",
              "       [0.3329972 , 0.3345674 , 0.3324354 ],\n",
              "       [0.32666722, 0.33503953, 0.33829325]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dc5c1a5-8eb0-4bf4-befa-9c1d383cc028",
      "metadata": {
        "id": "5dc5c1a5-8eb0-4bf4-befa-9c1d383cc028"
      },
      "outputs": [],
      "source": [
        "# for early stopping\n",
        "# https://qiita.com/sasayabaku/items/b7872a3b8acc7d6261bf\n",
        "\n",
        "# stacked lstm model example\n",
        "# https://machinelearningknowledge.ai/keras-lstm-layer-explained-for-beginners-with-example/\n",
        "\n",
        "# lstm layer documentation\n",
        "# https://keras.io/api/layers/recurrent_layers/lstm/\n",
        "\n",
        "# Should you use relu for activation in lstm\n",
        "# https://stats.stackexchange.com/questions/444923/activation-function-between-lstm-layers\n",
        "\n",
        "# lstm explained\n",
        "# https://qiita.com/t_Signull/items/21b82be280b46f467d1b\n",
        "\n",
        "# stacked lstm explained\n",
        "# https://machinelearningmastery.com/stacked-long-short-term-memory-networks/\n",
        "\n",
        "# error solved\n",
        "# https://stackoverflow.com/questions/58119320/valueerror-input-0-of-layer-lstm-is-incompatible-with-the-layer-expected-ndim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f21014f0-96c6-4124-898f-228bc07ef250",
      "metadata": {
        "id": "f21014f0-96c6-4124-898f-228bc07ef250"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "24467e8a-3f01-459a-a198-4b6deaa3f78c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24467e8a-3f01-459a-a198-4b6deaa3f78c",
        "outputId": "f3442960-e825-4825-cadf-b398f7ae773f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0836329days (2.00719h)\n"
          ]
        }
      ],
      "source": [
        "!cat /proc/uptime | awk '{print $1 /60 /60 /24 \"days (\" $1 / 60 / 60 \"h)\"}'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'activation': 'tanh',\n",
        " 'batch_size': 128,\n",
        " 'hidden_layer_sizes': (50, 50),\n",
        " 'nb_epoch': 20,\n",
        " 'optimizer': 'adam'}"
      ],
      "metadata": {
        "id": "Hc_3KoTo-CdX"
      },
      "id": "Hc_3KoTo-CdX",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = language_model(activation=params['activation'], \n",
        "                   optimizer=params['optimizer'], \n",
        "                   hidden_layer_sizes=params['hidden_layer_sizes'])\n",
        "\n",
        "model3.fit(X_train, y_train, epochs=params['nb_epoch'], batch_size=params['batch_size'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3ACfYqKAGO4",
        "outputId": "ff33ace0-3aaf-4b29-c681-a5757d3feff9"
      },
      "id": "n3ACfYqKAGO4",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "19/19 [==============================] - 5s 18ms/step - loss: 1.0728 - accuracy: 0.4187\n",
            "Epoch 2/20\n",
            "19/19 [==============================] - 0s 12ms/step - loss: 1.0115 - accuracy: 0.4529\n",
            "Epoch 3/20\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.9750 - accuracy: 0.5071\n",
            "Epoch 4/20\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.9525 - accuracy: 0.5121\n",
            "Epoch 5/20\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.9291 - accuracy: 0.5221\n",
            "Epoch 6/20\n",
            "19/19 [==============================] - 0s 12ms/step - loss: 0.9039 - accuracy: 0.5412\n",
            "Epoch 7/20\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.9035 - accuracy: 0.5442\n",
            "Epoch 8/20\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.8892 - accuracy: 0.5592\n",
            "Epoch 9/20\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.9078 - accuracy: 0.5521\n",
            "Epoch 10/20\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.8461 - accuracy: 0.5800\n",
            "Epoch 11/20\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.8117 - accuracy: 0.6075\n",
            "Epoch 12/20\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.7826 - accuracy: 0.6271\n",
            "Epoch 13/20\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.7967 - accuracy: 0.6079\n",
            "Epoch 14/20\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.7431 - accuracy: 0.6558\n",
            "Epoch 15/20\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.7291 - accuracy: 0.6371\n",
            "Epoch 16/20\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.6837 - accuracy: 0.6833\n",
            "Epoch 17/20\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.6631 - accuracy: 0.6992\n",
            "Epoch 18/20\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.6256 - accuracy: 0.7146\n",
            "Epoch 19/20\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.5990 - accuracy: 0.7271\n",
            "Epoch 20/20\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.5800 - accuracy: 0.7221\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f36ad377d90>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(np.argmax(model3.predict(X_test), axis=1), np.argmax(y_test, axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYC4V_2dARYo",
        "outputId": "cf30cd79-84fc-4672-aed1-a16875863eae"
      },
      "id": "gYC4V_2dARYo",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/19 [==============================] - 1s 4ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5466666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej798su8HONS",
        "outputId": "f3a37ed9-6190-46a0-e5b4-d8dedf46ddf5"
      },
      "id": "Ej798su8HONS",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/19 [==============================] - 0s 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.3287083e-01, 4.6697378e-01, 1.5539600e-04],\n",
              "       [3.9313397e-01, 4.1999364e-01, 1.8687238e-01],\n",
              "       [4.2869858e-02, 3.6385888e-01, 5.9327132e-01],\n",
              "       ...,\n",
              "       [4.5412302e-04, 2.6838525e-04, 9.9927753e-01],\n",
              "       [7.0622128e-01, 2.9362595e-01, 1.5275500e-04],\n",
              "       [6.4372247e-01, 3.5624692e-01, 3.0594765e-05]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iMCrIHknJh21"
      },
      "id": "iMCrIHknJh21",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def language_model2(optimizer=\"adam\", hidden_layer_sizes=(100, 100)):\n",
        "    model = Sequential()\n",
        "    firstflag = True\n",
        "    for dim in hidden_layer_sizes:\n",
        "        if firstflag:\n",
        "            model.add(LSTM(n_mid, input_shape=(n_rnn, data_dim), return_sequences=True))\n",
        "            model.add(Dropout(0.2))\n",
        "            firstflag = False\n",
        "        else:\n",
        "            model.add(LSTM(n_mid, return_sequences=True))\n",
        "            model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(LSTM(n_mid))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(output_dim, activation=\"softmax\"))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "2Lhli50MJh5A"
      },
      "id": "2Lhli50MJh5A",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model5 = KerasClassifier(build_fn=language_model2, verbose=0)\n",
        "param_grid2 = dict(optimizer=optimizer, \n",
        "                  hidden_layer_sizes=hidden_layer_sizes, \n",
        "                  nb_epoch=nb_epoch, \n",
        "                  batch_size=batch_size,)\n",
        "grid3 = GridSearchCV(estimator=model5, param_grid=param_grid2, verbose=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBBeHPAaJjLD",
        "outputId": "262b96df-c806-4e00-cfca-3f08e2bafce1"
      },
      "id": "oBBeHPAaJjLD",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-c76952355f90>:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model5 = KerasClassifier(build_fn=language_model2, verbose=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid3_result = grid3.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYoElLCDJwNr",
        "outputId": "ca30352b-5c7c-4f24-a445-4c437fc43147"
      },
      "id": "MYoElLCDJwNr",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=  11.5s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   9.5s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   9.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   9.5s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   9.2s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   9.5s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=  10.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.6s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.2s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.9s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=  10.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.5s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.6s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.5s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.5s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.5s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.5s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=   9.2s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   9.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   9.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.6s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   9.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   9.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.5s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   9.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   9.2s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   9.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=  10.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   9.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   9.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=  11.0s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   9.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   9.5s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   9.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.0s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.0s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.2s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.9s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.9s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.9s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.2s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.9s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.0s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.6s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.6s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.0s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.5s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.5s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   6.9s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   6.9s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.0s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.9s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.2s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   5.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   5.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   5.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   6.0s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   6.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.5s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   5.5s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.0s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   5.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   5.6s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.6s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.6s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   5.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.2s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.2s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   5.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   5.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   5.6s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.2s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.6s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.2s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.6s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   6.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   6.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.0s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.9s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   5.9s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.6s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.6s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   3.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   3.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   3.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.5s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   3.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   3.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   3.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   3.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   3.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.9s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.9s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.5s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.9s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   3.9s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.1s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.5s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   3.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   3.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   3.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   3.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.3s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   3.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   3.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   3.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   3.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   3.7s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.5s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.9s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   3.9s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.5s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.0s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.4s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.8s\n",
            "[CV] END batch_size=128, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   3.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=  10.2s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=  10.0s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=  10.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=  10.0s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adam; total time=   9.5s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=  10.2s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=  10.0s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=  10.0s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   9.5s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=  11.3s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=  10.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.3s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=  10.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   9.9s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=sgd; total time=  10.0s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=  10.3s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=  10.6s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=  10.3s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=  10.5s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.5s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=  10.3s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=  10.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.4s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=  10.3s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=  10.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=  10.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=  10.0s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adam; total time=  10.3s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=  10.3s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=  10.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=  10.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   9.5s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=  10.2s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=  10.0s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   9.3s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=  10.4s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=  10.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=sgd; total time=  10.2s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   9.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=  10.4s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=  10.4s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=  10.4s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=  10.4s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   9.5s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=  10.3s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=  10.0s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=  10.2s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=  10.5s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.7s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.3s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   7.7s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.3s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adam; total time=   8.4s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.6s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.3s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.4s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   7.7s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=adagrad; total time=   8.5s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.5s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.5s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.0s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   7.6s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=sgd; total time=   8.5s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.2s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   9.0s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.9s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=RMSprop; total time=   8.6s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.6s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.5s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   8.3s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   7.9s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=20, optimizer=Adamax; total time=   9.6s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.3s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.7s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.6s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   7.7s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adam; total time=   8.4s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.4s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.6s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.2s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   8.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=adagrad; total time=   7.5s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.4s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.6s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.3s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   8.2s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=sgd; total time=   7.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.9s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   8.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.9s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.6s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   8.5s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.7s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   9.4s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50, 50), nb_epoch=25, optimizer=Adamax; total time=   7.5s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.6s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.6s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.2s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   6.2s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adam; total time=   7.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   6.0s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   6.6s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   6.0s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   6.7s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=adagrad; total time=   5.9s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.5s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   5.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=sgd; total time=   6.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.4s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   7.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.4s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=RMSprop; total time=   6.4s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.6s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.2s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.3s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=20, optimizer=Adamax; total time=   6.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.9s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.4s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adam; total time=   6.9s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   5.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.6s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.2s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.5s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=adagrad; total time=   6.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   6.4s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   6.0s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   6.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   5.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=sgd; total time=   6.5s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.4s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   7.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.5s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=RMSprop; total time=   6.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.6s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   5.9s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.2s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.7s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50, 50), nb_epoch=25, optimizer=Adamax; total time=   6.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.7s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.2s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.7s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adam; total time=   4.2s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.6s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.0s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.6s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=adagrad; total time=   4.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.0s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=sgd; total time=   4.7s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.3s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.6s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   5.5s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.3s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=RMSprop; total time=   4.2s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.2s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.0s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.7s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=20, optimizer=Adamax; total time=   4.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.7s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.2s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adam; total time=   4.7s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.2s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=adagrad; total time=   4.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.0s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.6s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.0s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.5s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=sgd; total time=   4.0s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.3s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.4s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.3s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=RMSprop; total time=   4.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.2s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.8s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.1s\n",
            "[CV] END batch_size=256, hidden_layer_sizes=(50,), nb_epoch=25, optimizer=Adamax; total time=   4.1s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid3_result.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEPoG5ZVJ5ca",
        "outputId": "9bb23dde-b4e7-42e4-afbb-81e61bde3c45"
      },
      "id": "TEPoG5ZVJ5ca",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 128,\n",
              " 'hidden_layer_sizes': (50, 50),\n",
              " 'nb_epoch': 20,\n",
              " 'optimizer': 'RMSprop'}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid3_result.best_score_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-GhuIe5UrSp",
        "outputId": "b670051a-42b8-4151-e5af-80db6b20664a"
      },
      "id": "J-GhuIe5UrSp",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.35"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model6 = language_model(optimizer=grid3_result.best_params_['optimizer'], \n",
        "                   hidden_layer_sizes=grid3_result.best_params_['hidden_layer_sizes'])\n",
        "\n",
        "model6.fit(X_train, y_train, epochs=grid3_result.best_params_['nb_epoch'], batch_size=grid3_result.best_params_['batch_size'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmWx3Bl0Uv-B",
        "outputId": "ef7ec859-4100-4e4b-8ef4-491f98ee2f48"
      },
      "id": "JmWx3Bl0Uv-B",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_4213 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4214 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4215 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "19/19 [==============================] - 5s 92ms/step - loss: 1.0989 - accuracy: 0.3767\n",
            "Epoch 2/20\n",
            "19/19 [==============================] - 2s 95ms/step - loss: 1.1938 - accuracy: 0.4638\n",
            "Epoch 3/20\n",
            "19/19 [==============================] - 2s 89ms/step - loss: 0.9828 - accuracy: 0.4908\n",
            "Epoch 4/20\n",
            "19/19 [==============================] - 2s 103ms/step - loss: 0.9649 - accuracy: 0.5083\n",
            "Epoch 5/20\n",
            "19/19 [==============================] - 2s 92ms/step - loss: 0.9601 - accuracy: 0.5075\n",
            "Epoch 6/20\n",
            "19/19 [==============================] - 2s 90ms/step - loss: 0.9468 - accuracy: 0.5108\n",
            "Epoch 7/20\n",
            "19/19 [==============================] - 2s 92ms/step - loss: 0.9067 - accuracy: 0.5354\n",
            "Epoch 8/20\n",
            "19/19 [==============================] - 2s 92ms/step - loss: 0.9009 - accuracy: 0.5483\n",
            "Epoch 9/20\n",
            "19/19 [==============================] - 2s 94ms/step - loss: 0.8529 - accuracy: 0.5554\n",
            "Epoch 10/20\n",
            "19/19 [==============================] - 2s 94ms/step - loss: 0.8269 - accuracy: 0.5917\n",
            "Epoch 11/20\n",
            "19/19 [==============================] - 2s 95ms/step - loss: 0.8143 - accuracy: 0.5633\n",
            "Epoch 12/20\n",
            "19/19 [==============================] - 2s 93ms/step - loss: 0.7844 - accuracy: 0.5854\n",
            "Epoch 13/20\n",
            "19/19 [==============================] - 2s 93ms/step - loss: 0.7583 - accuracy: 0.6125\n",
            "Epoch 14/20\n",
            "19/19 [==============================] - 2s 93ms/step - loss: 0.7525 - accuracy: 0.6254\n",
            "Epoch 15/20\n",
            "19/19 [==============================] - 2s 88ms/step - loss: 0.7048 - accuracy: 0.6321\n",
            "Epoch 16/20\n",
            "19/19 [==============================] - 2s 91ms/step - loss: 0.7189 - accuracy: 0.6396\n",
            "Epoch 17/20\n",
            "19/19 [==============================] - 3s 135ms/step - loss: 0.6620 - accuracy: 0.6692\n",
            "Epoch 18/20\n",
            "19/19 [==============================] - 2s 93ms/step - loss: 0.7166 - accuracy: 0.6746\n",
            "Epoch 19/20\n",
            "19/19 [==============================] - 3s 137ms/step - loss: 0.6379 - accuracy: 0.6908\n",
            "Epoch 20/20\n",
            "19/19 [==============================] - 3s 187ms/step - loss: 0.6460 - accuracy: 0.6979\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f37711fb0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(np.argmax(model6.predict(X_test), axis=1), np.argmax(y_test, axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Rz3OEy0VCH-",
        "outputId": "1728f517-ee9b-492f-8d5f-bc0ebc760158"
      },
      "id": "_Rz3OEy0VCH-",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/19 [==============================] - 1s 21ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.42833333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model6.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thiJN4CoVRpA",
        "outputId": "4baf59d0-9ace-48ee-effa-a108425e89bd"
      },
      "id": "thiJN4CoVRpA",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/19 [==============================] - 0s 17ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[8.02448750e-01, 1.97545916e-01, 5.38991071e-06],\n",
              "       [4.49187368e-01, 4.51720476e-01, 9.90922004e-02],\n",
              "       [1.68219313e-01, 2.58350998e-01, 5.73429704e-01],\n",
              "       ...,\n",
              "       [8.77957284e-01, 1.21951364e-01, 9.13981567e-05],\n",
              "       [8.25690866e-01, 1.74283981e-01, 2.52116788e-05],\n",
              "       [7.93215096e-01, 2.06522450e-01, 2.62525835e-04]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bRR2VjyIa6Vb"
      },
      "id": "bRR2VjyIa6Vb",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}